{
  "title": "From Tokens to Thoughts: Compression vs Meaning in LLMs and Humans",
  "summary": "A rigorous exploration of how Large Language Models and human cognition balance information compression and semantic fidelity using Rate-Distortion Theory, Information Bottleneck, and empirical benchmarks from psychology.",
  "category": ["LLMs", "Information Theory", "Research+"]
}
