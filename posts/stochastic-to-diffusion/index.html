<style>
  /* Put your styles here */
  /* ----------  TL;DR call-out ---------- */
/* ----------  TL;DR call-out (greyscale theme) ---------- */
.tldr {
  border: 1px solid #c0c0c0;      /* soft grey border */
  border-radius: 0.5rem;
  padding: 1rem 1.25rem;
  background: #f5f5f5;            /* very light grey background */
  color: #333;                    /* dark grey text for readability */
}

.tldr h2 {
  margin: 0 0 0.5rem 0;
  color: #111;                    /* nearly black for the heading */
}

.tldr ol {
  margin-left: 1.3rem;
}

.tldr li strong {
  color: #000;                    /* highlight keywords in pure black */
}

/* ----------  Ensure TL;DR inline math and text wrap on mobile  ---------- */
.tldr,
.tldr p,
.tldr ol,
.tldr li {
  /* allow words, symbols, and math to break anywhere if needed */
  word-wrap: break-word;
  overflow-wrap: break-word;
  word-break: break-word;
  hyphens: auto;
  white-space: normal;
}

/* target KaTeX/MathJax inline spans if you’re using them */
.tldr .katex,
.tldr .MathJax,
.tldr span {
  display: inline-block;
  max-width: 100%;
  white-space: normal;
}

/* shrink font slightly for extra safety on very small screens */
@media (max-width: 400px) {
  .tldr {
    font-size: 0.90rem;
  }
}


/* ----------  table styling ---------- */
table.tbl{
  width:100%;
  border-collapse:collapse;
  margin:2rem 0 2.5rem 0;
  font-size:.95rem;
}
table.tbl caption{
  caption-side:top;
  font-weight:600;
  margin-bottom:.4rem;
}
table.tbl th,
table.tbl td{
  border:1px solid #d0d0d0;
  padding:.45rem .65rem;
  text-align:left;
  vertical-align:top;
}
table.tbl thead{background:#f5f7ff;}
.tbl .shape{
  font-family:"Roboto Mono",ui-monospace,monospace;
  white-space:nowrap;
}
.fact-table th{
  background:#f5f5f5;
  width:160px;
}

/* ----------  lead paragraph ---------- */
.lead{
  font-size:1.05rem;
  line-height:1.6;
  margin:1.2rem 0 2rem 0;
  color:#333;
}
.lead ul{margin:.6rem 0 .6rem 1.4rem}

/* ----------  bridge note ---------- */
.bridge{
  font-size:.95rem;
  margin:.8rem 0 1.1rem 0;
  color:#444;
}          

/* ----------  note paragraph ---------- */
.note {
  border-left: 3px solid #888;     /* subtle grey accent */
  padding: 0.6rem 1rem;
  margin: 1rem 0;
  background: #f9f9f9;             /* very light grey background */
  color: #333;                     /* dark text for readability */
  font-size: 0.97rem;
  line-height: 1.5;
}
.note strong {
  color: #000;                     /* bold keywords in pure black */
}

/* ----------  display equations ---------- */
.eq-scroll{
  display:block;
  overflow-x:auto;
  white-space:nowrap;
  text-align:center;
  margin:1.2rem auto;
  font-size:1.02rem;
}
@media(max-width:600px){
  .eq-scroll{font-size:.9rem}
  table.tbl{font-size:.9rem}
}

/* ----------  inline & block code ---------- */
code,pre{
  font-family:"Fira Code","SFMono-Regular",ui-monospace,monospace;
  font-size:.92rem;
}
code{
  background:#f3f4f6;
  color:#1a1a1a;
  padding:0 .25em;
  border-radius:4px;
}
pre{
  background:#f8f9fb;
  border:1px solid #cfd2d7;
  border-radius:6px;
  padding:.9rem 1rem;
  line-height:1.45;
  overflow-x:auto;
  margin:1.6rem 0;
}
pre code{background:none;padding:0}
@media(max-width:600px){
  pre{font-size:.82rem}
  code{font-size:.86rem}
}

/* ----------  GitHub-style code card ---------- */
.code-card{
  background:#f6f8fa;
  border:1px solid #d0d7de;
  border-radius:6px;
  overflow:hidden;
  margin:1.6rem 0;
}
.code-card .code-header{
  background:#eaeef2;
  border-bottom:1px solid #d0d7de;
  font:.75rem/1 system-ui,sans-serif;
  color:#24292f;
  padding:.45rem .9rem;
  text-transform:lowercase;
}
.code-card pre{
  margin:0;
  padding:.8rem 1rem;
  background:inherit;
  font-size:.92rem;
  white-space:pre;
}
@media(max-width:600px){
  .code-card pre{font-size:.82rem}
}
  
/* === let wide tables side-scroll on narrow screens === */
@media (max-width: 600px){
  table.tbl{
    display:block;          /* makes it a scroll container   */
    overflow-x:auto;        /* side-scroll if too wide        */
    -webkit-overflow-scrolling: touch;
  }
  table.tbl thead,
  table.tbl tbody{
    display:table;          /* keeps header & body aligned    */
    width:100%;
  }
  table.tbl th,
  table.tbl td{
    white-space:nowrap;     /* prevent ugly line wraps        */
  }
}

/* ----------  Responsive heading wraps & scaling ---------- */

/* Allow long words in headings to break */
h1, h2, h3, h4, h5, h6 {
  overflow-wrap: break-word;
  word-wrap: break-word;
  hyphens: auto;
  white-space: normal;       /* override any no-wrap */
}

/* Shrink heading text on narrow viewports */
@media (max-width: 600px) {
  h1 { font-size: 1.5rem; }
  h2 { font-size: 1.3rem; }
  h3 { font-size: 1.15rem; }
  /* you can add h4, h5 as needed */
}

/* Optional: make the entire page text flow better on mobile */
body {
  word-wrap: break-word;
  overflow-wrap: break-word;
}

/* Override scroll-to-top button on mobile so it never hangs off-screen */
#scrollTopBtn {
  /* default for desktop */
  bottom: 2rem;
  right: 2rem;
}

/* on small viewports, reduce the offsets & size */
@media (max-width: 600px) {
  #scrollTopBtn {
    bottom: 1rem !important;
    right: 1rem !important;
    padding: 0.4rem 0.6rem !important;
    font-size: 1rem !important;
    max-width: 2.5rem;    /* ensure it stays compact */
    max-height: 2.5rem;
  }
}



</style>


<section>
  <!-- ========== Blog Title ========== -->
  <h1 id="heading">
    From Brownian Motion to Discrete Diffusion: The Mathematical Foundations of DDPM
  </h1>

  <!-- ========== TL;DR Summary (no math here) ========== -->
  <aside class="tldr">
    <h2 id="TL;DR">TL;DR</h2>
    <p>
      This post traces the mathematical spine of diffusion models, building a clear path from stochastic calculus to practical algorithms:
    </p>
    <ol>
      <li><strong>Brownian motion &amp; Itô calculus:</strong> Understand how stochastic processes evolve and why (dBₜ)² = dt is key.</li>
      <li><strong>Fokker–Planck PDE:</strong> Show how SDEs induce PDEs for probability densities.</li>
      <li><strong>Reverse-time SDE:</strong> Step-by-step derivation of reverse drift from Anderson (1982) → Song et al. (2020).</li>
      <li><strong>Discrete DDPM:</strong> Derive forward noising schedule and variational ELBO.</li>
      <li><strong>Toy model:</strong> Minimal Python DDPM to visualize forward/reverse diffusion.</li>
    </ol>
    <p>
      By the end, you'll see Ho et al. (2020) as a direct descendant of stochastic calculus.
    </p>
  </aside>

  <!-- ========== Introduction Section ========== -->
  <h2 id="introduction">Introduction</h2>
  <p class="lead">
    Diffusion models progressively corrupt data with Gaussian noise and learn to reverse that process.
    Underneath lies a rich interplay of stochastic calculus, PDEs, and variational inference.
  </p>
  <p>
    This blog continues our journey from <a href="https://backpropthoughts.netlify.app/post?postId=diffusion-maths" target="_blank">the previous post on score matching</a>. Here we dive deep into:
  </p>
  <ul>
    <li>Brownian motion: why \((dB_t)^2 = dt\) is central to stochastic integrals.</li>
    <li>Fokker–Planck equation: deriving density evolution from SDEs.</li>
    <li>Reverse-time SDE: unlocking the denoising step.</li>
    <li>Discrete DDPM: bridging continuous SDEs to Ho et al.'s formulation.</li>
  </ul>
  <p class="bridge">
    Along the way, we’ll offer rigorous proofs and practical code, building a solid foundation for understanding diffusion models mathematically.
  </p>

  <!-- ========== Section 2: Brownian Motion & Quadratic Variation ========== -->
<h2 id="brownian-motion">2. Brownian Motion &amp; Quadratic Variation</h2>

<!-- ----- 2.1 Intuition ----- -->
<h3 id="brownian-intuition">2.1 Intuition: Brownian Motion and Quadratic Variation</h3>
<p class="lead">
  Brownian motion, or Wiener process, is the cornerstone of stochastic calculus.
  It describes a continuous-time random walk with independent Gaussian increments:
</p>
<div class="eq-scroll">
  \[
    B_{t+s} - B_t \sim \mathcal{N}(0, s),
    \quad \text{for all } s \geq 0.
  \tag{1}
  \]
</div>

<ul>
  <li><strong>Independent increments:</strong> Each step is independent of the past.</li>
  <li><strong>Variance scaling:</strong> \(\mathrm{Var}[B_t] = t\).</li>
</ul>

<p class="bridge">
  Crucially, Brownian paths are nowhere differentiable but have a well-defined quadratic variation:
</p>
<div class="eq-scroll">
  \[
    (dB_t)^2 = dt.
  \tag{2}
  \]
</div>

<details>
  <summary><strong>Step-by-step derivation of \((dB_t)^2 = dt\)</strong></summary>
  <p>
    Consider partitioning \([0, t]\) into \(n\) subintervals of width \(\Delta t = t/n\). Define increments:
  </p>
  <div class="eq-scroll">
  \[
    \Delta B_i = B_{t_i} - B_{t_{i-1}} \sim \mathcal{N}(0, \Delta t).
  \]
  </div>

  <p>
    The quadratic variation is defined as:
  </p>
  <div class="eq-scroll">
  \[
    [B]_t = \lim_{n \to \infty} \sum_{i=1}^n (\Delta B_i)^2.
  \]
  </div>

  <p>
    Since each increment satisfies \(\mathbb{E}[(\Delta B_i)^2] = \Delta t\), the expected sum is:
  </p>
  <div class="eq-scroll">
  \[
    \mathbb{E}\left[\sum_{i=1}^n (\Delta B_i)^2\right] = n\Delta t = t.
  \]
  </div>

  <p>
    Using variance bounds and convergence in probability (law of large numbers for martingales), the quadratic variation converges:
  </p>
  <div class="eq-scroll">
  \[
    [B]_t = t \quad \text{almost surely}.
  \]
  </div>

  <p>
    In differential notation, this leads directly to:
  </p>
  <div class="eq-scroll">
  \[
    (dB_t)^2 = dt.
  \]
  </div>
</details>

<!-- ----- 2.2 Connection to Diffusion Models ----- -->
<h3 id="brownian-connection">2.2 Connection to Diffusion Models</h3>
<p class="lead">
  The identity \((dB_t)^2 = dt\) is not just an abstract feature of Brownian motion; it is the
  linchpin in the mathematics of diffusion models.
</p>
<p>
  Consider a stochastic differential equation (SDE):
</p>
<div class="eq-scroll">
  \[
    dx_t = \mu(x_t, t)\,dt + \sigma(t)\,dB_t.
  \tag{3}
  \]
</div>

<p>
  When applying Itô’s lemma to a function \(f(x,t)\), quadratic variation contributes second-order terms that are critical for deriving the Fokker–Planck equation (see Section 3).
</p>

<p class="note">
  <strong>Key Takeaway:</strong> Without \((dB_t)^2=dt\), the stochastic calculus foundation of diffusion models would collapse.
</p>

<!-- ========== Section 3: Fokker–Planck PDE ========== -->
<h2 id="fokker-planck">3. Deriving the Fokker–Planck Equation</h2>

<p class="lead">
  The Fokker–Planck equation describes how the probability density \(p(x, t)\) of a stochastic process evolves over time.
  In diffusion models, it formalizes the forward noising process as a PDE on \(p(x,t)\).
</p>

<p>
  Starting with a general SDE:
</p>

<div class="eq-scroll">
  \[
    dx_t = \mu(x_t, t)\,dt + \sigma(t)\,dB_t,
  \tag{4}
  \]
</div>

<p>
  where:
</p>
<ul>
  <li>\(\mu(x, t)\): drift vector field (deterministic dynamics)</li>
  <li>\(\sigma(t)\): diffusion coefficient (strength of noise)</li>
  <li>\(dB_t\): Brownian increment</li>
</ul>

<p class="bridge">
  We derive the PDE governing \(p(x,t)\) by applying Itô’s lemma and using the expectation operator.
</p>

<!-- ----- 3.1 Itô’s Lemma Expanded ----- -->
<h3 id="ito-lemma">3.1 Itô’s Lemma (Expanded)</h3>
<p class="lead">
  For a twice-differentiable test function \(f(x,t)\), Itô’s lemma gives:
</p>

<div class="eq-scroll">
  \[
  df(x_t, t) = \partial_t f\,dt + \nabla_x f^\top dx_t + \tfrac12\,dx_t^\top \nabla_x^2 f\,dx_t.
  \tag{5}
  \]
</div>

<details>
  <summary><strong>Step-by-step derivation of Eq. (5)</strong></summary>
  <p>
    Expanding \(df(x_t, t)\) via Taylor’s theorem (to second order):
  </p>
  <div class="eq-scroll">
  \[
  df = \partial_t f\,dt + \nabla_x f^\top dx + \tfrac12 dx^\top \nabla_x^2 f\,dx + o(dt).
  \]
  </div>

  <p>
    Substituting \(dx = \mu dt + \sigma dB_t\):
  </p>
  <div class="eq-scroll">
  \[
  df = \partial_t f\,dt + \nabla_x f^\top (\mu dt + \sigma dB_t) + \tfrac12(\mu dt + \sigma dB_t)^\top \nabla_x^2 f(\mu dt + \sigma dB_t).
  \]
  </div>

  <p>
    Quadratic terms like \((dt)^2\) vanish. The only surviving second-order term comes from \((\sigma dB_t)^\top \nabla_x^2 f(\sigma dB_t)\):
  </p>
  <div class="eq-scroll">
  \[
  \tfrac12 \sigma^2 \Delta_x f\,dt.
  \]
  </div>

  <p>
    Resulting in:
  </p>
  <div class="eq-scroll">
  \[
  df = \big[\partial_t f + \mu^\top\nabla_x f + \tfrac12\sigma^2\Delta_x f\big]dt + \sigma\nabla_x f^\top dB_t.
  \]
  </div>
</details>

<!-- ----- 3.2 Expectation and Time Evolution ----- -->
<h3 id="expectation-evolution">3.2 Expectation and Time Evolution</h3>
<p>
  Taking expectations and using \(\mathbb{E}[dB_t] = 0\) removes the stochastic term:
</p>

<div class="eq-scroll">
  \[
  \frac{d}{dt}\,\mathbb{E}[f(x_t, t)] =
  \mathbb{E}\left[\partial_t f + \mu^\top\nabla_x f + \tfrac12\sigma^2\Delta_x f\right].
  \tag{6}
  \]
</div>

<details>
  <summary><strong>Step-by-step derivation of Eq. (6)</strong></summary>
  <p>
    \(\mathbb{E}[df] = \mathbb{E}[...]\,dt\), and the stochastic integral’s expectation vanishes because \(\mathbb{E}[dB_t] = 0\).
  </p>
  <p>
    So:
  </p>
  <div class="eq-scroll">
  \[
  \frac{d}{dt}\mathbb{E}[f(x_t, t)] = \mathbb{E}\left[\partial_t f + \mu^\top\nabla_x f + \tfrac12\sigma^2\Delta_x f\right].
  \]
  </div>
</details>

<!-- ----- 3.3 From Expectations to PDE ----- -->
<h3 id="expectation-pde">3.3 From Expectations to PDE</h3>
<p>
  Since \(\mathbb{E}[f(x,t)] = \int f(x)p(x,t)dx\), differentiating under the integral:
</p>

<div class="eq-scroll">
  \[
  \frac{d}{dt}\mathbb{E}[f] = \int f(x)\,\partial_t p(x,t)\,dx + \int \partial_t f(x,t)\,p(x,t)\,dx.
  \tag{7}
  \]
</div>

<details>
  <summary><strong>Step-by-step derivation of Eq. (7)</strong></summary>
  <p>
    Differentiate:
  </p>
  <div class="eq-scroll">
  \[
  \frac{d}{dt}\int f(x,t)p(x,t)dx
  \]
  </div>

  <p>
    Chain rule splits it into two terms—one from \(\partial_t f\), one from \(\partial_t p\).
  </p>
</details>

<p class="bridge">
  Equating both expressions for \(\frac{d}{dt}\mathbb{E}[f]\) gives:
</p>

<div class="eq-scroll">
  \[
  \int f(x)\,\partial_t p(x,t)\,dx
  = \int \big[\mu^\top\nabla_x f + \tfrac12\sigma^2\Delta_x f\big]\,p(x,t)\,dx.
  \tag{8}
  \]
</div>

<!-- ----- 3.4 Integration by Parts ----- -->
<h3 id="integration-parts">3.4 Integration by Parts</h3>
<p>
  Using integration by parts and divergence theorem to transfer derivatives from \(f\) to \(p\):
</p>

<div class="eq-scroll">
  \[
  \int \mu^\top\nabla_x f\,p\,dx = -\int f\,\nabla_x^\top(\mu p)\,dx,
  \tag{9}
  \]
</div>

<div class="eq-scroll">
  \[
  \int \tfrac12\sigma^2\Delta_x f\,p\,dx = -\int f\,\nabla_x^\top\left(\tfrac12\sigma^2\nabla_x p\right)\,dx.
  \tag{10}
  \]
</div>

<p class="note">
  Boundary terms vanish as \(p(x,t)\) decays at infinity.
</p>

<details>
  <summary><strong>Why boundary terms vanish</strong></summary>
  <p>
    Since \(p(x,t)\) represents a probability density, it goes to zero sufficiently fast as \(|x| \to \infty\).
  </p>
</details>

<!-- ----- 3.5 Final PDE ----- -->
<h3 id="fokker-planck-final">3.5 The Fokker–Planck PDE</h3>
<p>
  Substituting back yields:
</p>

<div class="eq-scroll">
  \[
  \partial_t p(x,t) = -\nabla_x^\top[\mu(x,t)p(x,t)] + \nabla_x^\top\left(\tfrac12\sigma^2(t)\nabla_x p(x,t)\right).
  \tag{11}
  \]
</div>

<p class="explain">
  This is the <strong>Fokker–Planck equation</strong> (a.k.a. forward Kolmogorov equation).
</p>

<figure>
  <img src="posts/stochastic-to-diffusion/assets/figure1.png" alt="Drift and Diffusion of probability density">
  <figcaption>
    <strong>Figure 1:</strong> Evolution of probability density showing drift (left) and diffusion (right). Drift compresses or shifts densities; diffusion spreads them out.
  </figcaption>
</figure>

<!-- ========== Section 4: Reverse-Time SDE ========== -->
<h2 id="reverse-sde">4. Deriving the Reverse-Time SDE</h2>

<p class="lead">
  The forward SDE models how noise progressively corrupts data. But in diffusion models, our goal is the reverse: we want to denoise step-by-step.
  To achieve this, we derive the SDE that governs a stochastic process when time is reversed. First shown by Anderson (1982), this forms the basis of modern diffusion models (Song et al., 2020).
</p>

<p>
  Recall the forward SDE:
</p>

<div class="eq-scroll">
  \[
    dx_t = \mu(x_t, t)\,dt + \sigma(t)\,dB_t,
  \tag{12}
  \]
</div>

<p>
  where:
</p>
<ul>
  <li>\(\mu(x, t)\): drift (deterministic dynamics)</li>
  <li>\(\sigma(t)\): diffusion coefficient (noise strength)</li>
  <li>\(dB_t\): Brownian motion increment</li>
</ul>

<p class="bridge">
  We now ask: what is the SDE governing \(x_t\) when time runs backward from \(t=T\) to \(t=0\)?
</p>

<!-- ----- 4.1 Probability Flow Perspective ----- -->
<h3 id="probability-flow">4.1 Probability Flow Perspective</h3>
<p class="lead">
  Reversing time requires reversing not just sample paths but also the evolution of probability densities \(p(x,t)\).
</p>

<p>
  The forward Fokker–Planck equation describes density evolution:
</p>

<div class="eq-scroll">
  \[
    \partial_t p(x,t) = -\nabla_x\cdot[\mu(x,t)p(x,t)] + \tfrac12\sigma^2(t)\Delta_x p(x,t).
  \tag{13}
  \]
</div>

<p class="note">
  Our goal is to find a new drift \(\tilde{\mu}(x,t)\) such that evolving forward under \(\tilde{\mu}(x,t)\) corresponds to reversing the original process in time.
</p>

<!-- ----- 4.2 Derivation of Reverse Drift ----- -->
<h3 id="reverse-drift-derivation">4.2 Derivation of the Reverse Drift</h3>
<p>
  Anderson (1982) showed that the reverse-time drift is:
</p>

<div class="eq-scroll">
  \[
    \tilde{\mu}(x,t) = \mu(x,t) - \sigma^2(t)\nabla_x\log p(x,t).
  \tag{14}
  \]
</div>

<details>
  <summary><strong>Step-by-step derivation of Eq. (14)</strong></summary>
  <p>
    Start from the forward Fokker–Planck equation:
  </p>
  <div class="eq-scroll">
  \[
    \partial_t p = -\nabla_x\cdot(\mu p) + \tfrac12\sigma^2\Delta_x p.
  \]
  </div>

  <p>
    Under time reversal (\(t \mapsto T-t\)), define \(\tilde{p}(x,t)=p(x,T-t)\).
  </p>

  <p>
    The reverse-time Fokker–Planck PDE becomes:
  </p>
  <div class="eq-scroll">
  \[
    \partial_t \tilde{p}(x,t) = -\nabla_x\cdot(\tilde{\mu}(x,t)\tilde{p}(x,t)) + \tfrac12\sigma^2(t)\Delta_x\tilde{p}(x,t).
  \]
  </div>

  <p>
    Matching this with the forward PDE gives:
  </p>
  <div class="eq-scroll">
  \[
    \tilde{\mu}(x,t)\tilde{p}(x,t) = \mu(x,t)\tilde{p}(x,t) - \sigma^2(t)\nabla_x\tilde{p}(x,t).
  \]
  </div>

  <p>
    Divide both sides by \(\tilde{p}(x,t)\) (assumed \(>0\)):
  </p>
  <div class="eq-scroll">
  \[
    \tilde{\mu}(x,t) = \mu(x,t) - \sigma^2(t)\nabla_x\log\tilde{p}(x,t).
  \]
  </div>

  <p>
    Since \(\tilde{p}(x,t) = p(x,T-t)\), we finally write:
  </p>
  <div class="eq-scroll">
  \[
    \tilde{\mu}(x,t) = \mu(x,t) - \sigma^2(t)\nabla_x\log p(x,t).
  \]
  </div>
</details>

<!-- ----- 4.3 Final Reverse SDE ----- -->
<h3 id="reverse-sde-final">4.3 The Reverse-Time SDE</h3>
<p class="lead">
  Substituting \(\tilde{\mu}(x,t)\) gives the reverse-time SDE:
</p>

<div class="eq-scroll">
  \[
    dx_t = \left[\mu(x,t) - \sigma^2(t)\nabla_x\log p(x,t)\right]dt + \sigma(t)\,d\bar{B}_t,
  \tag{15}
  \]
</div>

<p>
  where \(d\bar{B}_t\) is Brownian motion in reverse time.
</p>

<details>
  <summary><strong>Why does the noise term stay the same?</strong></summary>
  <p>
    The Wiener process is symmetric under time reversal. So \(d\bar{B}_t\) remains a Brownian increment but with time flipped.
  </p>
</details>

<!-- ----- 4.4 Connection to Diffusion Models ----- -->
<h3 id="reverse-sde-diffusion">4.4 Connection to Diffusion Models</h3>
<p class="lead">
  In practice, we don’t know \(p(x,t)\). Instead, we train a neural network to approximate the score function:
</p>

<div class="eq-scroll">
  \[
    s_\theta(x,t) \approx \nabla_x\log p(x,t).
  \]
</div>

<p>
  Substituting \(s_\theta(x,t)\) into Eq. (15), we get a learnable reverse SDE:
</p>

<div class="eq-scroll">
  \[
    dx_t = \left[\mu(x,t) - \sigma^2(t)s_\theta(x,t)\right]dt + \sigma(t)\,d\bar{B}_t.
  \tag{16}
  \]
</div>

<p class="note">
  <strong>Key Insight:</strong> The neural network effectively guides noisy samples back to the data manifold by approximating the gradient of the log-density.
</p>

<!-- ----- 4.5 Visual Explanation ----- -->
<figure>
  <img src="posts/stochastic-to-diffusion/assets/figure2.png" alt="Forward and reverse-time SDE illustration">
  <figcaption>
    <strong>Figure 2:</strong> Forward SDE (left) progressively corrupts data. The reverse-time SDE (right) denoises step by step.
  </figcaption>
</figure>

<!-- ----- 4.6 Summary ----- -->
<h3 id="reverse-sde-summary">4.6 Summary</h3>
<p class="lead">
  The reverse-time SDE derived by Anderson provides the theoretical foundation for sampling in diffusion models. By estimating \(\nabla_x\log p(x,t)\), modern models reverse the stochastic corruption applied during training.
</p>

<!-- ========== Section 5: Discrete DDPM Formulation ========== -->
<h2 id="ddpm-formulation">5. Discrete DDPM Formulation</h2>

<p class="lead">
  With the continuous-time SDEs established, we now discretize time into \(T\) steps to arrive at the discrete formulation of denoising diffusion probabilistic models (DDPMs) introduced by Ho et al. (2020).
</p>

<p>
  The forward process starts with a clean data point \(x_0 \sim q(x_0)\) and progressively corrupts it over \(T\) steps by adding Gaussian noise:
</p>

<div class="eq-scroll">
  \[
    q(x_t|x_{t-1}) = \mathcal{N}\big(x_t; \sqrt{1-\beta_t}\,x_{t-1},\,\beta_t I\big),
  \tag{28}
  \]
</div>

<p>
  Here \(\beta_t\) is a variance schedule (\(0 < \beta_t < 1\)) that determines how much noise is added at each step.
</p>

<!-- ----- 5.1 Forward Noising Process ----- -->
<h3 id="forward-noising">5.1 Forward Noising Process</h3>
<p class="lead">
  At each timestep \(t\), \(x_{t-1}\) is scaled and perturbed with Gaussian noise:
</p>

<div class="eq-scroll">
  \[
    x_t = \sqrt{1-\beta_t}\,x_{t-1} + \sqrt{\beta_t}\,\epsilon_t,\quad \epsilon_t \sim \mathcal{N}(0,I).
  \tag{29}
  \]
</div>

<p>
  This recursion propagates noise over multiple steps. To simplify sampling, Ho et al. derived a closed-form expression for the marginal distribution \(q(x_t|x_0)\).
</p>

<!-- ----- 5.2 Marginal Distribution Derivation ----- -->
<h3 id="marginal-q">5.2 Marginal \(q(x_t|x_0)\)</h3>
<p class="lead">
  Repeated application of Eq. (29) leads to:
</p>

<div class="eq-scroll">
  \[
    q(x_t|x_0) = \mathcal{N}\big(x_t; \sqrt{\bar{\alpha}_t}\,x_0,\,(1-\bar{\alpha}_t)I\big),
  \tag{30}
  \]
</div>

<details>
  <summary><strong>Step-by-step derivation of Eq. (30)</strong></summary>
  <div class="eq-scroll">
  \[
  \begin{aligned}
  x_t &= \sqrt{1-\beta_t}\,x_{t-1} + \sqrt{\beta_t}\,\epsilon_t \\
      &= \sqrt{\alpha_t}\left(\sqrt{\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_{t-1}}\,\epsilon_{t-1}\right) + \sqrt{1-\alpha_t}\,\epsilon_t \\
      &\vdots \\
      &= \sqrt{\prod_{s=1}^t\alpha_s}x_0 + \sum_{s=1}^t\sqrt{\beta_s}\left(\prod_{j=s+1}^t\sqrt{\alpha_j}\right)\epsilon_s
  \end{aligned}
  \]
  </div>
  <p>
    Defining \(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\) gives:
  </p>
  <div class="eq-scroll">
  \[
    x_t = \sqrt{\bar{\alpha}_t}\,x_0 + \sqrt{1-\bar{\alpha}_t}\,\epsilon, \quad \epsilon \sim \mathcal{N}(0,I).
  \]
  </div>
</details>

<p class="note">
  This closed-form means we can directly sample \(x_t\) at any step \(t\) from \(x_0\) without simulating all intermediate steps.
</p>

<!-- ----- 5.3 Posterior Distribution Derivation ----- -->
<h3 id="posterior-q">5.3 Posterior \(q(x_{t-1}|x_t, x_0)\)</h3>
<p class="lead">
  For training, we also require the posterior \(q(x_{t-1}|x_t, x_0)\). Using Gaussian conditioning rules:
</p>

<div class="eq-scroll">
  \[
    q(x_{t-1}|x_t, x_0) = \mathcal{N}\big(x_{t-1};\,\tilde{\mu}_t(x_t, x_0),\,\tilde{\beta}_t I\big),
  \tag{33}
  \]
</div>

<details>
  <summary><strong>Step-by-step derivation of Eq. (34) and Eq. (35)</strong></summary>
  <p>
    From the joint distribution:
  </p>
  <div class="eq-scroll">
  \[
    q(x_{t-1},x_t|x_0) = q(x_t|x_{t-1})q(x_{t-1}|x_0),
  \]
  </div>
  <p>
    both terms are Gaussian. Conditioning gives:
  </p>
  <div class="eq-scroll">
  \[
    \tilde{\mu}_t(x_t, x_0) = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}x_0
    + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t,
  \]
  </div>
  <div class="eq-scroll">
  \[
    \tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t.
  \]
  </div>
</details>

<p class="note">
  These posteriors are crucial for training objectives based on variational inference.
</p>

<!-- ----- 5.4 Reverse Process Approximation ----- -->
<h3 id="reverse-process">5.4 Reverse Process Approximation</h3>
<p class="lead">
  In generation, we approximate the reverse process with a learned model:
</p>

<div class="eq-scroll">
  \[
    p_\theta(x_{t-1}|x_t) = \mathcal{N}\big(x_{t-1};\,\mu_\theta(x_t,t),\,\Sigma_\theta(x_t,t)\big).
  \tag{36}
  \]
</div>

<p>
  \(\mu_\theta(x_t,t)\) and \(\Sigma_\theta(x_t,t)\) are predicted by a neural network trained to approximate the true reverse posterior.
</p>

<!-- ----- 5.5 Visual Explanation ----- -->
<figure>
  <img src="posts/stochastic-to-diffusion/assets/figure3.png" alt="Forward and reverse discrete diffusion steps">
  <figcaption>
    <strong>Figure 3:</strong> Forward process \(q(x_t|x_{t-1})\) adds Gaussian noise over \(T\) steps. The reverse process \(p_\theta(x_{t-1}|x_t)\) denoises step-by-step.
  </figcaption>
</figure>

<!-- ----- 5.6 Summary ----- -->
<h3 id="ddpm-summary">5.5 Summary</h3>
<p class="lead">
  The discrete DDPM formulation is the bridge from theory to practical implementation. It allows efficient training via variational objectives and sampling by iterating reverse transitions.
</p>

    <!-- ========== Section 6: ELBO and Training Objective ========== -->
<h2 id="elbo-training">6. ELBO and Variational Objective</h2>

<p class="lead">
  To train diffusion models, we maximize a variational lower bound on the log-likelihood of the data.
  This section derives the Evidence Lower Bound (ELBO) step by step.
</p>

<p>
  Recall the reverse model:
</p>

<div class="eq-scroll">
  \[
    p_\theta(x_{0:T}) = p(x_T)\prod_{t=1}^T p_\theta(x_{t-1}|x_t),
  \tag{37}
  \]
</div>

<p>
  where \(p(x_T) = \mathcal{N}(x_T; 0, I)\) is the prior and \(p_\theta(x_{t-1}|x_t)\) is learned.
</p>

<p>
  The goal is to maximize \(\log p_\theta(x_0)\). Using variational inference:
</p>

<div class="eq-scroll">
  \[
    \log p_\theta(x_0) \geq \mathbb{E}_{q(x_{1:T}|x_0)}\left[\log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}\right].
  \tag{38}
  \]
</div>

<p>
  The RHS is the ELBO, which we expand below.
</p>

<!-- ----- 6.1 Expanding the ELBO ----- -->
<h3 id="elbo-expansion">6.1 Expanding the ELBO</h3>
<p class="lead">
  Using the factorized forms of \(p_\theta\) and \(q\), the ELBO becomes:
</p>

<div class="eq-scroll">
  \[
  \begin{aligned}
  \text{ELBO} = \mathbb{E}_{q}\bigg[&
  \log p(x_T) + \sum_{t=1}^T \log p_\theta(x_{t-1}|x_t) \\
  & - \sum_{t=1}^T \log q(x_t|x_{t-1})\bigg].
  \end{aligned}
  \tag{39}
  \]
</div>

<details>
  <summary><strong>Derivation of Eq. (39)</strong></summary>
  <p>
    Start from the ELBO:
  </p>
  <div class="eq-scroll">
  \[
    \mathbb{E}_q\left[\log\frac{p(x_T)\prod_{t}p_\theta(x_{t-1}|x_t)}{\prod_t q(x_t|x_{t-1})}\right].
  \]
  </div>
  <p>
    Expand the products:
  </p>
  <div class="eq-scroll">
  \[
    = \mathbb{E}_q\left[\log p(x_T) + \sum_t \log p_\theta(x_{t-1}|x_t) - \sum_t \log q(x_t|x_{t-1})\right].
  \]
  </div>
</details>

<!-- ----- 6.2 Grouping KL Terms ----- -->
<h3 id="elbo-kl-terms">6.2 Grouping KL Terms</h3>
<p>
  Rearranging gives a sum of KL divergences and an expected negative log-likelihood:
</p>

<div class="eq-scroll">
  \[
  \begin{aligned}
  \text{ELBO} = -\sum_{t=1}^T\mathbb{E}_q\big[
  D_\mathrm{KL}\big(q(x_{t-1}|x_t, x_0)\,\|\,p_\theta(x_{t-1}|x_t)\big)
  \big] - \mathbb{E}_q[\log p(x_T)].
  \end{aligned}
  \tag{40}
  \]
</div>

<details>
  <summary><strong>Step-by-step derivation of Eq. (40)</strong></summary>
  <p>
    Use Bayes’ rule: \(q(x_{t-1}|x_t,x_0)\) appears naturally from \(q(x_{1:T}|x_0)\).
  </p>
  <p>
    Rewrite:
  </p>
  <div class="eq-scroll">
  \[
  D_\mathrm{KL}\big(q(x_{t-1}|x_t, x_0)\,\|\,p_\theta(x_{t-1}|x_t)\big)
  \]
  </div>
  <p>
    This KL measures how close the true posterior is to the model prediction.
  </p>
</details>

<!-- ----- 6.3 Simplified Training Objective ----- -->
<h3 id="simplified-elbo">6.3 Simplified Training Objective</h3>
<p class="lead">
  Ho et al. (2020) make two simplifications:
</p>
<ul>
  <li>Fix \(\Sigma_\theta(x_t,t)\) to known \(\tilde{\beta}_t I\).</li>
  <li>Parametrize \(\mu_\theta(x_t,t)\) to predict the noise \(\epsilon_\theta(x_t,t)\).</li>
</ul>

<p>
  The simplified loss is:
</p>

<div class="eq-scroll">
  \[
  L_{\text{simple}} = \mathbb{E}_{t,x_0,\epsilon}\bigg[
  \|\epsilon - \epsilon_\theta(x_t, t)\|^2
  \bigg].
  \tag{41}
  \]
</div>

<details>
  <summary><strong>Derivation of Eq. (41)</strong></summary>
  <p>
    Substitute \(\mu_\theta(x_t,t)\) in terms of \(\epsilon_\theta(x_t,t)\) and simplify the KL.
  </p>
  <p>
    Results in a denoising score-matching objective.
  </p>
</details>

<p class="note">
  \(L_{\text{simple}}\) means the model learns to predict the noise added at each step.
</p>

<!-- ----- 6.4 Summary ----- -->
<h3 id="elbo-summary">6.4 Summary</h3>
<p class="lead">
  The ELBO connects probabilistic modeling to a tractable loss function. This elegant derivation reduces the complex variational problem to a simple denoising objective.
</p>

<figure>
  <img src="posts/stochastic-to-diffusion/assets/figure4.png" alt="ELBO and training objective schematic">
  <figcaption>
    <strong>Figure 4:</strong> Schematic of the ELBO decomposition into KL terms and simplified loss for DDPM training.
  </figcaption>
</figure>


<!-- ========== Section 7: Minimal 2D Diffusion Model ========== -->
<h2 id="toy-ddpm">7. Minimal 2D Diffusion Model</h2>

<p class="lead">
  To make the abstract mathematics tangible, we now implement a toy 2D diffusion model.
  This example uses a Gaussian mixture dataset to visualize both the forward noising process and the reverse denoising process.
</p>

<p>
  The implementation is lightweight—purely in NumPy—and demonstrates how diffusion steps progressively add noise to data and how reverse steps recover structure.
</p>

<!-- ----- 7.1 Original Data Distribution ----- -->
<h3 id="toy-data">7.1 Original Data Distribution</h3>

<p>
  We begin with a simple 2D Gaussian mixture—two blobs centered at \((2, 2)\) and \((-2, -2)\).
</p>

<figure>
  <img src="posts/stochastic-to-diffusion/assets/figure5.png" alt="Original Data Distribution (2D Gaussian Mixture)">
  <figcaption>
    <strong>Figure 5:</strong> Original data distribution \(x_0\), generated from the code output below.
  </figcaption>
</figure>

<details>
  <summary><strong>Code: Generating 2D Gaussian Mixture</strong></summary>
  <div class="code-card">
    <div class="code-header">data_generation.py</div>
    <pre><code>import numpy as np
import matplotlib.pyplot as plt

# Set random seed
np.random.seed(42)

# Create a 2D Gaussian mixture
n_samples = 1000
mean1, mean2 = [2, 2], [-2, -2]
cov = 0.1 * np.eye(2)

samples1 = np.random.multivariate_normal(mean1, cov, n_samples // 2)
samples2 = np.random.multivariate_normal(mean2, cov, n_samples // 2)
x0 = np.vstack([samples1, samples2])

plt.scatter(x0[:, 0], x0[:, 1], alpha=0.5)
plt.title('Original Data Distribution $x_0$')
plt.xlabel('x1')
plt.ylabel('x2')
plt.axis('equal')
plt.show()</code></pre>
  </div>
</details>

<!-- ----- 7.2 Forward Noising Process ----- -->
<h3 id="toy-forward">7.2 Forward Noising Process</h3>

<p class="lead">
  Next, we apply the forward noising process using a linear variance schedule (\(\beta_t\)). This progressively corrupts \(x_0\) into Gaussian noise \(x_T\).
</p>

<div class="eq-scroll">
\[
  x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\,\epsilon,\quad \epsilon \sim \mathcal{N}(0, I).
\tag{42}
\]
</div>

<figure>
  <img src="posts/stochastic-to-diffusion/assets/figure6.png" alt="Forward Noising Process">
  <figcaption>
    <strong>Figure 6:</strong> Forward noising process at various timesteps \(t\), showing how the original data distribution \(x_0\) is gradually transformed into pure Gaussian noise \(x_T\). Generated from the code output below.
  </figcaption>
</figure>

<details>
  <summary><strong>Code: Forward Diffusion Process</strong></summary>
  <div class="code-card">
    <div class="code-header">forward_diffusion.py</div>
    <pre><code># Forward noising schedule
T = 1000
betas = np.linspace(1e-4, 0.02, T)
alphas = 1.0 - betas
alpha_bars = np.cumprod(alphas)

# Sample noisy x_t at selected timesteps
timesteps = [0, T//4, T//2, 3*T//4, T-1]
fig, axes = plt.subplots(1, len(timesteps), figsize=(15, 3))

for idx, t in enumerate(timesteps):
    sqrt_alpha_bar = np.sqrt(alpha_bars[t])
    sqrt_one_minus_alpha_bar = np.sqrt(1 - alpha_bars[t])
    noise = np.random.randn(*x0.shape)
    xt = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise

    axes[idx].scatter(xt[:, 0], xt[:, 1], alpha=0.5)
    axes[idx].set_title(f'$x_{{{t}}}$')
    axes[idx].axis('equal')

plt.suptitle('Forward Noising Process')
plt.show()</code></pre>
  </div>
</details>

<!-- ----- 7.3 Reverse Denoising Process ----- -->
<h3 id="toy-reverse">7.3 Reverse Denoising Process</h3>

<p class="lead">
  For simplicity, we assume a perfect noise predictor \(\epsilon_\theta(x_t, t)\). The reverse process progressively denoises \(x_T\) to recover \(x_0\).
</p>

<figure>
  <img src="posts/stochastic-to-diffusion/assets/figure7.png" alt="Forward and Reverse Diffusion Trajectories">
  <figcaption>
    <strong>Figure 7:</strong> (Left) Original data \(x_0\), (Center) pure noise \(x_T\), and (Right) recovered \(\hat{x}_0\) via reverse diffusion. Generated from the code output below.
  </figcaption>
</figure>

<details>
  <summary><strong>Code: Reverse Denoising Process</strong></summary>
  <div class="code-card">
    <div class="code-header">reverse_diffusion.py</div>
    <pre><code># Start from pure noise
xT = np.random.randn(*x0.shape)  # Pure Gaussian noise

x_hat = xT.copy()
for t in reversed(range(T)):
    beta = betas[t]
    alpha = alphas[t]
    alpha_bar = alpha_bars[t]

    # Oracle noise prediction (since we know true x0 in toy example)
    sqrt_alpha_bar = np.sqrt(alpha_bar)
    sqrt_one_minus_alpha_bar = np.sqrt(1 - alpha_bar)
    epsilon_pred = (x_hat - sqrt_alpha_bar * x0) / sqrt_one_minus_alpha_bar
    x0_pred = (x_hat - sqrt_one_minus_alpha_bar * epsilon_pred) / sqrt_alpha_bar

    # Update x_hat
    if t > 0:
        noise = np.random.randn(*x0.shape)
        x_hat = np.sqrt(alphas[t-1]) * x0_pred + np.sqrt(1 - alphas[t-1]) * noise
    else:
        x_hat = x0_pred  # Last step, no noise

# Plot original, noisy, and denoised
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
axes[0].scatter(x0[:, 0], x0[:, 1], alpha=0.5)
axes[0].set_title('Original $x_0$')

axes[1].scatter(xT[:, 0], xT[:, 1], alpha=0.5)
axes[1].set_title('Pure Noise $x_T$')

axes[2].scatter(x_hat[:, 0], x_hat[:, 1], alpha=0.5)
axes[2].set_title('Denoised $\hat{x}_0$')

for ax in axes:
    ax.axis('equal')

plt.suptitle('Forward and Reverse Diffusion')
plt.show()</code></pre>
  </div>
</details>

<p class="note">
  <strong>Key Insight:</strong> Even with a simple oracle, we see how the reverse process undoes Gaussian corruption step by step.
</p>

<!-- ----- 7.4 Summary ----- -->
<h3 id="toy-summary">7.4 Summary</h3>
<p class="lead">
  This minimal 2D diffusion demo highlights the essence of DDPMs: iterative noise addition and learned denoising. In practice, \(\epsilon_\theta(x_t, t)\) is trained to approximate the score function.
</p>

<!-- ========== Section 8: Beyond DDPM: Improvements and Variants ========== -->
<h2 id="beyond-ddpm">8. Beyond DDPM: Improvements and Variants</h2>

<p class="lead">
  While DDPMs established a new paradigm for generative modeling, their vanilla formulation is not without drawbacks. Over the past few years, a wave of innovations has emerged to overcome these limitations and push diffusion models to state-of-the-art performance.
</p>

<!-- ----- 8.1 Limitations of Vanilla DDPM ----- -->
<h3 id="ddpm-limitations">8.1 Limitations of Vanilla DDPM</h3>
<ul>
  <li><strong>Slow Sampling:</strong> Original DDPM requires hundreds to thousands of denoising steps to generate a single sample, leading to impractical runtimes.</li>
  <li><strong>High Memory and Compute:</strong> Operating directly in pixel space makes training and sampling expensive on high-resolution data.</li>
  <li><strong>Fixed Noise Schedule:</strong> Predefined \(\beta_t\) schedules can restrict flexibility and may not be optimal for all datasets.</li>
  <li><strong>Training Instability:</strong> Sensitive to learning rates, schedules, and architecture choices.</li>
</ul>

<p class="note">
  These challenges motivated a series of refinements and alternative formulations.
</p>

<!-- ----- 8.2 Major Improvements in Diffusion Models ----- -->
<h3 id="ddpm-improvements">8.2 Major Improvements in Diffusion Models</h3>
<p class="lead">
  Let’s briefly highlight some of the most impactful advancements building on the original DDPM framework.
</p>

<!-- ---- 8.2.a DDIM ---- -->
<h4 id="ddim">8.2.a Denoising Diffusion Implicit Models (DDIM)</h4>
<p>
  Proposed by Song et al. (2021), DDIM introduces non-Markovian deterministic sampling. It allows for:
</p>
<ul>
  <li>Fast sampling with as few as 50 steps (compared to 1000 in DDPM).</li>
  <li>Interpolation in latent space for smooth transitions between samples.</li>
  <li>Preservation of the original DDPM training objective while modifying the sampling process.</li>
</ul>

<!-- ---- 8.2.b Score-Based Generative Models ---- -->
<h4 id="sgm">8.2.b Score-Based Generative Models (SGMs)</h4>
<p>
  Song et al. generalized diffusion models as stochastic differential equations (SDEs). This viewpoint enables:
</p>
<ul>
  <li>Advanced sampling techniques like probability flow ODEs.</li>
  <li>Unification of score-based and diffusion models under a common framework.</li>
  <li>Foundations for modern large-scale diffusion systems.</li>
</ul>

<!-- ---- 8.2.c Latent Diffusion Models ---- -->
<h4 id="ldm">8.2.c Latent Diffusion Models (LDM)</h4>
<p>
  Rombach et al. (2022) proposed performing diffusion in a compressed latent space using autoencoders:
</p>
<ul>
  <li>Reduces computational cost by orders of magnitude.</li>
  <li>Enables training on high-resolution images (e.g., 512×512, 1024×1024).</li>
  <li>Forms the core of <strong>Stable Diffusion</strong> and similar models.</li>
</ul>

<!-- ---- 8.2.d Classifier-Free Guidance ---- -->
<h4 id="cfg">8.2.d Classifier-Free Guidance</h4>
<p>
  Proposed by Ho and Salimans (2022), this method allows conditional generation without needing a separate classifier:
</p>
<ul>
  <li>Combines conditional and unconditional models to guide generation.</li>
  <li>Crucial for text-to-image systems like **Stable Diffusion** and **Imagen**.</li>
</ul>

<!-- ----- 8.3 Summary ----- -->
<h3 id="ddpm-summary">8.3 Summary</h3>
<p class="lead">
  These advancements have transformed DDPMs from a theoretical curiosity into a practical tool powering cutting-edge generative systems. Today’s diffusion models are faster, more memory-efficient, and capable of stunning image synthesis at scale.
</p>

<p class="note">
  <strong>Key Takeaway:</strong> DDPM was only the beginning. Its successors—DDIM, SGMs, LDMs, and classifier-free guidance—define the current state of generative modeling.
</p>

  <!-- ========== Section 9: Conclusions and Outlook ========== -->
<h2 id="conclusions">9. Conclusions and Outlook</h2>

<p class="lead">
  From Brownian motion to score-based modeling, we have traced the mathematical backbone of diffusion models. Starting with stochastic calculus, we derived the Fokker–Planck equation, its reverse-time SDE, and ultimately arrived at the discrete DDPM framework powering today’s generative models.
</p>

<p>
  Along the way, we saw how the elegance of Itô calculus and variational inference converge in the DDPM training objective. Our toy 2D implementation demonstrated the core principles of forward noising and reverse denoising, bridging theory with practice.
</p>

<!-- ----- 9.1 Key Takeaways ----- -->
<h3 id="key-takeaways">9.1 Key Takeaways</h3>
<ul>
  <li>Stochastic differential equations underpin the forward and reverse dynamics in diffusion models.</li>
  <li>The Fokker–Planck PDE provides a probabilistic description of density evolution.</li>
  <li>The reverse-time SDE links Anderson’s 1982 theory to modern denoising frameworks.</li>
  <li>The DDPM formulation discretizes these ideas for practical, scalable training.</li>
</ul>

<p class="note">
  <strong>Essential Insight:</strong> Diffusion models elegantly merge physics-inspired dynamics with deep learning to achieve state-of-the-art generative performance.
</p>

<!-- ----- 9.2 Looking Ahead ----- -->
<h3 id="future-work">9.2 Looking Ahead</h3>
<p>
  As the field evolves, several exciting directions lie ahead:
</p>
<ul>
  <li><strong>Faster Sampling:</strong> Reducing the number of steps from hundreds to just a few without sacrificing quality.</li>
  <li><strong>Conditional Generation:</strong> Expanding diffusion models for text, audio, and multimodal tasks.</li>
  <li><strong>Theoretical Foundations:</strong> Deeper understanding of convergence, stability, and expressiveness of diffusion processes.</li>
  <li><strong>Scalable Architectures:</strong> Leveraging latent spaces and efficient parameterizations to scale to ultra-high resolutions.</li>
</ul>

<p class="bridge">
  Diffusion models are still young but already transforming fields from image synthesis to protein folding. Their strong theoretical grounding suggests they are here to stay as a core paradigm in generative modeling.
</p>

<p class="lead">
  <em>This post completes our deep dive into the mathematics of diffusion. From here, we can explore applications, optimizations, and new theoretical frontiers.</em>
</p>

<!-- ========== Section 10: References & Further Reading ========== -->
<h2 id="references">10. References & Further Reading</h2>

<p class="lead">
  To explore diffusion models and their extensions in greater depth, here are foundational and recent papers worth reading:
</p>

<ul>
  <li>
    <strong>Denoising Diffusion Probabilistic Models</strong><br>
    J. Ho, A. Jain, P. Abbeel<br>
    <a href="https://arxiv.org/abs/2006.11239" target="_blank">
      arXiv:2006.11239
    </a>
  </li>

  <li>
    <strong>Denoising Diffusion Implicit Models</strong><br>
    J. Song, C. Meng, S. Ermon<br>
    <a href="https://arxiv.org/abs/2010.02502" target="_blank">
      arXiv:2010.02502
    </a>
  </li>

  <li>
    <strong>Score-Based Generative Modeling through Stochastic Differential Equations</strong><br>
    Y. Song, S. Sohl-Dickstein, D. Kingma, et al.<br>
    <a href="https://arxiv.org/abs/2011.13456" target="_blank">
      arXiv:2011.13456
    </a>
  </li>

  <li>
    <strong>High-Resolution Image Synthesis with Latent Diffusion Models</strong><br>
    R. Rombach, A. Blattmann, D. Lorenz, P. Esser, B. Ommer<br>
    <a href="https://arxiv.org/abs/2112.10752" target="_blank">
      arXiv:2112.10752
    </a>
  </li>

  <li>
    <strong>Improved Techniques for Training Score-Based Generative Models</strong><br>
    Y. Song, S. Sohl-Dickstein, J. Ho, et al.<br>
    <a href="https://arxiv.org/abs/2105.05233" target="_blank">
      arXiv:2105.05233
    </a>
  </li>

  <li>
    <strong>A General Framework for Score-Based Generative Modeling</strong><br>
    Y. Song, J. Ho, C. Meng, et al.<br>
    <a href="https://arxiv.org/abs/2202.00512" target="_blank">
      arXiv:2202.00512
    </a>
  </li>
</ul>

<p class="note">
  These papers expand on the mathematical foundations and showcase practical advancements in diffusion modeling.
</p>
