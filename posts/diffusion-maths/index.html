<style>
  /* Put your styles here */
  /* ----------  TL;DR call-out ---------- */
/* ----------  TL;DR call-out (greyscale theme) ---------- */
.tldr {
  border: 1px solid #c0c0c0;      /* soft grey border */
  border-radius: 0.5rem;
  padding: 1rem 1.25rem;
  background: #f5f5f5;            /* very light grey background */
  color: #333;                    /* dark grey text for readability */
}

.tldr h2 {
  margin: 0 0 0.5rem 0;
  color: #111;                    /* nearly black for the heading */
}

.tldr ol {
  margin-left: 1.3rem;
}

.tldr li strong {
  color: #000;                    /* highlight keywords in pure black */
}

/* ----------  table styling ---------- */
table.tbl{
  width:100%;
  border-collapse:collapse;
  margin:2rem 0 2.5rem 0;
  font-size:.95rem;
}
table.tbl caption{
  caption-side:top;
  font-weight:600;
  margin-bottom:.4rem;
}
table.tbl th,
table.tbl td{
  border:1px solid #d0d0d0;
  padding:.45rem .65rem;
  text-align:left;
  vertical-align:top;
}
table.tbl thead{background:#f5f7ff;}
.tbl .shape{
  font-family:"Roboto Mono",ui-monospace,monospace;
  white-space:nowrap;
}
.fact-table th{
  background:#f5f5f5;
  width:160px;
}

/* ----------  lead paragraph ---------- */
.lead{
  font-size:1.05rem;
  line-height:1.6;
  margin:1.2rem 0 2rem 0;
  color:#333;
}
.lead ul{margin:.6rem 0 .6rem 1.4rem}

/* ----------  bridge note ---------- */
.bridge{
  font-size:.95rem;
  margin:.8rem 0 1.1rem 0;
  color:#444;
}            /* <‚Äî THIS was the missing brace */

/* ----------  display equations ---------- */
.eq-scroll{
  display:block;
  overflow-x:auto;
  white-space:nowrap;
  text-align:center;
  margin:1.2rem auto;
  font-size:1.02rem;
}
@media(max-width:600px){
  .eq-scroll{font-size:.9rem}
  table.tbl{font-size:.9rem}
}

/* ----------  inline & block code ---------- */
code,pre{
  font-family:"Fira Code","SFMono-Regular",ui-monospace,monospace;
  font-size:.92rem;
}
code{
  background:#f3f4f6;
  color:#1a1a1a;
  padding:0 .25em;
  border-radius:4px;
}
pre{
  background:#f8f9fb;
  border:1px solid #cfd2d7;
  border-radius:6px;
  padding:.9rem 1rem;
  line-height:1.45;
  overflow-x:auto;
  margin:1.6rem 0;
}
pre code{background:none;padding:0}
@media(max-width:600px){
  pre{font-size:.82rem}
  code{font-size:.86rem}
}

/* ----------  GitHub-style code card ---------- */
.code-card{
  background:#f6f8fa;
  border:1px solid #d0d7de;
  border-radius:6px;
  overflow:hidden;
  margin:1.6rem 0;
}
.code-card .code-header{
  background:#eaeef2;
  border-bottom:1px solid #d0d7de;
  font:.75rem/1 system-ui,sans-serif;
  color:#24292f;
  padding:.45rem .9rem;
  text-transform:lowercase;
}
.code-card pre{
  margin:0;
  padding:.8rem 1rem;
  background:inherit;
  font-size:.92rem;
  white-space:pre;
}
@media(max-width:600px){
  .code-card pre{font-size:.82rem}
}
  
/* === let wide tables side-scroll on narrow screens === */
@media (max-width: 600px){
  table.tbl{
    display:block;          /* makes it a scroll container   */
    overflow-x:auto;        /* side-scroll if too wide        */
    -webkit-overflow-scrolling: touch;
  }
  table.tbl thead,
  table.tbl tbody{
    display:table;          /* keeps header & body aligned    */
    width:100%;
  }
  table.tbl th,
  table.tbl td{
    white-space:nowrap;     /* prevent ugly line wraps        */
  }
}

/* ----------  Responsive heading wraps & scaling ---------- */

/* Allow long words in headings to break */
h1, h2, h3, h4, h5, h6 {
  overflow-wrap: break-word;
  word-wrap: break-word;
  hyphens: auto;
  white-space: normal;       /* override any no-wrap */
}

/* Shrink heading text on narrow viewports */
@media (max-width: 600px) {
  h1 { font-size: 1.5rem; }
  h2 { font-size: 1.3rem; }
  h3 { font-size: 1.15rem; }
  /* you can add h4, h5 as needed */
}

/* Optional: make the entire page text flow better on mobile */
body {
  word-wrap: break-word;
  overflow-wrap: break-word;
}


</style>
<section>
  <h1 id="diffusion-maths-updated">
    Foundations of Score Matching: Mathematical Principles Behind Diffusion Model Training"
  </h1>

  <!-- =======================  TL;DR box ======================= -->
  <aside class="tldr">
    <h2 id="TL;DR">TL;DR</h2>
    <p>
      This post unpacks the core math that powers diffusion models, turning
      image generation into a straightforward regression problem:
    </p>
    <ol>
      <li>
        <strong>Forward noising.</strong>
        Derive a closed-form Gaussian corruption \(q(x_t\mid x_0)\)
        so you can sample \(x_t\) analytically at any noise level.
      </li>
      <li>
        <strong>Unbiased score labels.</strong>
        Use Bayes‚Äô rule to show
        \(\mathbb{E}[-\varepsilon/\sigma_t \mid x_t] = \nabla_{x_t}\log p_t(x_t)\),
        yielding a free supervised target.
      </li>
      <li>
        <strong>Score matching &amp; Hyv√§rinen.</strong>
        Integration-by-parts removes the intractable partition function
        \(Z_\theta\), yielding a loss computable via autograd.
      </li>
      <li>
        <strong>Denoising auto-encoder link.</strong>
        Vincent‚Äôs small-œÉ Taylor shows denoising MSE ‚Üí Fisher divergence,
        so each timestep is one finite-œÉ DAE objective.
      </li>
      <li>
        <strong>Numerical tricks.</strong>
        The Hutchinson trace trick makes any Laplacian
        \(O(d)\) in memory; diffusion cleverly sidesteps even that
        via the Gaussian label.
      </li>
    </ol>
    <p>
      Four lines of PyTorch‚ÄîU-Net, forward kernel, label = \(-\varepsilon/\sigma_t\),
      MSE loss‚Äîare all you need to turn noise into photorealistic images.
    </p>
  </aside>
</section>


<h2 id="notations-and-big-picture">Notations and Big Picture</h2>
  <!-- ============  Notation Cheat-Sheet & Shapes  ============ -->
<table class="tbl note-table">
  <caption>Notation Cheat-Sheet &amp; Typical Shapes</caption>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Meaning (plain language)</th>
      <th class="shape">Typical shape / note</th>
    </tr>
  </thead>

  <tbody>
    <tr><td><code>x<sub>0</sub> ‚àà ‚Ñù<sup>d</sup></code></td>
        <td>clean data vector (flattened image)</td>
        <td class="shape"><code>d = 3 H W</code></td></tr>

    <tr><td><code>t ‚àà {0,‚Ä¶,T}</code></td>
        <td>discrete diffusion timestep</td>
        <td class="shape">integer</td></tr>

    <tr><td><code>Œ±<sub>t</sub></code></td>
        <td>retain-signal factor at step <code>t</code></td>
        <td class="shape">scalar in (0,1)</td></tr>

    <tr><td><code>ùõºÃÑ<sub>t</sub> = ‚àè<sub>s=1</sub><sup>t</sup> Œ±<sub>s</sub></code></td>
        <td>cumulative signal survival</td>
        <td class="shape">scalar</td></tr>

    <tr><td><code>œÉ<sub>t</sub> = ‚àö(1-ùõºÃÑ<sub>t</sub>)</code></td>
        <td>total noise std-dev after step <code>t</code></td>
        <td class="shape">scalar</td></tr>

    <tr><td><code>Œµ ‚àº ùí©(0,I<sub>d</sub>)</code></td>
        <td>fresh isotropic Gaussian noise</td>
        <td class="shape"><code>‚Ñù<sup>d</sup></code></td></tr>

    <tr><td><code>p<sub>data</sub></code></td>
        <td>true‚Äîbut unknown‚Äîimage data density</td>
        <td class="shape">never explicit</td></tr>

    <tr><td><code>q(x<sub>t</sub> | x<sub>0</sub>)</code></td>
        <td>designer-chosen Gaussian noising kernel</td>
        <td class="shape">analytic</td></tr>

    <tr><td><code>p<sub>t</sub>(x<sub>t</sub>)</code></td>
        <td>marginal noisy density after corruption</td>
        <td class="shape">unknown but samplable</td></tr>

    <tr><td><code>s<sub>Œ∏</sub>(x<sub>t</sub>,t)</code></td>
        <td>neural network score estimator</td>
        <td class="shape">same shape as <code>x<sub>t</sub></code></td></tr>
  </tbody>
</table>
<p class="note">
  Throughout, <code>Œµ&nbsp;‚àº&nbsp;ùí©(0,&nbsp;I<sub>d</sub>)</code>.
  All inner products&nbsp;/&nbsp;norms are Euclidean.
</p>

<!-- ============  Stock-the-Toolbox Facts  ============ -->
<table class="tbl fact-table">
  <caption>Stock-the-Toolbox Facts (handy formulas)</caption>
  <tbody>
    <tr><th>Gaussian PDF</th>
        <td><code>œï<sub>Œ£</sub>(z) = exp(-¬Ω z<sup>‚ä§</sup>Œ£<sup>-1</sup>z) / ‚àö((2œÄ)<sup>d</sup> det Œ£)</code></td></tr>

    <tr><th>Quadratic gradient</th>
        <td><code>‚àá<sub>x</sub>‚Äñx-a‚Äñ<sup>2</sup> = 2 (x-a)</code></td></tr>

    <tr><th>Chain rule for logs</th>
        <td><code>‚àá log f = (‚àáf) / f</code></td></tr>

    <tr><th>Affine-Gaussian rule</th>
        <td>If <code>Z‚àºùí©(0,I)</code> and <code>Y=a+BZ</code> then <code>Y‚àºùí©(a, B B<sup>‚ä§</sup>)</code>.</td></tr>
  </tbody>
</table>
<p class="note">
  Keep these identities at hand&mdash;we&rsquo;ll cite them explicitly in the derivations below.
</p>


<!-- =====================================================
     1. Forward (Noising) Process ‚Äî From One Step to Closed Form
     ===================================================== -->
<h2 id="forward">1.&nbsp;Forward&nbsp;(Noising) Process &mdash; from a single&nbsp;noise injection to a closed-form kernel</h2>
<p class="lead">
  Imagine pressing a camera‚Äôs shutter while slowly cranking up the ISO:
  each tick makes the photo grainier until only static remains.
  The <em>forward&nbsp;process</em> in diffusion models is exactly that:
  a time-indexed recipe that <strong>systematically degrades</strong> a clean
  image into pure Gaussian noise.
  <br>
  In this section we:
  <ul>
    <li>quantify one such ‚Äúgrain-up‚Äù tick (<span class="math">\( \beta \)</span>-noise injection),</li>
    <li>show how chaining many ticks collapses into a single closed-form
        distribution, and</li>
    <li>write that distribution analytically, so its gradient becomes a
        free supervised label in later training.</li>
  </ul>
  Master these three steps and you hold the <em>entire</em> forward half of a diffusion model in your hands.
</p>
<!-- 1.1 one tiny noise injection -->
<h3 id="noise-injection">1.1&nbsp;One tiny noise injection</h3>

<p>
  Think of the forward process as a photographic fade-out:
  each step <em>shrinks</em> the original signal and sprinkles in fresh Gaussian static.  
  A blend parameter&nbsp;\( \beta\in(0,1) \) tells us
  exactly how much signal versus noise we keep.
</p>

<div class="eq-scroll">\[
  x' \;=\; \sqrt{\,1-\beta\,}\;x
        \;+\; \sqrt{\beta}\;\varepsilon,
  \qquad
  \varepsilon \sim \mathcal N(0,I_d).
\tag{1.1}\]</div>

<p>
  The square-root factors guarantee the variance of \(x'\) is
  <span class="math">\( \beta \)</span> higher than that of \(x\):
  the signal‚Äôs power is scaled by \(1-\beta\), the noise‚Äôs by \(\beta\).
</p>

<!-- 1.2 repeat t times -->
<h3 id="repeat-steps">1.2&nbsp;Repeat <em>t</em> steps &rarr; one closed-form jump</h3>

<p>
  Apply (1.1) successively with variances
  \( \beta_1,\beta_2,\dots,\beta_t \) &nbsp;
  \(\bigl(\alpha_s := 1-\beta_s\bigr)\).
  Induction collapses the chain into one affine transform:
</p>

<div class="eq-scroll">\[
  x_t \;=\; \sqrt{\bar\alpha_t}\,x_0
        \;+\; \sqrt{1-\bar\alpha_t}\,\varepsilon,
  \qquad
  \bar\alpha_t := \prod_{s=1}^{t}\alpha_s .
\tag{1.2}\]</div>

<p>
  In words: after \(t\) steps a fixed share
  \( \sqrt{\bar\alpha_t} \) of the original image survives; the rest is
  i.i.d.&nbsp;Gaussian noise with total standard deviation
  \( \sigma_t := \sqrt{1-\bar\alpha_t} \).
</p>

<!-- ================= Figure 1: Forward Noising ================= -->
<figure>
  <img src="posts/diffusion-maths/assets/figure1.png"
       alt="Clean vs Noisy ring dataset"
       style="max-width:100%;height:auto;">
  <figcaption>
    <strong>Figure 1.</strong>
    This section showed a 2D ring before and after applying
    the forward kernel \(x_t = \sqrt{\bar\alpha_t}x_0 + \sigma_t\varepsilon\) with œÉ = 1.
  </figcaption>
</figure>

<!-- 1.3 conditional kernel -->
<h3 id="conditional-density">1.3&nbsp;Conditional density \( q(x_t \mid x_0) \)</h3>

<p>
  Any affine map of a Gaussian remains Gaussian, so conditioning on the
  clean image \(x_0\) we obtain an <em>analytic</em> forward kernel:
</p>

<div class="eq-scroll">\[
  q(x_t \mid x_0)
  \;=\;
  \mathcal N\!\bigl(
      x_t;
      \sqrt{\bar\alpha_t}\,x_0,\;
      \sigma_t^2 I_d
  \bigr).
\tag{1.3}\]</div>

<p>
  Equation&nbsp;(1.3) is a workhorse: its gradient will give us a
  <em>free</em> supervised label when we train the score network.
</p>

<!-- =====================================================
     2. Why We Need the Score ‚Äî and How to Get It for Free
     ===================================================== -->
<h2 id="score">2.&nbsp;Why We Need the&nbsp;Score&nbsp;&mdash; and&nbsp;How&nbsp;to&nbsp;Get&nbsp;It for&nbsp;Free</h2>

<!-- ---------- lead-in ---------- -->
<p class="lead">
  Training a diffusion model is not about matching full probability
  densities &mdash; that would require intractable normalising constants.
  Instead, we learn the <em>score&nbsp;field</em>:
  the gradient of the log-density, which points toward higher probability
  exactly like a small compass needle.  Remarkably, the Gaussian forward
  kernel we just derived lets us fabricate <strong>perfect ground-truth
  score labels</strong> on the fly, at <em>every</em> noise level.
</p>

<!-- bridge: why a quadratic appears in log q -->
<p class="bridge">
  <strong>Where does that quadratic originate?</strong><br>
  The conditional kernel in Eq.&nbsp;(1.3) is a multivariate Gaussian,
  so its probability density function is
  \[
     q(x_t \mid x_0)
     = \frac{\exp\!\bigl(
         -\tfrac12
         \tfrac{\|x_t-\sqrt{\bar\alpha_t}\,x_0\|^2}
               {\sigma_t^2}
       \bigr)}
       {(2\pi)^{d/2}\sigma_t^{\,d}} .
  \]
  When we take <em>log</em> of that PDF, every constant goes to an additive
  ‚Äú<code>const</code>,‚Äù and the exponent‚Äôs
  squared-distance term
  \(
     \|x_t-\sqrt{\bar\alpha_t}x_0\|^2
  \)
  is the <em>quadratic</em> we differentiate.
  Substituting
  \( x_t = \sqrt{\bar\alpha_t}\,x_0 + \sqrt{1-\bar\alpha_t}\,\varepsilon \)
  from Eq.&nbsp;(1.2) makes the difference
  \( x_t-\sqrt{\bar\alpha_t}x_0 \)
  collapse neatly to
  \( \sqrt{1-\bar\alpha_t}\,\varepsilon \).
  Divide by \( \sigma_t^2 = 1-\bar\alpha_t \),
  apply the gradient of a norm-square
  \( \nabla_x\|x\|^2 = 2x \),
  and you land on the compact
  \(
     -\,\varepsilon/\sigma_t
  \)
  result shown next.
</p>
<!-- 2.1 What is a score? -->
<h3 id="what-is-score">2.1&nbsp;What is a score?</h3>
<p>
  For any probability density \( p(z) \) the&nbsp;
  <em>score</em> is defined as
  \( s(z) := \nabla_z \log p(z) \).
  It is the vector pointing in the direction
  the density rises fastest&mdash;a gradient ascent arrow in probability
  space.
</p>

<!-- 2.2 Gradient of the Gaussian kernel -->
<h3 id="graodent-of-gaussian">2.2&nbsp;Gradient of the Gaussian&nbsp;kernel (one-liner!)</h3>
<!-- quick recap of x_t and alpha-bar_t -->
<p class="bridge">
  <strong>Recall the forward step.</strong>&nbsp;
  From Eq.&nbsp;(1.2) we have
  \( x_t = \sqrt{\bar\alpha_t}\,x_0 + \sqrt{1-\bar\alpha_t}\,\varepsilon \)
  with the cumulative factor
  \( \displaystyle \bar\alpha_t = \prod_{s=1}^{t}\alpha_s \).
  <br>
  Insert that expression into the quadratic
  \( \|x_t-\sqrt{\bar\alpha_t}x_0\|^2 \)
  and notice the difference term simplifies to
  \( \sqrt{1-\bar\alpha_t}\,\varepsilon \).
  Dividing by \(1-\bar\alpha_t\) and taking the gradient yields the tidy
  ‚Äúnegative&nbsp;noise over&nbsp;\(\sigma_t\)‚Äù formula below.
</p>
<p>
  Take the log of the forward kernel \( q(x_t \mid x_0) \) from&nbsp;(1.3)
  and differentiate with respect to \( x_t \).  A single line of algebra
  (using the quadratic-gradient rule) gives
</p>

<div class="eq-scroll">\[
  \nabla_{x_t}\log q(x_t\mid x_0)
  \;=\;
  -\frac{x_t - \sqrt{\bar\alpha_t}\,x_0}{1-\bar\alpha_t}
  \;=\;
  -\frac{\varepsilon}{\sigma_t},
  \qquad
  \bigl(\sigma_t^2 = 1-\bar\alpha_t\bigr).
\tag{2.1}\]</div>

<p class="explain">
  Beautifully simple: the score of the <em>conditional</em> Gaussian is
  just &ldquo;negative noise divided by&nbsp;\(\sigma_t\).&rdquo;
</p>

<!-- ================= Figure 2: Score Field ================= -->
<figure>
  <img src="posts/diffusion-maths/assets/figure2.png"
       alt="True score field vector plot"
       style="max-width:100%;height:auto;">
  <figcaption>
    <strong>Figure 2.</strong>
    This section demonstrated the true marginal score
    field \(\nabla_{x_t}\log p_t(x_t)\) on the noisy ring, visualized as red arrows.
  </figcaption>
</figure>

<!-- 2.3 Training label inside each mini-batch -->
<h3 id="train-label">2.3&nbsp;Training label in every mini-batch</h3>
<p>
  During training we <em>know</em> the exact noise
  \( \varepsilon \) we injected when we built \( x_t \).
  Hence we can compute a <strong>free, per-image, per-timestep label</strong>
</p>

<div class="eq-scroll">\[
  y_t := -\,\varepsilon / \sigma_t.
\tag{2.2}\]</div>

<p class="explain">
  No approximation, no Monte-Carlo &mdash; \( y_t \) drops out of thin air
  for every example in the batch.  All that remains is to prove this label
  is an <em>unbiased</em> estimator of the true, unknown
  marginal score \( \nabla_{x_t}\log p_t(x_t) \);
  we do that in the next subsection using a three-line Bayes trick.
</p>

<!-- 2.4 the marginal score we really want -->
<h3 id="marginal-score">2.4&nbsp;The marginal score we really want</h3>

<p class="bridge">
  The label \( y_t \) we just fabricated is derived from the
  <em>conditional</em> density \( q(x_t\mid x_0) \),
  but generation will ultimately need the score of the
  <strong>marginal</strong> noisy distribution&nbsp;
  \( p_t(x_t) \).
  Let‚Äôs write that object explicitly.
</p>

<div class="eq-scroll">\[
  s^{\star}(x_t,t) \;:=\;
  \nabla_{x_t}\log p_t(x_t),
  \qquad
  p_t(x_t)
  \;=\;
  \int p_{\text{data}}(x_0)\,
       q(x_t \mid x_0)\,
       dx_0.
\tag{2.3}\]</div>

<p class="explain">
  Sadly, \( p_t(x_t) \) has no closed form because
  \( p_{\text{data}} \) is unknown.
  The magic of diffusion is that the label
  \( y_t = -\varepsilon / \sigma_t \)
  turns out to be an <em>unbiased</em> estimator of this intractable
  score.  We prove that with a three-line Bayes trick in the next
  subsection.
</p>

<!-- 2.5 Three-line Bayes proof ‚Äî y_t is an unbiased target -->
<h3 id="three-liner-proof">2.5&nbsp;Three-line&nbsp;Bayes proof ‚Äî <code>y_t</code> is an unbiased target</h3>

<!-- intro sentence -->
<p class="bridge">
  We need to prove
  \( \mathbb E\!\bigl[\,y_t \mid x_t\bigr] = \nabla_{x_t}\log p_t(x_t) \).
  The derivation uses only the chain rule, a gradient-under-the-integral swap,
  and Bayes‚Äô rule.
</p>

<!-- ---- LINE 1 ---- -->
<p><strong>Line&nbsp;1&nbsp;&mdash; chain rule for logs</strong></p>
<div class="eq-scroll">\[
  \nabla_{x_t}\log p_t(x_t)
  \;=\;
  \frac{\nabla_{x_t}p_t(x_t)}{p_t(x_t)} .
\tag{2.4a}\]</div>

<!-- ---- LINE 2 ---- -->
<p><strong>Line&nbsp;2&nbsp;&mdash; move ‚àá under the integral</strong><br>
  (dominated-convergence applies because the Gaussian kernel has bounded
  derivatives):
</p>
<div class="eq-scroll">\[
  \nabla_{x_t}p_t(x_t)
  \;=\;
  \int p_{\text{data}}(x_0)\,
       \nabla_{x_t}q(x_t\mid x_0)\,dx_0 .
\tag{2.4b}\]</div>

<!-- ---- LINE 3 ---- -->
<p><strong>Line&nbsp;3&nbsp;&mdash; split the gradient &amp; recognise the posterior</strong></p>
<ul>
  <li><em>Split the gradient:</em>
      \( \nabla_{x_t}q = q\,\nabla_{x_t}\log q \).  
      Insert the one-liner from&nbsp;(2.1),
      \( \nabla_{x_t}\log q = -\,\varepsilon/\sigma_t \), which is exactly
      \( y_t \).</li>

  <li><em>Bayes‚Äô identity:</em>
      \( p_{\text{data}}(x_0)\,q(x_t\mid x_0) / p_t(x_t)
         = p(x_0\mid x_t) \).</li>
</ul>

<div class="eq-scroll">\[
  \frac{1}{p_t(x_t)}
  \int p_{\text{data}}(x_0)\,
       q(x_t\mid x_0)\,
       \Bigl(-\tfrac{\varepsilon}{\sigma_t}\Bigr)\,dx_0
  \;=\;
  \mathbb E_{x_0\sim p(\,\cdot\mid x_t)}[\,y_t\,] .
\tag{2.4c}\]</div>

<!-- summary -->
<p class="eq-scroll">
  Combine (2.4a)‚Äì(2.4c):
  \[
    \boxed{\;
      \mathbb E\!\bigl[\,y_t \mid x_t\bigr]
      \;=\;
      \nabla_{x_t}\log p_t(x_t)
    \;}. \tag{2.5}
  \]
  Taking an outer expectation over \(x_t\) preserves the equality, so
  swapping the unknown score with \(y_t\) inside our loss
  introduces <em>no bias whatsoever</em>.
</p>

<!-- =====================================================
     3. From Fisher Divergence to a One-Line PyTorch Loss
     ===================================================== -->
<h2 id="loss">3.&nbsp;From Fisher Divergence&nbsp;to a One-Line&nbsp;PyTorch&nbsp;Loss</h2>

<p class="lead">
  With an unbiased label \(y_t\) in hand, we can rewrite the
  Fisher-divergence objective so that <em>every unknown quantity drops
  out</em>.  The result is the single MSE line that powers all modern
  diffusion code-bases.
</p>

<p class="bridge">
  <strong>A 30-second Fisher‚Äìdivergence refresher.</strong><br>
  Whereas maximum likelihood measures the gap between two densities by
  comparing their <em>heights</em>, the Fisher divergence compares their
  <em>slopes</em>:
  \[
    D_F\bigl(p_{\text{data}}\|p_\theta\bigr)
    \;=\;
    \tfrac12
    \int p_{\text{data}}(x)\,
         \bigl\|
           \nabla_x \log p_\theta(x)
           -
           \nabla_x \log p_{\text{data}}(x)
         \bigr\|^{2}\,dx .
  \]
  Matching slopes is golden for diffusion models because
  the gradient of a log-density
  \( \nabla_x\log p_\theta(x) \)
  <em>does not depend</em> on the intractable normalising constant
  \( Z_\theta \).
  By driving this slope-gap to zero, we avoid the partition-function
  bottleneck altogether and still push the model toward the true data
  manifold.
</p>


<!-- 3.1 one-timestep Fisher divergence -->
<h3>3.1&nbsp;Single-timestep Fisher divergence</h3>

<p class="bridge">
  We now translate the Fisher divergence primer into a concrete loss.
  Because diffusion training iterates over noise levels, it is natural to
  start with <em>one</em> timestep&nbsp;<code>t</code>.
  The single-step objective below measures how far the network‚Äôs score
  \( s_\theta(x_t,t) \) is from the <em>true</em> marginal score
  \( \nabla_{x_t}\log p_t(x_t) \).  We will immediately swap the latter
  for its unbiased label \( y_t \), but writing it once in full keeps the
  statistical grounding clear.
</p>


<div class="eq-scroll">\[
  \mathcal L_t(\theta)
  \;=\;
  \tfrac12\,
  \mathbb E_{p_t}
    \bigl\|
      s_\theta(x_t,t)
      -\nabla_{x_t}\log p_t(x_t)
    \bigr\|^{2}.
\tag{3.1}\]</div>

<!-- 3.2 swap score for label -->
<h3>3.2&nbsp;Swap the unknown score for its label</h3>

<p class="bridge">
  Using the equality
  \( \mathbb E[\,y_t \mid x_t] = \nabla_{x_t}\log p_t(x_t) \)
  proved in Section 2.5, we can replace the intractable score by
  \( y_t = -\varepsilon/\sigma_t \):
</p>

<div class="eq-scroll">\[
  \mathcal L_t(\theta)
  \;=\;
  \tfrac12\,
  \mathbb E_{x_0,\varepsilon}
    \bigl\|
      s_\theta(x_t,t)
      +\tfrac{\varepsilon}{\sigma_t}
    \bigr\|^{2}.
\tag{3.2}\]</div>

<!-- 3.3 aggregate over timesteps -->
<h3>3.3&nbsp;Aggregate over all timesteps</h3>

<p>
  Sample \(t\) uniformly (or with any weighting schedule) to get the full
  training objective:
</p>

<div class="eq-scroll">\[
  \boxed{%
  \mathcal L(\theta)
  \;=\;
  \mathbb E_{\,t,x_0,\varepsilon}
    \bigl\|
      s_\theta(x_t,t)
      +\tfrac{\varepsilon}{\sigma_t}
    \bigr\|^{2}
  }.
\tag{3.3}\]</div>

<p class="explain">
  All expectations are taken over quantities we <em>sample directly</em>
  (data, noise, timestep), so the loss is fully computable.
</p>

<!-- 3.4 predict-noise variant -->
<h3>3.4&nbsp;Predict-noise variant &amp; PyTorch one-liner</h3>

<p>
  Many implementations train the network to output raw noise rather than
  the score.  Define
  \( \hat\varepsilon_\theta := -\sigma_t\,s_\theta(x_t,t) \); then
  Eq.&nbsp;(3.3) becomes
</p>

<div class="eq-scroll">\[
  \mathcal L(\theta)
  \;=\;
  \mathbb E_{\,t,x_0,\varepsilon}
    \bigl\|
      \hat\varepsilon_\theta - \varepsilon
    \bigr\|^{2}.
\tag{3.4}\]</div>

<p class="explain">
  In PyTorch that reduces to the canonical&nbsp;one-liner:
</p>

<div class="code-card">
  <div class="code-header">python</div>
  <pre><code>loss = F.mse_loss(pred_eps, eps)</code></pre>
</div>


<!-- =====================================================
     4. Hyv√§rinen Score Matching ‚Äî Full Integration-by-Parts Walk-Through
     ===================================================== -->
<h2 id="hyvarinen">4.&nbsp;Hyv√§rinen&nbsp;Score&nbsp;Matching &mdash; Integration-by-Parts Walk-Through</h2>

<p class="lead">
  Fisher divergence lets us compare two probability densities without
  ever touching their normalising constants.
  That‚Äôs perfect for energy-based models, whose partition
  functions are hopeless in image dimensions.
  The catch?  Fisher divergence still contains the unknown
  <em>data&nbsp;score</em>
  \( \nabla_x\log p_{\text{data}} \).
  Hyv√§rinen‚Äôs 2005 trick uses a single integration-by-parts move
  to <strong>eliminate</strong> that data-score,
  leaving a loss we can evaluate with automatic differentiation alone.
</p>

<!-- 4.1 why we need it -->
<h3 id="hyvarinen-why">4.1&nbsp;Why we need it</h3>
<p>
  We want a discrepancy measure
  \( D_F(p_{\text{data}}\Vert p_\theta) \)
  that:
</p>
<ul>
  <li>does <em>not</em> require the unknown
      \( p_{\text{data}} \) in closed form, and</li>
  <li>does <em>not</em> require the
      partition function \( Z_\theta \) of the model.</li>
</ul>

<div class="eq-scroll">\[
  D_F(p_{\text{data}}\Vert p_\theta)
  \;=\;
  \tfrac12
  \int p_{\text{data}}(x)\,
       \bigl\|
         \nabla_x \log p_\theta(x)
         - \nabla_x \log p_{\text{data}}(x)
       \bigr\|^{2}\,dx .
\]</div>

<p class="bridge">
  Fisher divergence solves the
  <em>partition-function</em> problem
  (because derivatives kill constants),
  but it still contains the inaccessible
  data-score
  \( \nabla_x\log p_{\text{data}} \).
  Hyv√§rinen‚Äôs idea:
  convert that term into a <em>Laplacian</em>
  of the model‚Äôs log-density, using the divergence theorem.
</p>

<!-- 4.2 step-by-step derivation -->
<h3 id="hyvarinen-derivation">4.2&nbsp;Step-by-step derivation</h3>

<!-- (a) expand the square -->
<p><strong>Step&nbsp;1 &mdash; expand the squared norm</strong></p>
<div class="eq-scroll">\[
  \|a-b\|^2
  =\|a\|^2+\|b\|^2-2\,a\!\cdot\!b .
\]</div>

<p>
  Let
  \( a=\nabla_x\log p_\theta \),
  \( b=\nabla_x\log p_{\text{data}} \).
  Plugging into the divergence gives
</p>

<div class="eq-scroll">\[
  J(\theta)
  =\tfrac12
     \int p_{\text{data}}\|a\|^2dx
  -\!\!
     \int p_{\text{data}}\,a\!\cdot\!b\,dx
  +\text{const}.
  \tag{4.1}
\]</div>

<p class="explain">
  The const term (from
  \( \|b\|^2 \))
  has no&nbsp;\( \theta \); drop it for optimisation.
</p>

<!-- (b) rewrite the cross-term -->
<p><strong>Step&nbsp;2 &mdash; rewrite the cross-term</strong></p>
<p>
  Because
  \( b=\nabla_x\log p_{\text{data}} \),
  we have
  \( p_{\text{data}}\,b = \nabla_x p_{\text{data}} \).
  Thus
</p>

<div class="eq-scroll">\[
  C(\theta)
  :=\int p_{\text{data}}\,a\!\cdot\!b\,dx
  =\int \nabla_x p_{\text{data}}\,\cdot\,a\,dx .
  \tag{4.2}
\]</div>

<!-- (c) integration by parts -->
<p><strong>Step&nbsp;3 &mdash; integration-by-parts in
  \( \mathbb R^{d} \)</strong></p>
<p>
  For a smooth vector field \(g\) that decays faster than
  \(1/\|x\|^{d+1}\),
</p>

<div class="eq-scroll">\[
  \int_{\mathbb R^{d}}\!\nabla_x\!\cdot g(x)\,dx = 0 .
\]</div>

<p>
  Choose
  \( g(x)=p_{\text{data}}(x)\,a(x) \).
  Compute its divergence:
</p>

<div class="eq-scroll">\[
  \nabla_x\!\cdot g
  = \nabla_x p_{\text{data}}\!\cdot a
    + p_{\text{data}}\,
      \underbrace{\nabla_x\!\cdot a}_{=\nabla_x^{2}\log p_\theta}.
\]</div>

<p>
  Because the integral of the divergence is zero, rearrange:
</p>

<div class="eq-scroll">\[
  \int \nabla_x p_{\text{data}}\!\cdot a\,dx
  = -\int p_{\text{data}}\,
           \nabla_x^{2}\!\log p_\theta\,dx .
  \tag{4.3}
\]</div>

<!-- (d) plug back -->
<p><strong>Step&nbsp;4 &mdash; plug (4.3) into&nbsp;(4.1)</strong></p>

<div class="eq-scroll">\[
  \boxed{
  J(\theta)=
  \int p_{\text{data}}
  \Bigl[
    \tfrac12
      \bigl\|\nabla_x\log p_\theta\bigr\|^2
    \;+\;
      \nabla_x^{2}\log p_\theta
  \Bigr]dx
  +\text{const}.
  }
  \tag{4.4}
\]</div>

<p class="explain">
  Every term now depends <em>only</em> on the model
  \( p_\theta \).
  Because
  \( p_\theta(x) = e^{-E_\theta(x)}/Z_\theta \),
  derivatives of \( \log p_\theta \)
  involve <strong>no partition function</strong>:
  \( \nabla_x\log p_\theta = -\nabla_xE_\theta \).
  Minimising \( J(\theta) \) is therefore equivalent to minimising the
  original Fisher divergence, but <em>computable</em> with autograd.
</p>

<!-- summary takeaway -->
<h3 id="hyvarinen-takeaway">4.3&nbsp;What to remember</h3>
<ul>
  <li>The data-score term was replaced by a Laplacian of
      \( \log p_\theta \) via a single divergence-theorem step.</li>
  <li>No normalising constant \( Z_\theta \) survives; score matching
      avoids the partition-function bottleneck entirely.</li>
  <li>Implementations often estimate the Laplacian with the Hutchinson
      trick (covered next), but diffusion training with a
      <em>Gaussian label</em> cleverly sidesteps even that.</li>
</ul>


<!-- =====================================================
     5. Denoising Auto-Encoder ‚áí Fisher Divergence (Vincent 2011)
     ===================================================== -->
<h2 id="vincent">5.&nbsp;Denoising&nbsp;Auto-Encoder&nbsp;‚áí Fisher&nbsp;Divergence&nbsp;(Vincent 2011)</h2>

<p class="lead">
  Before diffusion took off, Vincent (2011) proved that a denoising
  auto-encoder (DAE) trained on <em>tiny</em> Gaussian noise is‚Äîup to
  second order‚Äîidentical to score matching.  Diffusion models keep the
  same mathematics but swap ‚Äútiny noise‚Äù for a full <strong>noise
  schedule</strong>.  Each timestep is one finite-œÉ DAE loss;
  the U-Net is shared (amortised) across all timesteps.
</p>

<!-- 5.1 set-up -->
<h3 id="dae-setup">5.1&nbsp;Set-up</h3>
<p>
  Corrupt a clean sample <code>x</code> with Gaussian noise
  \( \varepsilon\sim\mathcal N(0,\sigma^{2}I) \):
</p>

<div class="eq-scroll">\[
  x' = x + \varepsilon,
  \qquad
  \varepsilon \sim \mathcal N(0,\sigma^{2}I).
\]</div>

<p>
  Train a decoder \( f_\theta \) to predict the clean signal under the
  scaled MSE objective
</p>

<div class="eq-scroll">\[
  L_{\sigma}(\theta)
  = \frac{1}{\sigma^{2}}\,
    \mathbb E_{p_{\text{data}},\varepsilon}
      \bigl\|
        x - f_\theta(x')
      \bigr\|^{2}.
  \tag{5.1}
\]</div>

<!-- 5.2 optimal decoder -->
<h3 id="dae-optimal">5.2&nbsp;Optimal decoder ‚Äì line-by-line calculus</h3>

<p>
  Fix one noisy input \(x'\) and treat
  \(f(x') \in \mathbb R^{d}\) as an unknown vector. 
  <em>(We write&nbsp;\(f^{\star}\) for the decoder that minimises the
  inner loss‚Äîthink of it as the ‚Äúoracle‚Äù decoder, in contrast to the real
  network&nbsp;\(f_\theta\).)</em>
  The inner loss (5.1) becomes the quadratic
</p>


<div class="eq-scroll">\[
  R_{x'}(f)
  \;:=\;
  \mathbb E_{x\mid x'}\!
    \bigl\|
      x - f(x')
    \bigr\|^{2}.
\]</div>

<p class="bridge">
  <strong>Take the gradient&nbsp;w.r.t.&nbsp;<code>f(x')</code></strong>
  (remember \(x'\) is held constant inside this expectation):
</p>

<div class="eq-scroll">\[
  \nabla_{f(x')} R_{x'}(f)
  = -2\,
    \mathbb E_{x\mid x'}
      \bigl[
        x - f(x')
      \bigr].
\]</div>

<p>
  <strong>Set the derivative to zero</strong>‚Äîthat kills the factor 2 and
  yields
</p>

<div class="eq-scroll">\[
  \mathbb E_{x\mid x'}[\,x\,] \;=\; f^{\star}(x').
\tag{5.2}
\]</div>

<p class="explain">
  In words: the decoder that minimises the mean-squared reconstruction
  error for this particular noisy input is simply the
  <em>conditional mean</em> of the clean sample given that input.
  This is the classic Bayesian MMSE estimator.
</p>

<!-- 5.3 small-œÉ Taylor -->
<h3 id="dae-taylor">5.3&nbsp;Small-œÉ Taylor expansion</h3>
<p>
  Perform a second-order Taylor expansion of
  \( f^{\star}(x') \) w.r.t.&nbsp;\( \sigma \) (Vincent 2011, Prop.&nbsp;1):
</p>

<div class="eq-scroll">\[
  f^{\star}(x')
  = x'
    + \sigma^{2}\,
      \nabla_{x'}\log p_{\text{data}}(x')
    + O(\sigma^{4}).
  \tag{5.3}
\]</div>

<p>
  Substitute (5.3) back into the loss (5.1) and keep terms up to
  \( O(\sigma^{2}) \):
</p>

<div class="eq-scroll">\[
  L_{\sigma}(\theta)
  = \text{const}
    + \sigma^{2}\!
      \mathbb E_{p_{\text{data}}}
        \bigl\|
          \nabla\log p_\theta
          - \nabla\log p_{\text{data}}
        \bigr\|^{2}
    + O(\sigma^{4}).
  \tag{5.4}
\]</div>

<p class="explain">
  Divide by \( \sigma^{2} \) and let \( \sigma \to 0 \):
  the leading term is precisely
  the Fisher divergence
  \( D_F(p_{\text{data}}\Vert p_\theta) \).
  <strong>Conclusion:</strong>
  minimising the DAE loss as \( \sigma \) vanishes
  converges to score matching.
</p>

<!-- ================= Figure 3: Denoising Illustration ================= -->
<figure>
  <img src="posts/diffusion-maths/assets/figure3.png"
       alt="Denoising autoencoder static example"
       style="max-width:100%;height:auto;">
  <figcaption>
    <strong>Figure 3.</strong>
    This section demonstrated the DAE training
    on one noise level‚Äî(left) clean signal, (middle) noisy input, (right) Gaussian-denoised output‚Äî
    recovering \(x\approx f^{\star}(x')\).
  </figcaption>
</figure>
  
<!-- 5.4 zoom-out to diffusion -->
<h3 id="dae-diffusion-insight">5.4&nbsp;How this becomes diffusion training</h3>

<table class="tbl">
  <thead><tr><th>Finite-œÉ DAE ingredient</th><th>Diffusion analogue</th></tr></thead>
  <tbody>
    <tr><td>One fixed noise level \( \sigma \)</td>
        <td>A timetable \( \{\sigma_t\}_{t=1}^{T} \) (<code>Œ≤</code>-schedule)</td></tr>

    <tr><td>Decoder \( f_\theta(x') \)</td>
        <td>U-Net score head \( s_\theta(x_t,t) \)  
            <br>(or Œµ-prediction \( \hat\varepsilon_\theta \))</td></tr>

    <tr><td>Loss \( \|x - f_\theta(x')\|^{2}/\sigma^{2} \)</td>
        <td>Loss \( \bigl\|s_\theta(x_t,t)+\varepsilon/\sigma_t\bigr\|^{2} \)  
            (<em>one</em> such term per timestep)</td></tr>

    <tr><td>Separate network per œÉ (if you changed œÉ)</td>
        <td><strong>Shared</strong> network; timestep embedded as an input</td></tr>
  </tbody>
</table>

<p class="bridge">
  Training therefore amounts to <em>stacking Vincent DAEs</em> across a
  ladder of noise levels, with a single U-Net that
  learns to denoise <em>all</em> of them simultaneously.
</p>

<ul>
  <li>Large œÉ<sub>t</sub> timesteps teach the network about global
      structure (big gradients, low-detail context).</li>
  <li>Small œÉ<sub>t</sub> timesteps refine fine textures and
      edges &mdash; they approximate the
      \(\sigma\!\to\!0\) Fisher-divergence regime.</li>
  <li>The aggregate loss
      \( \sum_{t=1}^{T}\mathcal L_t(\theta) \)
      is a weighted sum of Vincent objectives,
      giving a <strong>multi-scale</strong> estimator of the
      true score field.</li>
</ul>

<p class="explain">
  In short: <em>every</em> diffusion timestep is a Vincent DAE with its
  own œÉ<sub>t</sub>; sharing parameters across those DAEs and sweeping œÉ
  from large ‚Üí small is what lets diffusion models learn an accurate
  score field for the entire data manifold.
</p>


<!-- =====================================================
     6. Hutchinson Trace Trick ‚Äî Computing the Laplacian Cheaply
     ===================================================== -->
<h2 id="hutchinson">6.&nbsp;Hutchinson&nbsp;Trace&nbsp;Trick&nbsp;&mdash;&nbsp;Computing the Laplacian Cheaply</h2>

<p class="lead">
  Hyv√§rinen‚Äôs objective contains a <em>Laplacian term</em>
  \( \operatorname{tr}\!\bigl[\nabla_x^{2}\log p_\theta(x)\bigr] \).
  For images the dimensionality is enormous
  (\(d \!\gt\! 10^{5}\)),
  so a full Hessian is infeasible
  (\(O(d^{\,2})\) memory and time).
  The Hutchinson trick replaces that trace with two reverse-mode gradients
  &mdash; dropping the cost to \(O(d)\).
</p>

<!-- 6.1 key identity -->
<h3 id="hutchinson-identity">6.1&nbsp;Key identity</h3>

<p>
  For any matrix&nbsp;<code>H</code> and any random vector&nbsp;<code>v</code>
  satisfying
  \( \mathbb E\bigl[vv^{\!\top}\bigr]=I \)
  (Gaussian \( \mathcal N(0,I) \) or Rademacher ¬±1),
</p>

<div class="eq-scroll">\[
  \boxed{%
  \operatorname{tr}(H)
  \;=\;
  \mathbb E_{v}\bigl[v^{\!\top} H\,v\bigr].
  }
  \tag{6.1}
\]</div>

<p class="explain">
  Proof: \( \mathbb E[v_i v_j] = \delta_{ij} \) so only the diagonal
  elements of&nbsp;H survive the expectation.
</p>

<!-- ================= Figure 4: Hutchinson Trace Trick ================= -->
<figure>
  <img src="posts/diffusion-maths/assets/figure4.png"
       alt="Hutchinson trace trick illustration"
       style="max-width:100%;height:auto;">
  <figcaption>
    <strong>Figure 4.</strong>
    This illustrated the Hutchinson estimator
    \(\mathrm{tr}(H)=\mathbb{E}_v[v^\top H v]\) with a Hessian heatmap, probe vector, and scalar result.
  </figcaption>
</figure>

<!-- 6.2 autograd recipe -->
<h3 id="hutchinson-autograd">6.2&nbsp;Autograd recipe&nbsp;(PyTorch)</h3>

<div class="code-card">
  <div class="code-header">python</div>
  <pre><code>v  = torch.randn_like(x)                # Gaussian probe  (‚Ñù·µà)
g  = torch.autograd.grad(
        logp, x, create_graph=True)[0]  # ‚àá‚Çì log pŒ∏        (‚Ñù·µà)
dot = (g * v).sum()                     # v·µÄ g            (scalar)
Hv  = torch.autograd.grad(
        dot, x, create_graph=True)[0]   # Hessian¬∑v        (‚Ñù·µà)
trace_est = (Hv * v).sum()              # v·µÄ H v ‚âà tr(H)
</code></pre>
</div>

<ul>
  <li>Two reverse-mode grads ‚áí <strong>O(d)</strong> memory/time.</li>
  <li>Single probe is unbiased; use 2‚Äì4 probes to lower variance if
      you have GPU headroom.</li>
</ul>

<!-- 6.3 when diffusion skips it -->
<h3 id="hutchinson-diffusion">6.3&nbsp;Why diffusion rarely needs it</h3>
<p>
  In the Gaussian-label loss
  \( \bigl\|s_\theta(x_t,t)+\varepsilon/\sigma_t\bigr\|^{2} \)
  (Section&nbsp;3),
  the Laplacian term from Hyv√§rinen‚Äôs objective
  <em>cancelled out</em>.
  That‚Äôs why most diffusion repos never call Hutchinson.
  However, if you explore
  plain score-matching energies, or
  contrastive score estimation,
  Hutchinson is indispensable.
</p>

<p class="bridge">
  <strong>Take-away.</strong>&nbsp;
  The trace trick lets you scale second-order score-matching objectives
  to megapixel images with only two autograd passes,
  keeping the computational footprint on par with ordinary back-prop.
</p>

  <!-- =====================================================
     7. Why Killing the Partition Function ZŒ∏ Is Essential
     ===================================================== -->
<h2 id="partition">7.&nbsp;Why&nbsp;Killing&nbsp;the&nbsp;Partition&nbsp;Function&nbsp;<em>Z</em><sub>Œ∏</sub>&nbsp;Is&nbsp;Essential</h2>

<p class="lead">
  Maximum-likelihood training of deep energy models is crippled by the
  partition function&nbsp;\(Z_\theta\).
  Score matching sidesteps it entirely, turning a hopeless high-dim problem
  into an ordinary regression task.
</p>

<!-- 7.1 the obstacles table -->
<table class="tbl">
  <caption>What makes <em>Z</em><sub>Œ∏</sub> impossible in images?</caption>
  <thead>
    <tr><th>Obstacle</th><th>Concrete scale for 256 √ó 256 RGB</th></tr>
  </thead>
  <tbody>
    <tr><td><strong>Integral dimension</strong></td>
        <td>\(d = 3 \times 256 \times 256 = 196{,}608\).  
            A Monte-Carlo normaliser would need &raquo; 10<sup>6</sup>
            samples &mdash; <em>per gradient step</em>.</td></tr>

    <tr><td><strong>MCMC mixing</strong></td>
        <td>Deep energy landscapes are multi-modal; Langevin/Hamiltonian
            chains can take minutes or hours of burn-in <em>per image</em>.</td></tr>

    <tr><td><strong>Non-factorised likelihood</strong></td>
        <td>Pixels are globally coupled; there is no tractable
            autoregressive or factorial decomposition of the partition function.</td></tr>
  </tbody>
</table>

<!-- 7.2 score matching amputates ZŒ∏ -->
<h3 id="partition-score">7.1&nbsp;Score matching cuts <em>Z</em><sub>Œ∏</sub> out</h3>
<p>
  For an energy model
  \( p_\theta(x)=\exp[-E_\theta(x)]/Z_\theta \),
</p>

<div class="eq-scroll">\[
  \nabla_x\log p_\theta(x)
  = -\nabla_x E_\theta(x),
  \qquad
  \nabla_x\log Z_\theta = 0 .
\]</div>

<p class="explain">
  The constant log-normaliser disappears <em>before</em> we take any
  gradients wrt&nbsp;\(\theta\).
  Training a 100-million-parameter U-Net by score matching therefore costs
  no more than an equally-sized supervised network &mdash; no inner-loop
  MCMC, no partition-function estimates.
</p>

<!-- 7.3 takeaway -->
<h3 id="partition-takeaway">7.2&nbsp;Take-away</h3>
<ul>
  <li><strong>Evaluation</strong>: you still can‚Äôt get an exact
      likelihood, but you <em>can</em> sample once the score field is
      trained.</li>
  <li><strong>Training</strong>: score matching converts a
      global-normalisation problem into a local-derivative regression, the
      only reason diffusion models scale to megapixel images.</li>
</ul>

  <!-- =====================================================
     8. Connecting Every Math Block to Plain PyTorch Code
     ===================================================== -->
<h2 id="pytorch-map">8.&nbsp;Connecting&nbsp;Every&nbsp;Block&nbsp;to&nbsp;PyTorch&nbsp;Code</h2>

<table class="tbl">
  <thead>
    <tr><th>Math block</th><th style="min-width:320px">2&nbsp;&ndash;&nbsp;3&nbsp;lines of PyTorch</th></tr>
  </thead>
  <tbody>
    <tr>
      <td>Forward kernel (Eq.&nbsp;1.2)</td>
      <td><code>x_t = sqrt_alphas_cumprod[t] * x0</code><br>
          <code>     + sqrt_one_minus_alphas_cumprod[t] * torch.randn_like(x0)</code></td>
    </tr>

    <tr>
      <td>Label \(y_t\) (Eq.&nbsp;2.2)</td>
      <td><code>label = -eps / sigma_t</code></td>
    </tr>

    <tr>
      <td>Loss (Eq.&nbsp;3.3)</td>
      <td><code>loss = F.mse_loss(model(x_t, t_emb), label)</code></td>
    </tr>

    <tr>
      <td>Optimiser step</td>
      <td><code>loss.backward(); optimizer.step(); optimizer.zero_grad()</code></td>
    </tr>

    <tr>
      <td>Sampling loop</td>
      <td>Euler, DDIM, or DPM-Solver integrates the<br>
          reverse SDE/ODE using <code>model</code> as the drift.</td>
    </tr>
  </tbody>
</table>

<p class="bridge">
  Those five snippets <em>are</em> the entire research pipeline:
  anything else in a diffusion repo &mdash; gradient clipping, EMA,
  mixed-precision &mdash; is engineering garnish.
</p>


<!-- =====================================================
     9. Conclusion
     ===================================================== -->
<h2 id="conclusion">9.&nbsp;Conclusion</h2>

<p>
  We began with the simplest act of ‚Äúadding Gaussian noise‚Äù to an image and
  uncovered a chain of results whose elegance belies their impact.  A
  closed-form forward kernel let us write the conditional score in a single
  line; Bayes‚Äô rule then turned that score into an <em>unbiased label</em>
  available for every noisy sample.  Hyv√§rinen‚Äôs integration-by-parts step
  removed the intractable partition function, while Vincent‚Äôs small-œÉ
  analysis showed that denoising and score matching are merely two views of
  the same statistic.  Together these pieces form the mathematical spine of
  modern diffusion models.
</p>

<p>
  On the numerical side, the Hutchinson trace trick demonstrated how a
  second-order quantity that seems doomed to an <code>O(d¬≤)</code>
  explosion can be estimated with two reverse-mode gradients and
  <code>O(d)</code> memory.  Although the Laplacian vanishes from the
  Gaussian-label diffusion loss, Hutchinson remains indispensable for
  researchers who venture beyond the comfort of analytic kernels.
</p>

<p>
  What finally matters for practitioners is that this entire theoretical
  scaffold collapses into four lines of PyTorch: build an architecture,
  corrupt data with the forward kernel, fabricate the label
  <code>-Œµ/œÉ<sub>t</sub></code>, and drive an MSE loss.  Everything else‚Äî
  schedules, EMA, or GPU wizardry‚Äîmerely polishes those four lines.  In that
  sense the journey from probability calculus to photorealistic images is
  short: a direct footpath paved by careful mathematics.
</p>
</section>

