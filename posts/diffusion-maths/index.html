<style>
  /* Put your styles here */
  /* --- TL;DR --- */
  .tldr{
    border: 2px solid #4f81ff;
    padding:1rem 1.25rem;
    border-radius:0.5rem;
    background:#f7fbff;
  }
  .tldr h2{margin-top:0}
  .tldr ol{margin-left:1.2rem}

  /* --- shared table look --- */
table.tbl{
  width: 100%;
  border-collapse: collapse;
  margin: 2rem 0 2.5rem 0;
  font-size: 0.95rem;
}

table.tbl caption{
  caption-side: top;
  font-weight: 600;
  margin-bottom: .4rem;
}

table.tbl th,
table.tbl td{
  border: 1px solid #d0d0d0;
  padding: .45rem .65rem;
  text-align: left;
  vertical-align: top;
}

table.tbl thead{
  background: #f5f7ff;
}

table.tbl .shape{
  font-family: "Roboto Mono", ui-monospace, monospace;
  white-space: nowrap;
}

/* separate tone for the ‚Äúfacts‚Äù table */
.fact-table th{
  background:#f5f5f5;
  width: 160px;
}

/* --- Table Notes --- */
.note{
  font-size: 0.93rem;
  font-style: italic;
  margin: -0.5rem 0 2rem 0;   /* tighten space above, keep space below */
  color: #444;
}

</style>
<section>
<!-- =======================  Header   ======================= -->
<h1 id="diffusion-maths">The Mathematical Foundations of Diffusion Models <small>(A self-contained mini-monograph)</small></h1>

<!-- =======================  TL;DR box ======================= -->
<aside class="tldr">
  <h2 id="TL;DR">TL;DR</h2>
  <p>
    Diffusion models generate images in two conceptual moves:
  </p>
  <ol>
    <li>
      <strong>Corrupt&nbsp;the data.</strong>  
      Each clean image is progressively blended with analytically-tractable Gaussian noise.
    </li>
    <li>
      <strong>Learn&nbsp;the score.</strong>  
      A neural network is trained‚Äîvia an ordinary mean-squared-error loss‚Äîto predict the <em>gradient</em> of the log-density (the ‚Äúscore‚Äù) of every noisy distribution.
    </li>
  </ol>
  <p>
    Once that score field is known, a <em>reverse-time stochastic differential equation</em>
    (or an equivalent deterministic ODE) can be integrated to
    push pure white noise back onto the data manifold‚Äîyielding photorealistic
    images without adversarial training or intractable likelihoods.
  </p>
</aside>

<h2 id="notations-and-big-picture">1. Notations and Big Picture</h2>
  <!-- ============  Notation Cheat-Sheet & Shapes  ============ -->
<table class="tbl note-table">
  <caption>Notation Cheat-Sheet &amp; Typical Shapes</caption>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Meaning (plain language)</th>
      <th class="shape">Typical shape / note</th>
    </tr>
  </thead>

  <tbody>
    <tr><td><code>x<sub>0</sub> ‚àà ‚Ñù<sup>d</sup></code></td>
        <td>clean data vector (flattened image)</td>
        <td class="shape"><code>d = 3 H W</code></td></tr>

    <tr><td><code>t ‚àà {0,‚Ä¶,T}</code></td>
        <td>discrete diffusion timestep</td>
        <td class="shape">integer</td></tr>

    <tr><td><code>Œ±<sub>t</sub></code></td>
        <td>retain-signal factor at step <code>t</code></td>
        <td class="shape">scalar in (0,1)</td></tr>

    <tr><td><code>ùõºÃÑ<sub>t</sub> = ‚àè<sub>s=1</sub><sup>t</sup> Œ±<sub>s</sub></code></td>
        <td>cumulative signal survival</td>
        <td class="shape">scalar</td></tr>

    <tr><td><code>œÉ<sub>t</sub> = ‚àö(1-ùõºÃÑ<sub>t</sub>)</code></td>
        <td>total noise std-dev after step <code>t</code></td>
        <td class="shape">scalar</td></tr>

    <tr><td><code>Œµ ‚àº ùí©(0,I<sub>d</sub>)</code></td>
        <td>fresh isotropic Gaussian noise</td>
        <td class="shape"><code>‚Ñù<sup>d</sup></code></td></tr>

    <tr><td><code>p<sub>data</sub></code></td>
        <td>true‚Äîbut unknown‚Äîimage data density</td>
        <td class="shape">never explicit</td></tr>

    <tr><td><code>q(x<sub>t</sub> | x<sub>0</sub>)</code></td>
        <td>designer-chosen Gaussian noising kernel</td>
        <td class="shape">analytic</td></tr>

    <tr><td><code>p<sub>t</sub>(x<sub>t</sub>)</code></td>
        <td>marginal noisy density after corruption</td>
        <td class="shape">unknown but samplable</td></tr>

    <tr><td><code>s<sub>Œ∏</sub>(x<sub>t</sub>,t)</code></td>
        <td>neural network score estimator</td>
        <td class="shape">same shape as <code>x<sub>t</sub></code></td></tr>
  </tbody>
</table>
<p class="note">
  Throughout, <code>Œµ&nbsp;‚àº&nbsp;ùí©(0,&nbsp;I<sub>d</sub>)</code>.
  All inner products&nbsp;/&nbsp;norms are Euclidean.
</p>

<!-- ============  Stock-the-Toolbox Facts  ============ -->
<table class="tbl fact-table">
  <caption>Stock-the-Toolbox Facts (handy formulas)</caption>
  <tbody>
    <tr><th>Gaussian PDF</th>
        <td><code>œï<sub>Œ£</sub>(z) = exp(-¬Ω z<sup>‚ä§</sup>Œ£<sup>-1</sup>z) / ‚àö((2œÄ)<sup>d</sup> det Œ£)</code></td></tr>

    <tr><th>Quadratic gradient</th>
        <td><code>‚àá<sub>x</sub>‚Äñx-a‚Äñ<sup>2</sup> = 2 (x-a)</code></td></tr>

    <tr><th>Chain rule for logs</th>
        <td><code>‚àá log f = (‚àáf) / f</code></td></tr>

    <tr><th>Affine-Gaussian rule</th>
        <td>If <code>Z‚àºùí©(0,I)</code> and <code>Y=a+BZ</code> then <code>Y‚àºùí©(a, B B<sup>‚ä§</sup>)</code>.</td></tr>
  </tbody>
</table>
<p class="note">
  Keep these identities at hand&mdash;we&rsquo;ll cite them explicitly in the derivations below.
</p>


<!--  =====================================================
      Forward (Noising) Process ‚Äî From One Step to Closed Form
      ===================================================== -->
<h2 id="forward">1.&nbsp;Forward&nbsp;(Noising)&nbsp;Process ‚Äî derivation from first principles</h2>

<!-- 1.1 one tiny noise injection -->
<h3>1.1 One tiny noise injection</h3>
<p>
  <strong>Principle&nbsp;&raquo;</strong>
  A single DDPM step keeps a fraction \( \sqrt{1-\beta} \) of the signal and
  blends in \( \sqrt{\beta} \) of fresh Gaussian noise:
</p>

<p class="display-math">\[
  x' \;=\; \sqrt{1-\beta}\,x \;+\; \sqrt{\beta}\,\varepsilon,
  \qquad
  \varepsilon \sim \mathcal N(0, I_d).
\tag{1.1}\]</p>

<p class="explain">
  The square roots ensure that the variance of \( x' \) is exactly
  \( \beta \) larger than that of \( x \).
</p>

<!-- 1.2 repeat t times -->
<h3>1.2 Repeat \( t \) times &rarr; closed form</h3>
<p>
  Let \( \beta_s = 1-\alpha_s \) and
  \( \bar\alpha_t = \prod_{s=1}^{t} \alpha_s \).
  Re-applying (1.1) \( t \) times yields
</p>

<p class="display-math">\[
  x_t \;=\; \sqrt{\bar\alpha_t}\,x_0
        \;+\; \sqrt{1-\bar\alpha_t}\,\varepsilon.
\tag{1.2}\]</p>

<p class="explain">
  Define \( \sigma_t := \sqrt{1-\bar\alpha_t} \) for later use.
</p>

<!-- 1.3 conditional kernel -->
<h3>1.3 Conditional density \( q(x_t\mid x_0) \)</h3>
<p>
  An affine transform of a Gaussian is Gaussian, so
</p>

<p class="display-math">\[
  q(x_t\mid x_0)
  \;=\;
  \mathcal N\!\bigl(
    x_t; \sqrt{\bar\alpha_t}\,x_0,\,
    \sigma_t^2 I_d
  \bigr).
\tag{1.3}\]</p>

<p class="explain">
  We‚Äôll differentiate (1.3) next; its gradient becomes a ‚Äúfree‚Äù supervised
  label during training.
</p>




