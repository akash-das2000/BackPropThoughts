<style>
  /* Put your styles here */
  /* ----------  TL;DR call-out ---------- */
.tldr{
  border:2px solid #4f81ff;
  border-radius:.5rem;
  padding:1rem 1.25rem;
  background:#f7fbff;
}
.tldr h2{margin:0 0 .4rem 0}
.tldr ol{margin-left:1.3rem}

/* ----------  table styling ---------- */
table.tbl{
  width:100%;
  border-collapse:collapse;
  margin:2rem 0 2.5rem 0;
  font-size:.95rem;
}
table.tbl caption{
  caption-side:top;
  font-weight:600;
  margin-bottom:.4rem;
}
table.tbl th,
table.tbl td{
  border:1px solid #d0d0d0;
  padding:.45rem .65rem;
  text-align:left;
  vertical-align:top;
}
table.tbl thead{background:#f5f7ff;}
.tbl .shape{
  font-family:"Roboto Mono",ui-monospace,monospace;
  white-space:nowrap;
}
.fact-table th{
  background:#f5f5f5;
  width:160px;
}

/* ----------  lead paragraph ---------- */
.lead{
  font-size:1.05rem;
  line-height:1.6;
  margin:1.2rem 0 2rem 0;
  color:#333;
}
.lead ul{margin:.6rem 0 .6rem 1.4rem}

/* ----------  bridge note ---------- */
.bridge{
  font-size:.95rem;
  margin:.8rem 0 1.1rem 0;
  color:#444;
}            /* <‚Äî THIS was the missing brace */

/* ----------  display equations ---------- */
.eq-scroll{
  display:block;
  overflow-x:auto;
  white-space:nowrap;
  text-align:center;
  margin:1.2rem auto;
  font-size:1.02rem;
}
@media(max-width:600px){
  .eq-scroll{font-size:.9rem}
  table.tbl{font-size:.9rem}
}

/* ----------  inline & block code ---------- */
code,pre{
  font-family:"Fira Code","SFMono-Regular",ui-monospace,monospace;
  font-size:.92rem;
}
code{
  background:#f3f4f6;
  color:#1a1a1a;
  padding:0 .25em;
  border-radius:4px;
}
pre{
  background:#f8f9fb;
  border:1px solid #cfd2d7;
  border-radius:6px;
  padding:.9rem 1rem;
  line-height:1.45;
  overflow-x:auto;
  margin:1.6rem 0;
}
pre code{background:none;padding:0}
@media(max-width:600px){
  pre{font-size:.82rem}
  code{font-size:.86rem}
}

/* ----------  GitHub-style code card ---------- */
.code-card{
  background:#f6f8fa;
  border:1px solid #d0d7de;
  border-radius:6px;
  overflow:hidden;
  margin:1.6rem 0;
}
.code-card .code-header{
  background:#eaeef2;
  border-bottom:1px solid #d0d7de;
  font:.75rem/1 system-ui,sans-serif;
  color:#24292f;
  padding:.45rem .9rem;
  text-transform:lowercase;
}
.code-card pre{
  margin:0;
  padding:.8rem 1rem;
  background:inherit;
  font-size:.92rem;
  white-space:pre;
}
@media(max-width:600px){
  .code-card pre{font-size:.82rem}
}
  
/* === let wide tables side-scroll on narrow screens === */
@media (max-width: 600px){
  table.tbl{
    display:block;          /* makes it a scroll container   */
    overflow-x:auto;        /* side-scroll if too wide        */
    -webkit-overflow-scrolling: touch;
  }
  table.tbl thead,
  table.tbl tbody{
    display:table;          /* keeps header & body aligned    */
    width:100%;
  }
  table.tbl th,
  table.tbl td{
    white-space:nowrap;     /* prevent ugly line wraps        */
  }
}

</style>
<section>
<!-- =======================  Header   ======================= -->
<h1 id="diffusion-maths">The Mathematical Foundations of Diffusion Models <small>(A self-contained mini-monograph)</small></h1>

<!-- =======================  TL;DR box ======================= -->
<aside class="tldr">
  <h2 id="TL;DR">TL;DR</h2>
  <p>
    Diffusion models generate images in two conceptual moves:
  </p>
  <ol>
    <li>
      <strong>Corrupt&nbsp;the data.</strong>  
      Each clean image is progressively blended with analytically-tractable Gaussian noise.
    </li>
    <li>
      <strong>Learn&nbsp;the score.</strong>  
      A neural network is trained‚Äîvia an ordinary mean-squared-error loss‚Äîto predict the <em>gradient</em> of the log-density (the ‚Äúscore‚Äù) of every noisy distribution.
    </li>
  </ol>
  <p>
    Once that score field is known, a <em>reverse-time stochastic differential equation</em>
    (or an equivalent deterministic ODE) can be integrated to
    push pure white noise back onto the data manifold‚Äîyielding photorealistic
    images without adversarial training or intractable likelihoods.
  </p>
</aside>

<h2 id="notations-and-big-picture">1. Notations and Big Picture</h2>
  <!-- ============  Notation Cheat-Sheet & Shapes  ============ -->
<table class="tbl note-table">
  <caption>Notation Cheat-Sheet &amp; Typical Shapes</caption>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Meaning (plain language)</th>
      <th class="shape">Typical shape / note</th>
    </tr>
  </thead>

  <tbody>
    <tr><td><code>x<sub>0</sub> ‚àà ‚Ñù<sup>d</sup></code></td>
        <td>clean data vector (flattened image)</td>
        <td class="shape"><code>d = 3 H W</code></td></tr>

    <tr><td><code>t ‚àà {0,‚Ä¶,T}</code></td>
        <td>discrete diffusion timestep</td>
        <td class="shape">integer</td></tr>

    <tr><td><code>Œ±<sub>t</sub></code></td>
        <td>retain-signal factor at step <code>t</code></td>
        <td class="shape">scalar in (0,1)</td></tr>

    <tr><td><code>ùõºÃÑ<sub>t</sub> = ‚àè<sub>s=1</sub><sup>t</sup> Œ±<sub>s</sub></code></td>
        <td>cumulative signal survival</td>
        <td class="shape">scalar</td></tr>

    <tr><td><code>œÉ<sub>t</sub> = ‚àö(1-ùõºÃÑ<sub>t</sub>)</code></td>
        <td>total noise std-dev after step <code>t</code></td>
        <td class="shape">scalar</td></tr>

    <tr><td><code>Œµ ‚àº ùí©(0,I<sub>d</sub>)</code></td>
        <td>fresh isotropic Gaussian noise</td>
        <td class="shape"><code>‚Ñù<sup>d</sup></code></td></tr>

    <tr><td><code>p<sub>data</sub></code></td>
        <td>true‚Äîbut unknown‚Äîimage data density</td>
        <td class="shape">never explicit</td></tr>

    <tr><td><code>q(x<sub>t</sub> | x<sub>0</sub>)</code></td>
        <td>designer-chosen Gaussian noising kernel</td>
        <td class="shape">analytic</td></tr>

    <tr><td><code>p<sub>t</sub>(x<sub>t</sub>)</code></td>
        <td>marginal noisy density after corruption</td>
        <td class="shape">unknown but samplable</td></tr>

    <tr><td><code>s<sub>Œ∏</sub>(x<sub>t</sub>,t)</code></td>
        <td>neural network score estimator</td>
        <td class="shape">same shape as <code>x<sub>t</sub></code></td></tr>
  </tbody>
</table>
<p class="note">
  Throughout, <code>Œµ&nbsp;‚àº&nbsp;ùí©(0,&nbsp;I<sub>d</sub>)</code>.
  All inner products&nbsp;/&nbsp;norms are Euclidean.
</p>

<!-- ============  Stock-the-Toolbox Facts  ============ -->
<table class="tbl fact-table">
  <caption>Stock-the-Toolbox Facts (handy formulas)</caption>
  <tbody>
    <tr><th>Gaussian PDF</th>
        <td><code>œï<sub>Œ£</sub>(z) = exp(-¬Ω z<sup>‚ä§</sup>Œ£<sup>-1</sup>z) / ‚àö((2œÄ)<sup>d</sup> det Œ£)</code></td></tr>

    <tr><th>Quadratic gradient</th>
        <td><code>‚àá<sub>x</sub>‚Äñx-a‚Äñ<sup>2</sup> = 2 (x-a)</code></td></tr>

    <tr><th>Chain rule for logs</th>
        <td><code>‚àá log f = (‚àáf) / f</code></td></tr>

    <tr><th>Affine-Gaussian rule</th>
        <td>If <code>Z‚àºùí©(0,I)</code> and <code>Y=a+BZ</code> then <code>Y‚àºùí©(a, B B<sup>‚ä§</sup>)</code>.</td></tr>
  </tbody>
</table>
<p class="note">
  Keep these identities at hand&mdash;we&rsquo;ll cite them explicitly in the derivations below.
</p>


<!-- =====================================================
     1. Forward (Noising) Process ‚Äî From One Step to Closed Form
     ===================================================== -->
<h2 id="forward">1.&nbsp;Forward&nbsp;(Noising) Process &mdash; from a single&nbsp;noise injection to a closed-form kernel</h2>
<p class="lead">
  Imagine pressing a camera‚Äôs shutter while slowly cranking up the ISO:
  each tick makes the photo grainier until only static remains.
  The <em>forward&nbsp;process</em> in diffusion models is exactly that:
  a time-indexed recipe that <strong>systematically degrades</strong> a clean
  image into pure Gaussian noise.
  <br>
  In this section we:
  <ul>
    <li>quantify one such ‚Äúgrain-up‚Äù tick (<span class="math">\( \beta \)</span>-noise injection),</li>
    <li>show how chaining many ticks collapses into a single closed-form
        distribution, and</li>
    <li>write that distribution analytically, so its gradient becomes a
        free supervised label in later training.</li>
  </ul>
  Master these three steps and you hold the <em>entire</em> forward half of a diffusion model in your hands.
</p>
<!-- 1.1 one tiny noise injection -->
<h3 id="noise-injection">1.1&nbsp;One tiny noise injection</h3>

<p>
  Think of the forward process as a photographic fade-out:
  each step <em>shrinks</em> the original signal and sprinkles in fresh Gaussian static.  
  A blend parameter&nbsp;\( \beta\in(0,1) \) tells us
  exactly how much signal versus noise we keep.
</p>

<div class="eq-scroll">\[
  x' \;=\; \sqrt{\,1-\beta\,}\;x
        \;+\; \sqrt{\beta}\;\varepsilon,
  \qquad
  \varepsilon \sim \mathcal N(0,I_d).
\tag{1.1}\]</div>

<p>
  The square-root factors guarantee the variance of \(x'\) is
  <span class="math">\( \beta \)</span> higher than that of \(x\):
  the signal‚Äôs power is scaled by \(1-\beta\), the noise‚Äôs by \(\beta\).
</p>

<!-- 1.2 repeat t times -->
<h3 id="repeat-steps">1.2&nbsp;Repeat <em>t</em> steps &rarr; one closed-form jump</h3>

<p>
  Apply (1.1) successively with variances
  \( \beta_1,\beta_2,\dots,\beta_t \) &nbsp;
  \(\bigl(\alpha_s := 1-\beta_s\bigr)\).
  Induction collapses the chain into one affine transform:
</p>

<div class="eq-scroll">\[
  x_t \;=\; \sqrt{\bar\alpha_t}\,x_0
        \;+\; \sqrt{1-\bar\alpha_t}\,\varepsilon,
  \qquad
  \bar\alpha_t := \prod_{s=1}^{t}\alpha_s .
\tag{1.2}\]</div>

<p>
  In words: after \(t\) steps a fixed share
  \( \sqrt{\bar\alpha_t} \) of the original image survives; the rest is
  i.i.d.&nbsp;Gaussian noise with total standard deviation
  \( \sigma_t := \sqrt{1-\bar\alpha_t} \).
</p>

<!-- 1.3 conditional kernel -->
<h3 id="conditional-density">1.3&nbsp;Conditional density \( q(x_t \mid x_0) \)</h3>

<p>
  Any affine map of a Gaussian remains Gaussian, so conditioning on the
  clean image \(x_0\) we obtain an <em>analytic</em> forward kernel:
</p>

<div class="eq-scroll">\[
  q(x_t \mid x_0)
  \;=\;
  \mathcal N\!\bigl(
      x_t;
      \sqrt{\bar\alpha_t}\,x_0,\;
      \sigma_t^2 I_d
  \bigr).
\tag{1.3}\]</div>

<p>
  Equation&nbsp;(1.3) is a workhorse: its gradient will give us a
  <em>free</em> supervised label when we train the score network.
</p>

<!-- =====================================================
     2. Why We Need the Score ‚Äî and How to Get It for Free
     ===================================================== -->
<h2 id="score">2.&nbsp;Why We Need the&nbsp;Score&nbsp;&mdash; and&nbsp;How&nbsp;to&nbsp;Get&nbsp;It for&nbsp;Free</h2>

<!-- ---------- lead-in ---------- -->
<p class="lead">
  Training a diffusion model is not about matching full probability
  densities &mdash; that would require intractable normalising constants.
  Instead, we learn the <em>score&nbsp;field</em>:
  the gradient of the log-density, which points toward higher probability
  exactly like a small compass needle.  Remarkably, the Gaussian forward
  kernel we just derived lets us fabricate <strong>perfect ground-truth
  score labels</strong> on the fly, at <em>every</em> noise level.
</p>

<!-- bridge: why a quadratic appears in log q -->
<p class="bridge">
  <strong>Where does that quadratic originate?</strong><br>
  The conditional kernel in Eq.&nbsp;(1.3) is a multivariate Gaussian,
  so its probability density function is
  \[
     q(x_t \mid x_0)
     = \frac{\exp\!\bigl(
         -\tfrac12
         \tfrac{\|x_t-\sqrt{\bar\alpha_t}\,x_0\|^2}
               {\sigma_t^2}
       \bigr)}
       {(2\pi)^{d/2}\sigma_t^{\,d}} .
  \]
  When we take <em>log</em> of that PDF, every constant goes to an additive
  ‚Äú<code>const</code>,‚Äù and the exponent‚Äôs
  squared-distance term
  \(
     \|x_t-\sqrt{\bar\alpha_t}x_0\|^2
  \)
  is the <em>quadratic</em> we differentiate.
  Substituting
  \( x_t = \sqrt{\bar\alpha_t}\,x_0 + \sqrt{1-\bar\alpha_t}\,\varepsilon \)
  from Eq.&nbsp;(1.2) makes the difference
  \( x_t-\sqrt{\bar\alpha_t}x_0 \)
  collapse neatly to
  \( \sqrt{1-\bar\alpha_t}\,\varepsilon \).
  Divide by \( \sigma_t^2 = 1-\bar\alpha_t \),
  apply the gradient of a norm-square
  \( \nabla_x\|x\|^2 = 2x \),
  and you land on the compact
  \(
     -\,\varepsilon/\sigma_t
  \)
  result shown next.
</p>
<!-- 2.1 What is a score? -->
<h3 id="what-is-score">2.1&nbsp;What is a score?</h3>
<p>
  For any probability density \( p(z) \) the&nbsp;
  <em>score</em> is defined as
  \( s(z) := \nabla_z \log p(z) \).
  It is the vector pointing in the direction
  the density rises fastest&mdash;a gradient ascent arrow in probability
  space.
</p>

<!-- 2.2 Gradient of the Gaussian kernel -->
<h3 id="graodent-of-gaussian">2.2&nbsp;Gradient of the Gaussian&nbsp;kernel (one-liner!)</h3>
<!-- quick recap of x_t and alpha-bar_t -->
<p class="bridge">
  <strong>Recall the forward step.</strong>&nbsp;
  From Eq.&nbsp;(1.2) we have
  \( x_t = \sqrt{\bar\alpha_t}\,x_0 + \sqrt{1-\bar\alpha_t}\,\varepsilon \)
  with the cumulative factor
  \( \displaystyle \bar\alpha_t = \prod_{s=1}^{t}\alpha_s \).
  <br>
  Insert that expression into the quadratic
  \( \|x_t-\sqrt{\bar\alpha_t}x_0\|^2 \)
  and notice the difference term simplifies to
  \( \sqrt{1-\bar\alpha_t}\,\varepsilon \).
  Dividing by \(1-\bar\alpha_t\) and taking the gradient yields the tidy
  ‚Äúnegative&nbsp;noise over&nbsp;\(\sigma_t\)‚Äù formula below.
</p>
<p>
  Take the log of the forward kernel \( q(x_t \mid x_0) \) from&nbsp;(1.3)
  and differentiate with respect to \( x_t \).  A single line of algebra
  (using the quadratic-gradient rule) gives
</p>

<div class="eq-scroll">\[
  \nabla_{x_t}\log q(x_t\mid x_0)
  \;=\;
  -\frac{x_t - \sqrt{\bar\alpha_t}\,x_0}{1-\bar\alpha_t}
  \;=\;
  -\frac{\varepsilon}{\sigma_t},
  \qquad
  \bigl(\sigma_t^2 = 1-\bar\alpha_t\bigr).
\tag{2.1}\]</div>

<p class="explain">
  Beautifully simple: the score of the <em>conditional</em> Gaussian is
  just &ldquo;negative noise divided by&nbsp;\(\sigma_t\).&rdquo;
</p>

<!-- 2.3 Training label inside each mini-batch -->
<h3 id="train-label">2.3&nbsp;Training label in every mini-batch</h3>
<p>
  During training we <em>know</em> the exact noise
  \( \varepsilon \) we injected when we built \( x_t \).
  Hence we can compute a <strong>free, per-image, per-timestep label</strong>
</p>

<div class="eq-scroll">\[
  y_t := -\,\varepsilon / \sigma_t.
\tag{2.2}\]</div>

<p class="explain">
  No approximation, no Monte-Carlo &mdash; \( y_t \) drops out of thin air
  for every example in the batch.  All that remains is to prove this label
  is an <em>unbiased</em> estimator of the true, unknown
  marginal score \( \nabla_{x_t}\log p_t(x_t) \);
  we do that in the next subsection using a three-line Bayes trick.
</p>

<!-- 2.4 the marginal score we really want -->
<h3 id="marginal-score">2.4&nbsp;The marginal score we really want</h3>

<p class="bridge">
  The label \( y_t \) we just fabricated is derived from the
  <em>conditional</em> density \( q(x_t\mid x_0) \),
  but generation will ultimately need the score of the
  <strong>marginal</strong> noisy distribution&nbsp;
  \( p_t(x_t) \).
  Let‚Äôs write that object explicitly.
</p>

<div class="eq-scroll">\[
  s^{\star}(x_t,t) \;:=\;
  \nabla_{x_t}\log p_t(x_t),
  \qquad
  p_t(x_t)
  \;=\;
  \int p_{\text{data}}(x_0)\,
       q(x_t \mid x_0)\,
       dx_0.
\tag{2.3}\]</div>

<p class="explain">
  Sadly, \( p_t(x_t) \) has no closed form because
  \( p_{\text{data}} \) is unknown.
  The magic of diffusion is that the label
  \( y_t = -\varepsilon / \sigma_t \)
  turns out to be an <em>unbiased</em> estimator of this intractable
  score.  We prove that with a three-line Bayes trick in the next
  subsection.
</p>

<!-- 2.5 Three-line Bayes proof ‚Äî y_t is an unbiased target -->
<h3 id="three-liner-proof">2.5&nbsp;Three-line&nbsp;Bayes proof ‚Äî <code>y_t</code> is an unbiased target</h3>

<!-- intro sentence -->
<p class="bridge">
  We need to prove
  \( \mathbb E\!\bigl[\,y_t \mid x_t\bigr] = \nabla_{x_t}\log p_t(x_t) \).
  The derivation uses only the chain rule, a gradient-under-the-integral swap,
  and Bayes‚Äô rule.
</p>

<!-- ---- LINE 1 ---- -->
<p><strong>Line&nbsp;1&nbsp;&mdash; chain rule for logs</strong></p>
<div class="eq-scroll">\[
  \nabla_{x_t}\log p_t(x_t)
  \;=\;
  \frac{\nabla_{x_t}p_t(x_t)}{p_t(x_t)} .
\tag{2.4a}\]</div>

<!-- ---- LINE 2 ---- -->
<p><strong>Line&nbsp;2&nbsp;&mdash; move ‚àá under the integral</strong><br>
  (dominated-convergence applies because the Gaussian kernel has bounded
  derivatives):
</p>
<div class="eq-scroll">\[
  \nabla_{x_t}p_t(x_t)
  \;=\;
  \int p_{\text{data}}(x_0)\,
       \nabla_{x_t}q(x_t\mid x_0)\,dx_0 .
\tag{2.4b}\]</div>

<!-- ---- LINE 3 ---- -->
<p><strong>Line&nbsp;3&nbsp;&mdash; split the gradient &amp; recognise the posterior</strong></p>
<ul>
  <li><em>Split the gradient:</em>
      \( \nabla_{x_t}q = q\,\nabla_{x_t}\log q \).  
      Insert the one-liner from&nbsp;(2.1),
      \( \nabla_{x_t}\log q = -\,\varepsilon/\sigma_t \), which is exactly
      \( y_t \).</li>

  <li><em>Bayes‚Äô identity:</em>
      \( p_{\text{data}}(x_0)\,q(x_t\mid x_0) / p_t(x_t)
         = p(x_0\mid x_t) \).</li>
</ul>

<div class="eq-scroll">\[
  \frac{1}{p_t(x_t)}
  \int p_{\text{data}}(x_0)\,
       q(x_t\mid x_0)\,
       \Bigl(-\tfrac{\varepsilon}{\sigma_t}\Bigr)\,dx_0
  \;=\;
  \mathbb E_{x_0\sim p(\,\cdot\mid x_t)}[\,y_t\,] .
\tag{2.4c}\]</div>

<!-- summary -->
<p class="eq-scroll">
  Combine (2.4a)‚Äì(2.4c):
  \[
    \boxed{\;
      \mathbb E\!\bigl[\,y_t \mid x_t\bigr]
      \;=\;
      \nabla_{x_t}\log p_t(x_t)
    \;}. \tag{2.5}
  \]
  Taking an outer expectation over \(x_t\) preserves the equality, so
  swapping the unknown score with \(y_t\) inside our loss
  introduces <em>no bias whatsoever</em>.
</p>

<!-- =====================================================
     3. From Fisher Divergence to a One-Line PyTorch Loss
     ===================================================== -->
<h2 id="loss">3.&nbsp;From Fisher Divergence&nbsp;to a One-Line&nbsp;PyTorch&nbsp;Loss</h2>

<p class="lead">
  With an unbiased label \(y_t\) in hand, we can rewrite the
  Fisher-divergence objective so that <em>every unknown quantity drops
  out</em>.  The result is the single MSE line that powers all modern
  diffusion code-bases.
</p>

<p class="bridge">
  <strong>A 30-second Fisher‚Äìdivergence refresher.</strong><br>
  Whereas maximum likelihood measures the gap between two densities by
  comparing their <em>heights</em>, the Fisher divergence compares their
  <em>slopes</em>:
  \[
    D_F\bigl(p_{\text{data}}\|p_\theta\bigr)
    \;=\;
    \tfrac12
    \int p_{\text{data}}(x)\,
         \bigl\|
           \nabla_x \log p_\theta(x)
           -
           \nabla_x \log p_{\text{data}}(x)
         \bigr\|^{2}\,dx .
  \]
  Matching slopes is golden for diffusion models because
  the gradient of a log-density
  \( \nabla_x\log p_\theta(x) \)
  <em>does not depend</em> on the intractable normalising constant
  \( Z_\theta \).
  By driving this slope-gap to zero, we avoid the partition-function
  bottleneck altogether and still push the model toward the true data
  manifold.
</p>


<!-- 3.1 one-timestep Fisher divergence -->
<h3>3.1&nbsp;Single-timestep Fisher divergence</h3>

<p class="bridge">
  We now translate the Fisher divergence primer into a concrete loss.
  Because diffusion training iterates over noise levels, it is natural to
  start with <em>one</em> timestep&nbsp;<code>t</code>.
  The single-step objective below measures how far the network‚Äôs score
  \( s_\theta(x_t,t) \) is from the <em>true</em> marginal score
  \( \nabla_{x_t}\log p_t(x_t) \).  We will immediately swap the latter
  for its unbiased label \( y_t \), but writing it once in full keeps the
  statistical grounding clear.
</p>


<div class="eq-scroll">\[
  \mathcal L_t(\theta)
  \;=\;
  \tfrac12\,
  \mathbb E_{p_t}
    \bigl\|
      s_\theta(x_t,t)
      -\nabla_{x_t}\log p_t(x_t)
    \bigr\|^{2}.
\tag{3.1}\]</div>

<!-- 3.2 swap score for label -->
<h3>3.2&nbsp;Swap the unknown score for its label</h3>

<p class="bridge">
  Using the equality
  \( \mathbb E[\,y_t \mid x_t] = \nabla_{x_t}\log p_t(x_t) \)
  proved in Section 2.5, we can replace the intractable score by
  \( y_t = -\varepsilon/\sigma_t \):
</p>

<div class="eq-scroll">\[
  \mathcal L_t(\theta)
  \;=\;
  \tfrac12\,
  \mathbb E_{x_0,\varepsilon}
    \bigl\|
      s_\theta(x_t,t)
      +\tfrac{\varepsilon}{\sigma_t}
    \bigr\|^{2}.
\tag{3.2}\]</div>

<!-- 3.3 aggregate over timesteps -->
<h3>3.3&nbsp;Aggregate over all timesteps</h3>

<p>
  Sample \(t\) uniformly (or with any weighting schedule) to get the full
  training objective:
</p>

<div class="eq-scroll">\[
  \boxed{%
  \mathcal L(\theta)
  \;=\;
  \mathbb E_{\,t,x_0,\varepsilon}
    \bigl\|
      s_\theta(x_t,t)
      +\tfrac{\varepsilon}{\sigma_t}
    \bigr\|^{2}
  }.
\tag{3.3}\]</div>

<p class="explain">
  All expectations are taken over quantities we <em>sample directly</em>
  (data, noise, timestep), so the loss is fully computable.
</p>

<!-- 3.4 predict-noise variant -->
<h3>3.4&nbsp;Predict-noise variant &amp; PyTorch one-liner</h3>

<p>
  Many implementations train the network to output raw noise rather than
  the score.  Define
  \( \hat\varepsilon_\theta := -\sigma_t\,s_\theta(x_t,t) \); then
  Eq.&nbsp;(3.3) becomes
</p>

<div class="eq-scroll">\[
  \mathcal L(\theta)
  \;=\;
  \mathbb E_{\,t,x_0,\varepsilon}
    \bigl\|
      \hat\varepsilon_\theta - \varepsilon
    \bigr\|^{2}.
\tag{3.4}\]</div>

<p class="explain">
  In PyTorch that reduces to the canonical&nbsp;one-liner:
</p>

<div class="code-card">
  <div class="code-header">python</div>
  <pre><code>loss = F.mse_loss(pred_eps, eps)</code></pre>
</div>


<!-- =====================================================
     4. Hyv√§rinen Score Matching ‚Äî Full Integration-by-Parts Walk-Through
     ===================================================== -->
<h2 id="hyvarinen">4.&nbsp;Hyv√§rinen&nbsp;Score&nbsp;Matching &mdash; Integration-by-Parts Walk-Through</h2>

<p class="lead">
  Fisher divergence lets us compare two probability densities without
  ever touching their normalising constants.
  That‚Äôs perfect for energy-based models, whose partition
  functions are hopeless in image dimensions.
  The catch?  Fisher divergence still contains the unknown
  <em>data&nbsp;score</em>
  \( \nabla_x\log p_{\text{data}} \).
  Hyv√§rinen‚Äôs 2005 trick uses a single integration-by-parts move
  to <strong>eliminate</strong> that data-score,
  leaving a loss we can evaluate with automatic differentiation alone.
</p>

<!-- 4.1 why we need it -->
<h3 id="hyvarinen-why">4.1&nbsp;Why we need it</h3>
<p>
  We want a discrepancy measure
  \( D_F(p_{\text{data}}\Vert p_\theta) \)
  that:
</p>
<ul>
  <li>does <em>not</em> require the unknown
      \( p_{\text{data}} \) in closed form, and</li>
  <li>does <em>not</em> require the
      partition function \( Z_\theta \) of the model.</li>
</ul>

<div class="eq-scroll">\[
  D_F(p_{\text{data}}\Vert p_\theta)
  \;=\;
  \tfrac12
  \int p_{\text{data}}(x)\,
       \bigl\|
         \nabla_x \log p_\theta(x)
         - \nabla_x \log p_{\text{data}}(x)
       \bigr\|^{2}\,dx .
\]</div>

<p class="bridge">
  Fisher divergence solves the
  <em>partition-function</em> problem
  (because derivatives kill constants),
  but it still contains the inaccessible
  data-score
  \( \nabla_x\log p_{\text{data}} \).
  Hyv√§rinen‚Äôs idea:
  convert that term into a <em>Laplacian</em>
  of the model‚Äôs log-density, using the divergence theorem.
</p>

<!-- 4.2 step-by-step derivation -->
<h3 id="hyvarinen-derivation">4.2&nbsp;Step-by-step derivation</h3>

<!-- (a) expand the square -->
<p><strong>Step&nbsp;1 &mdash; expand the squared norm</strong></p>
<div class="eq-scroll">\[
  \|a-b\|^2
  =\|a\|^2+\|b\|^2-2\,a\!\cdot\!b .
\]</div>

<p>
  Let
  \( a=\nabla_x\log p_\theta \),
  \( b=\nabla_x\log p_{\text{data}} \).
  Plugging into the divergence gives
</p>

<div class="eq-scroll">\[
  J(\theta)
  =\tfrac12
     \int p_{\text{data}}\|a\|^2dx
  -\!\!
     \int p_{\text{data}}\,a\!\cdot\!b\,dx
  +\text{const}.
  \tag{4.1}
\]</div>

<p class="explain">
  The const term (from
  \( \|b\|^2 \))
  has no&nbsp;\( \theta \); drop it for optimisation.
</p>

<!-- (b) rewrite the cross-term -->
<p><strong>Step&nbsp;2 &mdash; rewrite the cross-term</strong></p>
<p>
  Because
  \( b=\nabla_x\log p_{\text{data}} \),
  we have
  \( p_{\text{data}}\,b = \nabla_x p_{\text{data}} \).
  Thus
</p>

<div class="eq-scroll">\[
  C(\theta)
  :=\int p_{\text{data}}\,a\!\cdot\!b\,dx
  =\int \nabla_x p_{\text{data}}\,\cdot\,a\,dx .
  \tag{4.2}
\]</div>

<!-- (c) integration by parts -->
<p><strong>Step&nbsp;3 &mdash; integration-by-parts in
  \( \mathbb R^{d} \)</strong></p>
<p>
  For a smooth vector field \(g\) that decays faster than
  \(1/\|x\|^{d+1}\),
</p>

<div class="eq-scroll">\[
  \int_{\mathbb R^{d}}\!\nabla_x\!\cdot g(x)\,dx = 0 .
\]</div>

<p>
  Choose
  \( g(x)=p_{\text{data}}(x)\,a(x) \).
  Compute its divergence:
</p>

<div class="eq-scroll">\[
  \nabla_x\!\cdot g
  = \nabla_x p_{\text{data}}\!\cdot a
    + p_{\text{data}}\,
      \underbrace{\nabla_x\!\cdot a}_{=\nabla_x^{2}\log p_\theta}.
\]</div>

<p>
  Because the integral of the divergence is zero, rearrange:
</p>

<div class="eq-scroll">\[
  \int \nabla_x p_{\text{data}}\!\cdot a\,dx
  = -\int p_{\text{data}}\,
           \nabla_x^{2}\!\log p_\theta\,dx .
  \tag{4.3}
\]</div>

<!-- (d) plug back -->
<p><strong>Step&nbsp;4 &mdash; plug (4.3) into&nbsp;(4.1)</strong></p>

<div class="eq-scroll">\[
  \boxed{
  J(\theta)=
  \int p_{\text{data}}
  \Bigl[
    \tfrac12
      \bigl\|\nabla_x\log p_\theta\bigr\|^2
    \;+\;
      \nabla_x^{2}\log p_\theta
  \Bigr]dx
  +\text{const}.
  }
  \tag{4.4}
\]</div>

<p class="explain">
  Every term now depends <em>only</em> on the model
  \( p_\theta \).
  Because
  \( p_\theta(x) = e^{-E_\theta(x)}/Z_\theta \),
  derivatives of \( \log p_\theta \)
  involve <strong>no partition function</strong>:
  \( \nabla_x\log p_\theta = -\nabla_xE_\theta \).
  Minimising \( J(\theta) \) is therefore equivalent to minimising the
  original Fisher divergence, but <em>computable</em> with autograd.
</p>

<!-- summary takeaway -->
<h3 id="hyvarinen-takeaway">4.3&nbsp;What to remember</h3>
<ul>
  <li>The data-score term was replaced by a Laplacian of
      \( \log p_\theta \) via a single divergence-theorem step.</li>
  <li>No normalising constant \( Z_\theta \) survives; score matching
      avoids the partition-function bottleneck entirely.</li>
  <li>Implementations often estimate the Laplacian with the Hutchinson
      trick (covered next), but diffusion training with a
      <em>Gaussian label</em> cleverly sidesteps even that.</li>
</ul>
