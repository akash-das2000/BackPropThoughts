<style>
  /* Put your styles here */
  /* --- TL;DR --- */
  .tldr{
    border: 2px solid #4f81ff;
    padding:1rem 1.25rem;
    border-radius:0.5rem;
    background:#f7fbff;
  }
  .tldr h2{margin-top:0}
  .tldr ol{margin-left:1.2rem}

  /* --- shared table look --- */
table.tbl{
  width: 100%;
  border-collapse: collapse;
  margin: 2rem 0 2.5rem 0;
  font-size: 0.95rem;
}

table.tbl caption{
  caption-side: top;
  font-weight: 600;
  margin-bottom: .4rem;
}

table.tbl th,
table.tbl td{
  border: 1px solid #d0d0d0;
  padding: .45rem .65rem;
  text-align: left;
  vertical-align: top;
}

table.tbl thead{
  background: #f5f7ff;
}

table.tbl .shape{
  font-family: "Roboto Mono", ui-monospace, monospace;
  white-space: nowrap;
}

/* separate tone for the ‚Äúfacts‚Äù table */
.fact-table th{
  background:#f5f5f5;
  width: 160px;
}

/* --- Table Notes --- */
.note{
  font-size: 0.93rem;
  font-style: italic;
  margin: -0.5rem 0 2rem 0;   /* tighten space above, keep space below */
  color: #444;

/* --- lead paragraph that introduces a major section --- */
.lead{
  font-size: 1.05rem;          /* slightly larger than body text   */
  line-height: 1.6;
  margin: 1.2rem 0 2rem 0;     /* extra space above & below        */
  color: #333;
}

.lead ul{
  margin: .6rem 0 .6rem 1.4rem;   /* tighter list spacing            */
  padding-left: 0;
}

.lead ul li{
  margin-bottom: .25rem;
}

}

</style>
<section>
<!-- =======================  Header   ======================= -->
<h1 id="diffusion-maths">The Mathematical Foundations of Diffusion Models <small>(A self-contained mini-monograph)</small></h1>

<!-- =======================  TL;DR box ======================= -->
<aside class="tldr">
  <h2 id="TL;DR">TL;DR</h2>
  <p>
    Diffusion models generate images in two conceptual moves:
  </p>
  <ol>
    <li>
      <strong>Corrupt&nbsp;the data.</strong>  
      Each clean image is progressively blended with analytically-tractable Gaussian noise.
    </li>
    <li>
      <strong>Learn&nbsp;the score.</strong>  
      A neural network is trained‚Äîvia an ordinary mean-squared-error loss‚Äîto predict the <em>gradient</em> of the log-density (the ‚Äúscore‚Äù) of every noisy distribution.
    </li>
  </ol>
  <p>
    Once that score field is known, a <em>reverse-time stochastic differential equation</em>
    (or an equivalent deterministic ODE) can be integrated to
    push pure white noise back onto the data manifold‚Äîyielding photorealistic
    images without adversarial training or intractable likelihoods.
  </p>
</aside>

<h2 id="notations-and-big-picture">1. Notations and Big Picture</h2>
  <!-- ============  Notation Cheat-Sheet & Shapes  ============ -->
<table class="tbl note-table">
  <caption>Notation Cheat-Sheet &amp; Typical Shapes</caption>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Meaning (plain language)</th>
      <th class="shape">Typical shape / note</th>
    </tr>
  </thead>

  <tbody>
    <tr><td><code>x<sub>0</sub> ‚àà ‚Ñù<sup>d</sup></code></td>
        <td>clean data vector (flattened image)</td>
        <td class="shape"><code>d = 3 H W</code></td></tr>

    <tr><td><code>t ‚àà {0,‚Ä¶,T}</code></td>
        <td>discrete diffusion timestep</td>
        <td class="shape">integer</td></tr>

    <tr><td><code>Œ±<sub>t</sub></code></td>
        <td>retain-signal factor at step <code>t</code></td>
        <td class="shape">scalar in (0,1)</td></tr>

    <tr><td><code>ùõºÃÑ<sub>t</sub> = ‚àè<sub>s=1</sub><sup>t</sup> Œ±<sub>s</sub></code></td>
        <td>cumulative signal survival</td>
        <td class="shape">scalar</td></tr>

    <tr><td><code>œÉ<sub>t</sub> = ‚àö(1-ùõºÃÑ<sub>t</sub>)</code></td>
        <td>total noise std-dev after step <code>t</code></td>
        <td class="shape">scalar</td></tr>

    <tr><td><code>Œµ ‚àº ùí©(0,I<sub>d</sub>)</code></td>
        <td>fresh isotropic Gaussian noise</td>
        <td class="shape"><code>‚Ñù<sup>d</sup></code></td></tr>

    <tr><td><code>p<sub>data</sub></code></td>
        <td>true‚Äîbut unknown‚Äîimage data density</td>
        <td class="shape">never explicit</td></tr>

    <tr><td><code>q(x<sub>t</sub> | x<sub>0</sub>)</code></td>
        <td>designer-chosen Gaussian noising kernel</td>
        <td class="shape">analytic</td></tr>

    <tr><td><code>p<sub>t</sub>(x<sub>t</sub>)</code></td>
        <td>marginal noisy density after corruption</td>
        <td class="shape">unknown but samplable</td></tr>

    <tr><td><code>s<sub>Œ∏</sub>(x<sub>t</sub>,t)</code></td>
        <td>neural network score estimator</td>
        <td class="shape">same shape as <code>x<sub>t</sub></code></td></tr>
  </tbody>
</table>
<p class="note">
  Throughout, <code>Œµ&nbsp;‚àº&nbsp;ùí©(0,&nbsp;I<sub>d</sub>)</code>.
  All inner products&nbsp;/&nbsp;norms are Euclidean.
</p>

<!-- ============  Stock-the-Toolbox Facts  ============ -->
<table class="tbl fact-table">
  <caption>Stock-the-Toolbox Facts (handy formulas)</caption>
  <tbody>
    <tr><th>Gaussian PDF</th>
        <td><code>œï<sub>Œ£</sub>(z) = exp(-¬Ω z<sup>‚ä§</sup>Œ£<sup>-1</sup>z) / ‚àö((2œÄ)<sup>d</sup> det Œ£)</code></td></tr>

    <tr><th>Quadratic gradient</th>
        <td><code>‚àá<sub>x</sub>‚Äñx-a‚Äñ<sup>2</sup> = 2 (x-a)</code></td></tr>

    <tr><th>Chain rule for logs</th>
        <td><code>‚àá log f = (‚àáf) / f</code></td></tr>

    <tr><th>Affine-Gaussian rule</th>
        <td>If <code>Z‚àºùí©(0,I)</code> and <code>Y=a+BZ</code> then <code>Y‚àºùí©(a, B B<sup>‚ä§</sup>)</code>.</td></tr>
  </tbody>
</table>
<p class="note">
  Keep these identities at hand&mdash;we&rsquo;ll cite them explicitly in the derivations below.
</p>


<!-- =====================================================
     1. Forward (Noising) Process ‚Äî From One Step to Closed Form
     ===================================================== -->
<h2 id="forward">1.&nbsp;Forward&nbsp;(Noising) Process &mdash; from a single&nbsp;noise injection to a closed-form kernel</h2>
<p class="lead">
  Imagine pressing a camera‚Äôs shutter while slowly cranking up the ISO:
  each tick makes the photo grainier until only static remains.
  The <em>forward&nbsp;process</em> in diffusion models is exactly that:
  a time-indexed recipe that <strong>systematically degrades</strong> a clean
  image into pure Gaussian noise.
  <br>
  In this section we:
  <ul>
    <li>quantify one such ‚Äúgrain-up‚Äù tick (<span class="math">\( \beta \)</span>-noise injection),</li>
    <li>show how chaining many ticks collapses into a single closed-form
        distribution, and</li>
    <li>write that distribution analytically, so its gradient becomes a
        free supervised label in later training.</li>
  </ul>
  Master these three steps and you hold the <em>entire</em> forward half of a diffusion model in your hands.
</p>
<!-- 1.1 one tiny noise injection -->
<h3>1.1&nbsp;One tiny noise injection</h3>

<p>
  Think of the forward process as a photographic fade-out:
  each step <em>shrinks</em> the original signal and sprinkles in fresh Gaussian static.  
  A blend parameter&nbsp;\( \beta\in(0,1) \) tells us
  exactly how much signal versus noise we keep.
</p>

<p class="display-math">\[
  x' \;=\; \sqrt{\,1-\beta\,}\;x
        \;+\; \sqrt{\beta}\;\varepsilon,
  \qquad
  \varepsilon \sim \mathcal N(0,I_d).
\tag{1.1}\]</p>

<p>
  The square-root factors guarantee the variance of \(x'\) is
  <span class="math">\( \beta \)</span> higher than that of \(x\):
  the signal‚Äôs power is scaled by \(1-\beta\), the noise‚Äôs by \(\beta\).
</p>

<!-- 1.2 repeat t times -->
<h3>1.2&nbsp;Repeat <em>t</em> steps &rarr; one closed-form jump</h3>

<p>
  Apply (1.1) successively with variances
  \( \beta_1,\beta_2,\dots,\beta_t \) &nbsp;
  \(\bigl(\alpha_s := 1-\beta_s\bigr)\).
  Induction collapses the chain into one affine transform:
</p>

<p class="display-math">\[
  x_t \;=\; \sqrt{\bar\alpha_t}\,x_0
        \;+\; \sqrt{1-\bar\alpha_t}\,\varepsilon,
  \qquad
  \bar\alpha_t := \prod_{s=1}^{t}\alpha_s .
\tag{1.2}\]</p>

<p>
  In words: after \(t\) steps a fixed share
  \( \sqrt{\bar\alpha_t} \) of the original image survives; the rest is
  i.i.d.&nbsp;Gaussian noise with total standard deviation
  \( \sigma_t := \sqrt{1-\bar\alpha_t} \).
</p>

<!-- 1.3 conditional kernel -->
<h3>1.3&nbsp;Conditional density \( q(x_t \mid x_0) \)</h3>

<p>
  Any affine map of a Gaussian remains Gaussian, so conditioning on the
  clean image \(x_0\) we obtain an <em>analytic</em> forward kernel:
</p>

<p class="display-math">\[
  q(x_t \mid x_0)
  \;=\;
  \mathcal N\!\bigl(
      x_t;
      \sqrt{\bar\alpha_t}\,x_0,\;
      \sigma_t^2 I_d
  \bigr).
\tag{1.3}\]</p>

<p>
  Equation&nbsp;(1.3) is a workhorse: its gradient will give us a
  <em>free</em> supervised label when we train the score network.
</p>





