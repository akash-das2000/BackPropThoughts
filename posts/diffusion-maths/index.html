<style>
  /* Put your styles here */
  /* --- TL;DR --- */
  .tldr{
    border: 2px solid #4f81ff;
    padding:1rem 1.25rem;
    border-radius:0.5rem;
    background:#f7fbff;
  }
  .tldr h2{margin-top:0}
  .tldr ol{margin-left:1.2rem}

  /* --- shared table look --- */
table.tbl{
  width: 100%;
  border-collapse: collapse;
  margin: 2rem 0 2.5rem 0;
  font-size: 0.95rem;
}

table.tbl caption{
  caption-side: top;
  font-weight: 600;
  margin-bottom: .4rem;
}

table.tbl th,
table.tbl td{
  border: 1px solid #d0d0d0;
  padding: .45rem .65rem;
  text-align: left;
  vertical-align: top;
}

table.tbl thead{
  background: #f5f7ff;
}

table.tbl .shape{
  font-family: "Roboto Mono", ui-monospace, monospace;
  white-space: nowrap;
}

/* separate tone for the ‚Äúfacts‚Äù table */
.fact-table th{
  background:#f5f5f5;
  width: 160px;
}

/* --- Table Notes --- */
.note{
  font-size: 0.93rem;
  font-style: italic;
  margin: -0.5rem 0 2rem 0;   /* tighten space above, keep space below */
  color: #444;

/* --- lead paragraph that introduces a major section --- */
.lead{
  font-size: 1.05rem;          /* slightly larger than body text   */
  line-height: 1.6;
  margin: 1.2rem 0 2rem 0;     /* extra space above & below        */
  color: #333;
}

.lead ul{
  margin: .6rem 0 .6rem 1.4rem;   /* tighter list spacing            */
  padding-left: 0;
}

.lead ul li{
  margin-bottom: .25rem;
}

}

/* ========= 1. responsive tables ========= */
table.tbl{
  width:100%;
  border-collapse:collapse;
  margin: 2rem 0 2.5rem 0;
  font-size:.95rem;
  display:block;              /*  <‚Äî ensures its own scroll box  */
  overflow-x:auto;            /*  <‚Äî side-scroll on mobile        */
}
table.tbl thead, table.tbl tbody{
  width:100%;                 /* keeps header & body aligned      */
}
table.tbl th, table.tbl td{
  white-space:nowrap;         /* keep long symbols on one line    */
}

/* ========= 2. responsive display equations ========= */
.display-math{
  display:block;
  overflow-x:auto;            /* side-scroll if too wide          */
  white-space:nowrap;         /* KaTeX/MathJax span stays together*/
  text-align:center;
  margin:1.2rem auto;
  font-size:1.02rem;
}

/* shrink math a bit on narrow screens */
@media (max-width: 600px){
  .display-math{
    font-size:.9rem;
  }
  table.tbl{
    font-size:.9rem;
  }
}


</style>
<section>
<!-- =======================  Header   ======================= -->
<h1 id="diffusion-maths">The Mathematical Foundations of Diffusion Models <small>(A self-contained mini-monograph)</small></h1>

<!-- =======================  TL;DR box ======================= -->
<aside class="tldr">
  <h2 id="TL;DR">TL;DR</h2>
  <p>
    Diffusion models generate images in two conceptual moves:
  </p>
  <ol>
    <li>
      <strong>Corrupt&nbsp;the data.</strong>  
      Each clean image is progressively blended with analytically-tractable Gaussian noise.
    </li>
    <li>
      <strong>Learn&nbsp;the score.</strong>  
      A neural network is trained‚Äîvia an ordinary mean-squared-error loss‚Äîto predict the <em>gradient</em> of the log-density (the ‚Äúscore‚Äù) of every noisy distribution.
    </li>
  </ol>
  <p>
    Once that score field is known, a <em>reverse-time stochastic differential equation</em>
    (or an equivalent deterministic ODE) can be integrated to
    push pure white noise back onto the data manifold‚Äîyielding photorealistic
    images without adversarial training or intractable likelihoods.
  </p>
</aside>

<h2 id="notations-and-big-picture">1. Notations and Big Picture</h2>
  <!-- ============  Notation Cheat-Sheet & Shapes  ============ -->
<table class="tbl note-table">
  <caption>Notation Cheat-Sheet &amp; Typical Shapes</caption>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Meaning (plain language)</th>
      <th class="shape">Typical shape / note</th>
    </tr>
  </thead>

  <tbody>
    <tr><td><code>x<sub>0</sub> ‚àà ‚Ñù<sup>d</sup></code></td>
        <td>clean data vector (flattened image)</td>
        <td class="shape"><code>d = 3 H W</code></td></tr>

    <tr><td><code>t ‚àà {0,‚Ä¶,T}</code></td>
        <td>discrete diffusion timestep</td>
        <td class="shape">integer</td></tr>

    <tr><td><code>Œ±<sub>t</sub></code></td>
        <td>retain-signal factor at step <code>t</code></td>
        <td class="shape">scalar in (0,1)</td></tr>

    <tr><td><code>ùõºÃÑ<sub>t</sub> = ‚àè<sub>s=1</sub><sup>t</sup> Œ±<sub>s</sub></code></td>
        <td>cumulative signal survival</td>
        <td class="shape">scalar</td></tr>

    <tr><td><code>œÉ<sub>t</sub> = ‚àö(1-ùõºÃÑ<sub>t</sub>)</code></td>
        <td>total noise std-dev after step <code>t</code></td>
        <td class="shape">scalar</td></tr>

    <tr><td><code>Œµ ‚àº ùí©(0,I<sub>d</sub>)</code></td>
        <td>fresh isotropic Gaussian noise</td>
        <td class="shape"><code>‚Ñù<sup>d</sup></code></td></tr>

    <tr><td><code>p<sub>data</sub></code></td>
        <td>true‚Äîbut unknown‚Äîimage data density</td>
        <td class="shape">never explicit</td></tr>

    <tr><td><code>q(x<sub>t</sub> | x<sub>0</sub>)</code></td>
        <td>designer-chosen Gaussian noising kernel</td>
        <td class="shape">analytic</td></tr>

    <tr><td><code>p<sub>t</sub>(x<sub>t</sub>)</code></td>
        <td>marginal noisy density after corruption</td>
        <td class="shape">unknown but samplable</td></tr>

    <tr><td><code>s<sub>Œ∏</sub>(x<sub>t</sub>,t)</code></td>
        <td>neural network score estimator</td>
        <td class="shape">same shape as <code>x<sub>t</sub></code></td></tr>
  </tbody>
</table>
<p class="note">
  Throughout, <code>Œµ&nbsp;‚àº&nbsp;ùí©(0,&nbsp;I<sub>d</sub>)</code>.
  All inner products&nbsp;/&nbsp;norms are Euclidean.
</p>

<!-- ============  Stock-the-Toolbox Facts  ============ -->
<table class="tbl fact-table">
  <caption>Stock-the-Toolbox Facts (handy formulas)</caption>
  <tbody>
    <tr><th>Gaussian PDF</th>
        <td><code>œï<sub>Œ£</sub>(z) = exp(-¬Ω z<sup>‚ä§</sup>Œ£<sup>-1</sup>z) / ‚àö((2œÄ)<sup>d</sup> det Œ£)</code></td></tr>

    <tr><th>Quadratic gradient</th>
        <td><code>‚àá<sub>x</sub>‚Äñx-a‚Äñ<sup>2</sup> = 2 (x-a)</code></td></tr>

    <tr><th>Chain rule for logs</th>
        <td><code>‚àá log f = (‚àáf) / f</code></td></tr>

    <tr><th>Affine-Gaussian rule</th>
        <td>If <code>Z‚àºùí©(0,I)</code> and <code>Y=a+BZ</code> then <code>Y‚àºùí©(a, B B<sup>‚ä§</sup>)</code>.</td></tr>
  </tbody>
</table>
<p class="note">
  Keep these identities at hand&mdash;we&rsquo;ll cite them explicitly in the derivations below.
</p>


<!-- =====================================================
     1. Forward (Noising) Process ‚Äî From One Step to Closed Form
     ===================================================== -->
<h2 id="forward">1.&nbsp;Forward&nbsp;(Noising) Process &mdash; from a single&nbsp;noise injection to a closed-form kernel</h2>
<p class="lead">
  Imagine pressing a camera‚Äôs shutter while slowly cranking up the ISO:
  each tick makes the photo grainier until only static remains.
  The <em>forward&nbsp;process</em> in diffusion models is exactly that:
  a time-indexed recipe that <strong>systematically degrades</strong> a clean
  image into pure Gaussian noise.
  <br>
  In this section we:
  <ul>
    <li>quantify one such ‚Äúgrain-up‚Äù tick (<span class="math">\( \beta \)</span>-noise injection),</li>
    <li>show how chaining many ticks collapses into a single closed-form
        distribution, and</li>
    <li>write that distribution analytically, so its gradient becomes a
        free supervised label in later training.</li>
  </ul>
  Master these three steps and you hold the <em>entire</em> forward half of a diffusion model in your hands.
</p>
<!-- 1.1 one tiny noise injection -->
<h3>1.1&nbsp;One tiny noise injection</h3>

<p>
  Think of the forward process as a photographic fade-out:
  each step <em>shrinks</em> the original signal and sprinkles in fresh Gaussian static.  
  A blend parameter&nbsp;\( \beta\in(0,1) \) tells us
  exactly how much signal versus noise we keep.
</p>

<p class="display-math">\[
  x' \;=\; \sqrt{\,1-\beta\,}\;x
        \;+\; \sqrt{\beta}\;\varepsilon,
  \qquad
  \varepsilon \sim \mathcal N(0,I_d).
\tag{1.1}\]</p>

<p>
  The square-root factors guarantee the variance of \(x'\) is
  <span class="math">\( \beta \)</span> higher than that of \(x\):
  the signal‚Äôs power is scaled by \(1-\beta\), the noise‚Äôs by \(\beta\).
</p>

<!-- 1.2 repeat t times -->
<h3>1.2&nbsp;Repeat <em>t</em> steps &rarr; one closed-form jump</h3>

<p>
  Apply (1.1) successively with variances
  \( \beta_1,\beta_2,\dots,\beta_t \) &nbsp;
  \(\bigl(\alpha_s := 1-\beta_s\bigr)\).
  Induction collapses the chain into one affine transform:
</p>

<p class="display-math">\[
  x_t \;=\; \sqrt{\bar\alpha_t}\,x_0
        \;+\; \sqrt{1-\bar\alpha_t}\,\varepsilon,
  \qquad
  \bar\alpha_t := \prod_{s=1}^{t}\alpha_s .
\tag{1.2}\]</p>

<p>
  In words: after \(t\) steps a fixed share
  \( \sqrt{\bar\alpha_t} \) of the original image survives; the rest is
  i.i.d.&nbsp;Gaussian noise with total standard deviation
  \( \sigma_t := \sqrt{1-\bar\alpha_t} \).
</p>

<!-- 1.3 conditional kernel -->
<h3>1.3&nbsp;Conditional density \( q(x_t \mid x_0) \)</h3>

<p>
  Any affine map of a Gaussian remains Gaussian, so conditioning on the
  clean image \(x_0\) we obtain an <em>analytic</em> forward kernel:
</p>

<p class="display-math">\[
  q(x_t \mid x_0)
  \;=\;
  \mathcal N\!\bigl(
      x_t;
      \sqrt{\bar\alpha_t}\,x_0,\;
      \sigma_t^2 I_d
  \bigr).
\tag{1.3}\]</p>

<p>
  Equation&nbsp;(1.3) is a workhorse: its gradient will give us a
  <em>free</em> supervised label when we train the score network.
</p>

<!-- =====================================================
     2. Why We Need the Score ‚Äî and How to Get It for Free
     ===================================================== -->
<h2 id="score">2.&nbsp;Why We Need the&nbsp;Score&nbsp;&mdash; and&nbsp;How&nbsp;to&nbsp;Get&nbsp;It for&nbsp;Free</h2>

<!-- ---------- lead-in ---------- -->
<p class="lead">
  Training a diffusion model is not about matching full probability
  densities &mdash; that would require intractable normalising constants.
  Instead, we learn the <em>score&nbsp;field</em>:
  the gradient of the log-density, which points toward higher probability
  exactly like a small compass needle.  Remarkably, the Gaussian forward
  kernel we just derived lets us fabricate <strong>perfect ground-truth
  score labels</strong> on the fly, at <em>every</em> noise level.
</p>

<!-- 2.1 What is a score? -->
<h3>2.1&nbsp;What is a score?</h3>
<p>
  For any probability density \( p(z) \) the&nbsp;
  <em>score</em> is defined as
  \( s(z) := \nabla_z \log p(z) \).
  It is the vector pointing in the direction
  the density rises fastest&mdash;a gradient ascent arrow in probability
  space.
</p>

<!-- 2.2 Gradient of the Gaussian kernel -->
<h3>2.2&nbsp;Gradient of the Gaussian&nbsp;kernel (one-liner!)</h3>
<p>
  Take the log of the forward kernel \( q(x_t \mid x_0) \) from&nbsp;(1.3)
  and differentiate with respect to \( x_t \).  A single line of algebra
  (using the quadratic-gradient rule) gives
</p>

<p class="display-math">\[
  \nabla_{x_t}\log q(x_t\mid x_0)
  \;=\;
  -\frac{x_t - \sqrt{\bar\alpha_t}\,x_0}{1-\bar\alpha_t}
  \;=\;
  -\frac{\varepsilon}{\sigma_t},
  \qquad
  \bigl(\sigma_t^2 = 1-\bar\alpha_t\bigr).
\tag{2.1}\]</p>

<p class="explain">
  Beautifully simple: the score of the <em>conditional</em> Gaussian is
  just &ldquo;negative noise divided by&nbsp;\(\sigma_t\).&rdquo;
</p>

<!-- 2.3 Training label inside each mini-batch -->
<h3>2.3&nbsp;Training label in every mini-batch</h3>
<p>
  During training we <em>know</em> the exact noise
  \( \varepsilon \) we injected when we built \( x_t \).
  Hence we can compute a <strong>free, per-image, per-timestep label</strong>
</p>

<p class="display-math">\[
  y_t := -\,\varepsilon / \sigma_t.
\tag{2.2}\]</p>

<p class="explain">
  No approximation, no Monte-Carlo &mdash; \( y_t \) drops out of thin air
  for every example in the batch.  All that remains is to prove this label
  is an <em>unbiased</em> estimator of the true, unknown
  marginal score \( \nabla_{x_t}\log p_t(x_t) \);
  we do that in the next subsection using a three-line Bayes trick.
</p>






