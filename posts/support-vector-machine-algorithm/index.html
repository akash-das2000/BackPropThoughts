<section>
  <h1 id="support-vector-machines">Maximal Margins: An In-Depth Mathematical Exploration of Support Vector Machines</h1>
  <p><em>An extensive technical treatise on Support Vector Machines: primal &amp; dual QP derivations, hinge-loss geometry, KKT proof, Representer Theorem, advanced kernels, duality gap analysis, PAC-Bayes bounds, Support Vector Regression, incremental learning, and structured-output extensions.</em></p>

  <h2 id="introduction">1. Introduction</h2>
  <p>
    Support Vector Machines (SVMs) are margin-based classifiers derived from convex optimization principles. By solving a quadratic program (QP), SVMs maximize the margin—the smallest perpendicular distance from data points to the decision hyperplane—thereby ensuring strong generalization guarantees via structural risk minimization.
  </p>

  <h2 id="primal-qps">2. Primal QP &amp; Hinge-Loss Formulation</h2>
  <p>Given training data \(\{(x_i,y_i)\}_{i=1}^n\), with \(x_i\in\mathbb R^d\), \(y_i\in\{\pm1\}\), the soft-margin primal QP is:</p>
  <pre><code>
min_{w,b,\xi}\quad \frac{1}{2}\|w\|^2 + C \sum_{i=1}^n \xi_i  \\
\text{s.t.}\quad y_i\bigl(w^\top x_i + b\bigr) \ge 1 - \xi_i, \quad \xi_i \ge 0.
  </code></pre>
  <p>Equivalently, one minimizes regularized hinge loss \(\ell(u)=\max(0,1-u)\):</p>
  <pre><code>
min_{w,b}\quad \frac12\|w\|^2 + C \sum_{i=1}^n \ell\bigl(y_i(w^\top x_i + b)\bigr).
  </code></pre>
  <p>The subgradient w.r.t. \(w\) is:</p>
  <pre><code>
\partial_w = w - C \sum_{i=1}^n y_i x_i \; \mathbf{1}_{y_i(w^\top x_i + b) < 1}.
  </code></pre>

  <h2 id="dual-derivation">3. Dual QP Derivation &amp; KKT Conditions</h2>
  <p>Introduce Lagrange multipliers \(\alpha_i\ge0\) for margin constraints and \(\mu_i\ge0\) for \(\xi_i\):</p>
  <pre><code>
L = \tfrac12\|w\|^2 + C\sum_i\xi_i
    - \sum_i \alpha_i\bigl[y_i(w^\top x_i + b)-1+\xi_i\bigr]
    - \sum_i \mu_i\xi_i.
  </code></pre>
  <p>Stationarity gives:</p>
  <ul>
    <li>\(\nabla_w L = 0 \implies w = \sum_{i=1}^n \alpha_i y_i x_i\)</li>
    <li>\(\partial_b L = 0 \implies \sum_{i=1}^n \alpha_i y_i = 0\)</li>
    <li>\(\partial_{\xi_i} L = 0 \implies C - \alpha_i - \mu_i = 0 \implies 0 \le \alpha_i \le C\)</li>
  </ul>
  <p>Substituting yields the dual QP:</p>
  <pre><code>
max_{\alpha}\quad \sum_{i=1}^n \alpha_i - \tfrac12 \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j x_i^\top x_j  \\
\text{s.t.}\quad 0 \le \alpha_i \le C, \quad \sum_{i=1}^n \alpha_i y_i = 0.
  </code></pre>

  <h3 id="kkt-conditions">3.1 KKT Conditions</h3>
  <ul>
    <li>Primal feasibility: \(y_i(f(x_i)) \ge 1 - \xi_i,\;\xi_i\ge0\)</li>
    <li>Dual feasibility: \(0\le\alpha_i\le C\)</li>
    <li>Complementary slackness:
      \(\alpha_i[y_i(f(x_i))-1 + \xi_i]=0,\;(C-\alpha_i)\xi_i=0.\)
    </li>
    <li>Support vectors: those with \(0<\alpha_i<C\) satisfy \(y_i f(x_i)=1\).</li>
  </ul>

  <h2 id="representer-theorem">4. Representer Theorem &amp; Kernelization</h2>
  <p>By the Representer Theorem, the solution lies in the span of \(\{x_i\}\):</p>
  <pre><code>
w = \sum_i \alpha_i y_i x_i, \quad f(x)=\sum_i \alpha_i y_i \langle x_i, x \rangle + b.
  </code></pre>
  <p>For non-linear separation, replace \(\langle x_i,x \rangle\) with kernel \(K(x_i,x)=\phi(x_i)^\top\phi(x)\). The dual becomes:</p>
  <pre><code>
max_{\alpha}\;\sum_i\alpha_i - \tfrac12\sum_{i,j}\alpha_i\alpha_j y_i y_j K(x_i,x_j)
\quad\text{s.t.}\quad 0\le\alpha_i\le C,\;\sum_i\alpha_i y_i=0.
  </code></pre>

  <h3 id="common-kernels">4.1 Common Kernel Functions</h3>
  <ul>
    <li>Linear: \(K(x,z)=x^\top z\)</li>
    <li>Polynomial: \(K(x,z)=(\gamma\,x^\top z + r)^d\)</li>
    <li>RBF (Gaussian): \(K(x,z)=\exp\bigl(-\|x-z\|^2/(2\sigma^2)\bigr)\)</li>
    <li>ANOVA: \(K(x,z)=\sum_{j=1}^d \exp(-\sigma\,(x_j-z_j)^2)\)</li>
  </ul>

  <h2 id="optimization-algorithms">5. Optimization Algorithms &amp; Complexity</h2>
  <ul>
    <li><strong>SMO (Sequential Minimal Optimization):</strong> solves 2-variable QP subproblems analytically; empirical complexity \(O(n^2 d)\).</li>
    <li><strong>Interior-point methods:</strong> primal-dual path-following; worst-case \(O(n^3)\).</li>
    <li><strong>Pegasos (SGD):</strong> optimizes primal hinge via stochastic gradient; iteration complexity \(O(1/(\lambda\epsilon))\).</li>
    <li><strong>Dual coordinate ascent:</strong> cyclic coordinate updates of \(\alpha_i\); converges linearly under strong convexity.</li>
  </ul>

  <h2 id="advanced-extensions">6. Advanced Extensions</h2>
  <h3 id="multi-class-svm">6.1 Multi-class SVMs</h3>
  <ul>
    <li>One-vs-Rest: train \(K\) binary SVMs; predict class \(\arg\max_k f_k(x)\).</li>
    <li>Crammer–Singer formulation:
      \(\min_{W}\tfrac12\sum_k\|w_k\|^2 + C\sum_i\max_{j\neq y_i}[1 + w_j^\top x_i - w_{y_i}^\top x_i]_+\).
    </li>
  </ul>
  <h3 id="structured-svm">6.2 Structured Output &amp; Ranking SVM</h3>
  <p>General joint-feature SVMs with loss-augmented inference: cutting-plane algorithm solves:</p>
  <pre><code>
min_W \tfrac12\|W\|^2 + C\sum_i \max_{y\neq y_i}\bigl[\Delta(y_i,y) + W^\top(\Psi(x_i,y)-\Psi(x_i,y_i))\bigr]_+.
  </code></pre>

  <h2 id="theoretical-bounds">7. Theoretical Generalization &amp; Stability Bounds</h2>
  <h3 id="vc-dimension-bound">7.1 VC-Dimension Bound</h3>
  <p>With data in an \(R\)-ball and margin \(\gamma\):</p>
  <div>
  \[
    \mathrm{VCdim} \le \min\Bigl\{d,\;\frac{R^2}{\gamma^2}\Bigr\}.
  \]
  </div>

  <h3 id="margin-bound">7.2 Margin-Based Generalization Bound</h3>
  <p>With probability \(1-\delta\):</p>
  <div>
  \[
    \mathrm{Err}(f) \le O\Bigl(\frac{R^2}{\gamma^2}\frac{\ln(1/\delta)}{n}\Bigr).
  \]
  </div>

  <h3 id="loo-bound">7.3 Leave-One-Out Bound</h3>
  <p>Let \(\#\mathrm{SV}\) be support vectors count:</p>
  <div>
  \[
    \mathrm{Err}_{\mathrm{LOO}} \le \frac{\#\mathrm{SV}}{n}.
  \]
  </div>

  <h3 id="pac-bayes-bound">7.4 PAC-Bayes Bound</h3>
  <p>Applying PAC-Bayes yields, for any posterior \(Q\) over weight vectors:</p>
  <div>
  \[
    \mathbb{E}_{w\sim Q}[\mathrm{Err}(w)] \le \mathbb{E}_{w\sim Q}[\widehat{\mathrm{Err}}(w)] + \sqrt{\frac{\mathrm{KL}(Q\|P) + \ln(2\sqrt{n}/\delta)}{2(n-1)}}.
  \]
  </div>

  <h2 id="svr">8. Support Vector Regression (SVR)</h2>
  <p>SVR solves:</p>
  <pre><code>
min_{w,b,\xi,\xi^*} \tfrac12\|w\|^2 + C\sum_i(\xi_i + \xi_i^*)  \\
\text{s.t.}\;
  y_i - (w^\top x_i + b) \le \epsilon + \xi_i,  \\
  (w^\top x_i + b) - y_i \le \epsilon + \xi_i^*,  \\
  \xi_i,\xi_i^* \ge 0.
  </code></pre>

  <h2 id="incremental-svm">9. Incremental &amp; Online SVM Learning</h2>
  <p>Online updates via dual coordinate ascent and budget maintenance to cap \(\#\mathrm{SV}\). Algorithms like NORMA, LASVM implement streaming updates with theoretical regret bounds \(O(\sqrt{T})\).</p>

  <h2 id="applications">10. Applications &amp; Practical Tips</h2>
  <ul>
    <li>Text &amp; NLP: linear SVM on TF-IDF, use Pegasos for large corpora.</li>
    <li>Image &amp; Vision: RBF for small datasets; approximate kernels for scalability.</li>
    <li>Bioinformatics: string/graph kernels for sequence and network data.</li>
    <li>Anomaly Detection: one-class SVM with \(\nu\)-parameter controlling support vectors.</li>
  </ul>

  <h2 id="conclusion">11. Conclusion</h2>
  <p>
    This deep dive has illuminated the mathematical foundations of SVMs, from primal/dual QP formulations and KKT proofs to advanced generalization bounds and practical extensions like SVR and online learning. SVMs remain a cornerstone of classical ML, blending convex optimization with statistical rigour.
  </p>
  <p><em>API Reference: <a href="https://scikit-learn.org/stable/modules/svm.html" target="_blank">scikit-learn SVM Modules</a></em></p>
</section>
