<section>
  <h1 id="support-vector-machines">Maximal Margins: An In-Depth Mathematical Exploration of Support Vector Machines</h1>
  <p><em>An extensive technical treatise on Support Vector Machines: primal &amp; dual QP derivations, hinge-loss geometry, KKT proof, Representer Theorem, advanced kernels, duality gap analysis, PAC-Bayes bounds, Support Vector Regression, incremental learning, and structured-output extensions—now with detailed intuitive explanations and full mathematical rigor.</em></p>

  <h2 id="introduction">1. Introduction</h2>
  <p>
    Support Vector Machines (SVMs) are supervised learning models rooted in convex optimization and geometric intuition. Their goal is to find the hyperplane that maximally separates two classes with the largest possible margin. This margin-based formulation not only reduces overfitting but enables powerful theoretical guarantees.
  </p>

  <h2 id="primal-qps">2. Primal QP &amp; Hinge-Loss Formulation</h2>
  <p>
    Given labeled data \( \{(x_i, y_i)\}_{i=1}^n \), where \( x_i \in \mathbb{R}^d \), \( y_i \in \{-1, +1\} \), we seek a hyperplane \( f(x) = w^T x + b \) such that:
  </p>
  <div>
  \[
    y_i(w^T x_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0
  \]
  </div>
  <p>
    The primal soft-margin SVM solves:
  </p>
  <div>
  \[
    \min_{w,b,\xi} \quad \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i
  \]
  </div>
  <p>
    This balances maximizing the margin (minimizing \( \|w\| \)) and penalizing misclassified or margin-violating points (via slack variables \( \xi_i \)).
  </p>
  <p>
    Using the hinge loss \( \ell(u) = \max(0, 1 - u) \), this is equivalent to:
  </p>
  <div>
  \[
    \min_{w,b} \quad \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \ell(y_i(w^T x_i + b))
  \]
  </div>

  <h2 id="dual-derivation">3. Dual QP Derivation &amp; KKT Conditions</h2>
  <p>
    The Lagrangian for the primal is:
  </p>
  <div>
  \[
    \mathcal{L} = \frac{1}{2} \|w\|^2 + C \sum_i \xi_i - \sum_i \alpha_i (y_i(w^T x_i + b) - 1 + \xi_i) - \sum_i \mu_i \xi_i
  \]
  </div>
  <p>
    Optimality conditions (setting gradients to zero):
  </p>
  <ul>
    <li>\( \nabla_w \mathcal{L} = 0 \Rightarrow w = \sum_i \alpha_i y_i x_i \)</li>
    <li>\( \nabla_b \mathcal{L} = 0 \Rightarrow \sum_i \alpha_i y_i = 0 \)</li>
    <li>\( \nabla_{\xi_i} \mathcal{L} = 0 \Rightarrow \alpha_i + \mu_i = C \Rightarrow 0 \le \alpha_i \le C \)</li>
  </ul>
  <p>
    Plugging into \( \mathcal{L} \), we derive the dual QP:
  </p>
  <div>
  \[
    \max_{\alpha} \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j x_i^T x_j \\
    \text{s.t.} \quad 0 \le \alpha_i \le C, \quad \sum_i \alpha_i y_i = 0
  \]
  </div>
  <p>
    The KKT conditions ensure optimality and determine support vectors (points for which \( \alpha_i > 0 \)).
  </p>

  <h2 id="representer-theorem">4. Representer Theorem &amp; Kernelization</h2>
  <p>
    The optimal hyperplane lies in the span of training examples:
  </p>
  <div>
  \[
    w = \sum_{i=1}^n \alpha_i y_i x_i \Rightarrow f(x) = \sum_i \alpha_i y_i \langle x_i, x \rangle + b
  \]
  </div>
  <p>
    Replacing \( \langle x_i, x_j \rangle \) with a kernel \( K(x_i, x_j) \), we work implicitly in feature space \( \phi(x) \) without computing it directly.
  </p>
  <p>
    Common kernels:
  </p>
  <ul>
    <li>Linear: \( K(x,z) = x^T z \)</li>
    <li>Polynomial: \( K(x,z) = (\gamma x^T z + r)^d \)</li>
    <li>Gaussian RBF: \( K(x,z) = \exp(-\|x-z\|^2 / (2\sigma^2)) \)</li>
  </ul>

  <h2 id="optimization-algorithms">5. Optimization Algorithms &amp; Complexity</h2>
  <p>
    The dual form is a convex QP. SMO solves it by iteratively optimizing two \(\alpha_i\) variables at a time. Pegasos minimizes the primal using stochastic subgradient descent:
  </p>
  <div>
  \[
    w_{t+1} = (1 - \eta_t \lambda)w_t + \eta_t y_i x_i \mathbf{1}[y_i w_t^T x_i < 1]
  \]
  </div>
  <p>
    The choice depends on problem size, kernel use, and batch vs. online setting.
  </p>

  <h2 id="advanced-extensions">6. Advanced Extensions</h2>
  <p>
    In multi-class SVMs, we learn \(K\) classifiers \( w_k \) and assign:
  </p>
  <div>
  \[
    \hat{y} = \arg\max_k w_k^T x
  \]
  </div>
  <p>
    The Crammer–Singer model jointly enforces margins across all class pairs. Structured SVMs generalize this further using joint feature maps \( \Psi(x, y) \) and solve for structured output prediction.
  </p>

  <h2 id="theoretical-bounds">7. Theoretical Generalization &amp; Stability Bounds</h2>

  <h3>7.1 VC-Dimension Bound</h3>
  <p>If inputs lie in a ball of radius \( R \), and margin is \( \gamma \), then:</p>
  <div>
  \[
    \mathrm{VCdim} \le \min\left\{d, \frac{R^2}{\gamma^2}\right\}
  \]
  </div>

  <h3>7.2 Margin-Based Bound</h3>
  <p>With probability \( 1 - \delta \):</p>
  <div>
  \[
    \mathrm{Err}(f) \le O\left( \frac{R^2}{\gamma^2} \cdot \frac{\log(1/\delta)}{n} \right)
  \]
  </div>

  <h3>7.3 Leave-One-Out (LOO) Bound</h3>
  <div>
  \[
    \mathrm{Err}_{\mathrm{LOO}} \le \frac{\#\mathrm{SV}}{n}
  \]
  </div>

  <h3>7.4 PAC-Bayes Bound</h3>
  <div>
  \[
    \mathbb{E}_{w \sim Q}[\mathrm{Err}(w)] \le \mathbb{E}_{w \sim Q}[\widehat{\mathrm{Err}}(w)] + \sqrt{\frac{\mathrm{KL}(Q\|P) + \ln(2\sqrt{n}/\delta)}{2(n-1)}}
  \]
  </div>

  <h2 id="svr">8. Support Vector Regression (SVR)</h2>
  <p>
    SVR aims to predict continuous outputs with a tolerance \( \epsilon \). Optimization:
  </p>
  <div>
  \[
    \min_{w,b,\xi,\xi^*} \tfrac12 \|w\|^2 + C \sum_i (\xi_i + \xi_i^*)
  \]
  <div>
    Subject to:
  </div>
  \[
    \begin{aligned}
    &y_i - (w^T x_i + b) \le \epsilon + \xi_i \\
    &(w^T x_i + b) - y_i \le \epsilon + \xi_i^* \\
    &\xi_i, \xi_i^* \ge 0
    \end{aligned}
  \]

  <h2 id="incremental-svm">9. Incremental &amp; Online SVM Learning</h2>
  <p>
    Algorithms like LASVM update \( \alpha_i \) online with low memory. Budgeted SVMs limit the number of support vectors to fixed-size pools.
  </p>

  <h2 id="applications">10. Applications &amp; Practical Tips</h2>
  <ul>
    <li>Text classification: linear SVMs on TF-IDF vectors</li>
    <li>Image recognition: RBF or deep-kernel SVMs</li>
    <li>Bioinformatics: domain-specific kernels (e.g. string kernels)</li>
    <li>Outlier detection: one-class SVM for novelty detection</li>
  </ul>

  <h2 id="conclusion">11. Conclusion</h2>
  <p>
    SVMs blend geometry, optimization, and statistical theory. Their performance and elegance lie in margin maximization and the use of duality for kernelization. Through this deep dive, we’ve uncovered their theoretical underpinnings and broad extensions.
  </p>
  <p><em>Reference API: <a href="https://scikit-learn.org/stable/modules/svm.html" target="_blank">scikit-learn SVM</a></em></p>
</section>
