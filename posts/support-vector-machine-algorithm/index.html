<section>
  <h1 id="support-vector-machines">Maximal Margins: An In-Depth Mathematical Exploration of Support Vector Machines</h1>
  <p><em>An extensive technical treatise on Support Vector Machines: full primal-dual derivation, hinge-loss analysis, geometric margin theory, KKT proofs, Representer Theorem, kernel expansions, optimization, generalization bounds, PAC-Bayes, SVR, and structured predictionâ€”each explained with both mathematics and intuition.</em></p>

  <h2 id="introduction">1. Introduction</h2>
  <p>
    Support Vector Machines (SVMs) are supervised learning models rooted in convex optimization and geometric intuition. Their core objective is to find a decision boundary that not only separates the classes but does so with the maximum margin. This margin-based formulation underpins both their empirical success and strong generalization guarantees.
  </p>

  <h2 id="primal-formulation">2. Primal Optimization Formulation</h2>
  <p>Let the training data be \( \{(x_i, y_i)\}_{i=1}^n \), where \( x_i \in \mathbb{R}^d \), \( y_i \in \{-1, +1\} \). The goal is to find a separating hyperplane \( f(x) = w^T x + b \) with maximum margin and minimal violations.</p>
  <p>The primal problem for soft-margin SVM is:</p>
  <div>
  \[
  \min_{w,b,\xi} \quad \frac{1}{2}\|w\|^2 + C \sum_{i=1}^n \xi_i \quad \text{s.t.} \quad y_i(w^T x_i + b) \ge 1 - \xi_i, \; \xi_i \ge 0
  \]
  </div>
  <p>
    This balances maximizing the margin \(\frac{1}{\|w\|}\) and penalizing misclassifications. \(C\) controls the trade-off.
  </p>
  <p>Using the hinge loss \( \ell(u) = \max(0, 1 - u) \), we equivalently solve:</p>
  <div>
  \[
  \min_{w,b} \; \frac{1}{2}\|w\|^2 + C \sum_{i=1}^n \max(0, 1 - y_i(w^T x_i + b))
  \]
  </div>

  <h2 id="geometric-margin">3. Geometry of the Margin</h2>
  <p>
    The geometric margin is the perpendicular distance from the closest point to the hyperplane:
  </p>
  <div>
  \[
    \gamma = \min_i \frac{y_i(w^T x_i + b)}{\|w\|}
  \]</div>
  <p>
    Rescaling \(w, b\) does not affect the decision boundary, so we constrain \(y_i(w^T x_i + b) \ge 1\). Then maximizing margin is equivalent to minimizing \(\|w\|^2/2\).
  </p>

  <h2 id="primal-to-dual">4. Full Primal to Dual Derivation</h2>
  <p>We introduce Lagrange multipliers \(\alpha_i \ge 0\) for margin constraints and \(\mu_i \ge 0\) for \(\xi_i \ge 0\). The Lagrangian is:</p>
  <div>
  \[
  \mathcal{L}(w,b,\xi,\alpha,\mu) = \frac{1}{2}\|w\|^2 + C\sum_i \xi_i - \sum_i \alpha_i[y_i(w^T x_i + b) - 1 + \xi_i] - \sum_i \mu_i \xi_i
  \]</div>
  <p>Take gradients and set to zero:</p>
  <ul>
    <li>\( \frac{\partial \mathcal{L}}{\partial w} = 0 \Rightarrow w = \sum_i \alpha_i y_i x_i \)</li>
    <li>\( \frac{\partial \mathcal{L}}{\partial b} = 0 \Rightarrow \sum_i \alpha_i y_i = 0 \)</li>
    <li>\( \frac{\partial \mathcal{L}}{\partial \xi_i} = 0 \Rightarrow \alpha_i + \mu_i = C \Rightarrow 0 \le \alpha_i \le C \)</li>
  </ul>
  <p>Substitute back to eliminate \(w, \xi_i\), giving the dual:</p>
  <div>
  \[
    \max_{\alpha} \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j x_i^T x_j \quad \text{s.t.} \; 0 \le \alpha_i \le C,\; \sum_i \alpha_i y_i = 0
  \]</div>
  <p>
    The solution \(w\) depends only on support vectors where \(\alpha_i &gt; 0\).
  </p>

  <h2 id="kkt-analysis">5. KKT Conditions and Support Vectors</h2>
  <p>The optimality conditions include:</p>
  <ul>
    <li><strong>Stationarity:</strong> \( w = \sum_i \alpha_i y_i x_i \)</li>
    <li><strong>Primal feasibility:</strong> \( y_i(w^T x_i + b) \ge 1 - \xi_i \)</li>
    <li><strong>Dual feasibility:</strong> \( 0 \le \alpha_i \le C \)</li>
    <li><strong>Complementary slackness:</strong> \( \alpha_i(y_i(w^T x_i + b) - 1 + \xi_i) = 0 \)</li>
  </ul>
  <p>Only points with \(0 &lt; \alpha_i &lt; C\) lie exactly on the margin.</p>

  <h2 id="kernel-trick">6. Kernel Trick and Feature Mapping</h2>
  <p>In the dual, inputs appear only via dot products \(x_i^T x_j\). Replace with kernel function:</p>
  <div>
  \[
    K(x_i, x_j) = \phi(x_i)^T \phi(x_j)
  \]</div>
  <p>Examples:</p>
  <ul>
    <li>Polynomial: \( (x^T z + r)^d \)</li>
    <li>Gaussian RBF: \( \exp(-\|x-z\|^2 / (2\sigma^2)) \)</li>
  </ul>
  <p>The kernel trick allows non-linear boundaries while keeping the computation in the input space.</p>

  <h2 id="optimization">7. Optimization Techniques</h2>
  <p>Popular solvers include:</p>
  <ul>
    <li><strong>SMO:</strong> Solves 2D subproblems analytically, enforces KKT at each step.</li>
    <li><strong>Pegasos:</strong> Stochastic gradient descent on primal with subgradient for hinge loss.</li>
    <li><strong>Coordinate ascent:</strong> Efficient for sparse data.</li>
  </ul>

  <h2 id="theory">8. Theoretical Guarantees</h2>
  <h3>VC Dimension:</h3>
  <div>
  \[
    \mathrm{VCdim} \le \min\left\{d, \frac{R^2}{\gamma^2}\right\}
  \]</div>
  <p>The smaller \(\|w\|\), the larger the margin \(\gamma\), the lower the capacity.</p>

  <h3>Margin Bound:</h3>
  <div>
  \[
    \Pr(\text{error}) \le O\left(\frac{R^2}{\gamma^2} \cdot \frac{\log(1/\delta)}{n}\right)
  \]</div>

  <h3>Leave-One-Out:</h3>
  <div>
  \[
    \mathrm{Err}_{\mathrm{LOO}} \le \frac{\#\mathrm{SV}}{n}
  \]</div>

  <h3>PAC-Bayes Bound:</h3>
  <div>
  \[
    \mathbb{E}_{w \sim Q}[\text{Err}(w)] \le \mathbb{E}_{w \sim Q}[\widehat{\text{Err}}(w)] + \sqrt{\frac{\mathrm{KL}(Q\|P) + \log(1/\delta)}{2n}}
  \]</div>

  <h2 id="svr">9. Support Vector Regression (SVR)</h2>
  <p>SVR minimizes deviation \(\le \epsilon\) from true outputs, allowing slack:</p>
  <div>
  \[
    \min_{w,b,\xi,\xi^*} \tfrac{1}{2}\|w\|^2 + C \sum_i(\xi_i + \xi_i^*)
  \]</div>
  <div>
  \[
    \text{s.t.}\; y_i - f(x_i) \le \epsilon + \xi_i, \quad f(x_i) - y_i \le \epsilon + \xi_i^*
  \]</div>

  <h2 id="structured">10. Structured & Multi-class Extensions</h2>
  <p>Structured SVMs generalize to outputs like sequences:</p>
  <div>
  \[
    \min_w \tfrac{1}{2}\|w\|^2 + C \sum_i \max_{y \ne y_i} \bigl[\Delta(y, y_i) + w^T(\Psi(x_i, y) - \Psi(x_i, y_i))\bigr]
  \]</div>

  <h2 id="conclusion">11. Conclusion</h2>
  <p>SVMs combine convex optimization, geometric intuition, and statistical theory. From primal-dual formulation to generalization bounds, their elegance and power make them foundational in machine learning.</p>
</section>
