<section>
  <h1 id="random-forest-algorithm">Rooted in Wisdom: A Deep Dive into the Random Forest Algorithm C2</h1>
  <p><em>An advanced technical breakdown of Random Forests, complete with mathematical rigor, splitting criteria, hyperparameters, and theoretical guarantees.</em></p>

  <h2 id="introduction">1. Introduction</h2>
  <p>
    Random Forest is a powerful ensemble method built upon the humble decision tree. It combines bagging (bootstrap aggregation) and randomized feature selection to create a robust model that reduces variance without increasing bias. In this blog, we delve deep into the technical underpinnings of Random Forest, exploring its mathematical formalism, splitting heuristics, hyperparameter interactions, and limitations.
  </p>

  <h2 id="why-random-forest">2. Why Random Forest?</h2>
  <p>
    Consider a dataset <script type="math/tex">D = \{(x_i, y_i)\}_{i=1}^n</script> with <script type="math/tex">x_i \in \mathbb{R}^d</script> and labels <script type="math/tex">y_i \in \mathcal{Y}</script>. A decision tree fits this data greedily and deterministically—making it prone to high variance. Random Forest counters this by constructing an ensemble of <script type="math/tex">B</script> trees <script type="math/tex">\{T_b\}_{b=1}^B</script>, each trained on a bootstrap sample and random feature subsets.
  </p>
  <p>The final prediction is:</p>
  <ul>
    <li><strong>Classification:</strong> <script type="math/tex">\hat{y} = \mathrm{mode}(T_1(x), T_2(x), \dots, T_B(x))</script></li>
    <li><strong>Regression:</strong> <script type="math/tex">\hat{y} = \frac{1}{B} \sum_{b=1}^B T_b(x)</script></li>
  </ul>

  <h2 id="tree-growth">3. Decision Tree Construction</h2>
  <p>Each decision tree in the forest is constructed using the CART algorithm. At every split, it chooses the feature and threshold that minimize an impurity function over the chosen subset of features <script type="math/tex">m \ll d</script>.</p>

  <h3 id="splitting-criteria">3.1 Splitting Criteria</h3>
  <ul>
    <li><strong>Gini Impurity (Classification):</strong></li>
  </ul>
  <p>For a node with class probabilities <script type="math/tex">p_k</script>, the Gini index is:</p>
  <script type="math/tex; mode=display">
    G = 1 - \sum_{k=1}^K p_k^2
  </script>

  <ul>
    <li><strong>Entropy (Information Gain):</strong></li>
  </ul>
  <script type="math/tex; mode=display">
    H = -\sum_{k=1}^K p_k \log_2 p_k
  </script>
  <p>The Information Gain for a split <script type="math/tex">s</script> is:</p>
  <script type="math/tex; mode=display">
    IG(s) = H(P) - \sum_{j \in \text{children}} \frac{|P_j|}{|P|} H(P_j)
  </script>

  <ul>
    <li><strong>Mean Squared Error (Regression):</strong></li>
  </ul>
  <script type="math/tex; mode=display">
    \mathrm{MSE} = \frac{1}{|P|} \sum_{i \in P} (y_i - \bar{y})^2
  </script>

  <h3 id="stopping-criteria">3.2 Stopping Criteria</h3>
  <ul>
    <li>Minimum samples to split: <script type="math/tex">\min_{\text{samples\_split}}</script></li>
    <li>Maximum depth: <script type="math/tex">\max_{\text{depth}}</script></li>
    <li>Minimum leaf size: <script type="math/tex">\min_{\text{samples\_leaf}}</script></li>
    <li>Impurity threshold: stop if decrease <script type="math/tex">\lt \epsilon</script></li>
  </ul>

  <h2 id="randomization">4. Randomization Mechanisms</h2>
  <p>Random Forest introduces two levels of randomness:</p>
  <ul>
    <li><strong>Bootstrap Sampling:</strong> From dataset <script type="math/tex">D</script>, sample <script type="math/tex">D_b \sim D</script> with replacement.</li>
    <li><strong>Feature Subspacing:</strong> At each split, draw a random subset of features <script type="math/tex">F_b \subset \{1, ..., d\}</script> with <script type="math/tex">|F_b| = m</script>.</li>
  </ul>

  <h2 id="hyperparameters">5. Key Hyperparameters</h2>
  <ul>
    <li><strong><script type="math/tex">B</script>: Number of trees.</strong> Higher <script type="math/tex">B</script> lowers variance but increases compute cost.</li>
    <li><strong><script type="math/tex">m</script>: Number of features to consider at each split.</strong> Common defaults: <script type="math/tex">\sqrt{d}</script> for classification, <script type="math/tex">d/3</script> for regression.</li>
    <li><strong><script type="math/tex">\text{max\_depth}</script>:</strong> Controls model complexity.</li>
    <li><strong><script type="math/tex">\text{min\_samples\_split}, \text{min\_samples\_leaf}</script>:</strong> Avoid overfitting to noise.</li>
  </ul>

  <h2 id="generalization-error">6. Generalization Error and Theoretical Insights</h2>
  <p>Let the margin function for an input <script type="math/tex">x</script> and correct class <script type="math/tex">y</script> be:</p>
  <script type="math/tex; mode=display">
    \text{margin}(x, y) = \frac{1}{B} \sum_{b=1}^B \mathbf{1}(T_b(x) = y) - \max_{k \neq y} \frac{1}{B} \sum_{b=1}^B \mathbf{1}(T_b(x) = k)
  </script>
  <p>The generalization error is bounded by:</p>
  <script type="math/tex; mode=display">
    \mathbb{P}(\text{margin}(x, y) < 0)
  </script>

  <figure>
    <img src="posts/random-forest/assets/random_forest_diagram.JPG" alt="Random Forest Diagram">
    <figcaption><strong>Figure 1:</strong> A forest of uncorrelated decision trees voting collectively. Feature and data randomness injects diversity.</figcaption>
  </figure>

  <h2 id="feature-importance">7. Feature Importance</h2>
  <ul>
    <li><strong>Mean Decrease in Impurity (MDI):</strong> Average reduction in Gini or entropy when splitting on a feature.</li>
    <li><strong>Permutation Importance:</strong> Performance drop when feature values are randomly shuffled.</li>
  </ul>

  <figure>
    <img src="posts/random-forest/assets/feature_importance.JPG" alt="Feature Importance">
    <figcaption><strong>Figure 2:</strong> Features ranked by their average importance across the ensemble.</figcaption>
  </figure>

  <h2 id="limitations">8. Limitations</h2>
  <ul>
    <li>Harder to interpret than single trees.</li>
    <li>May not extrapolate well outside training data range (esp. regression).</li>
    <li>Can be slow on very large datasets with many trees.</li>
  </ul>

  <h2 id="applications">9. Real-World Applications</h2>
  <ul>
    <li>Fraud detection, medical diagnosis, genomics.</li>
    <li>Credit scoring, customer segmentation.</li>
    <li>Environmental modeling and remote sensing.</li>
  </ul>

  <h2 id="final-thoughts">10. Final Thoughts</h2>
  <p>
    Random Forest is not just a convenient default—it is a deeply grounded, theoretically robust model that balances predictive power with resistance to overfitting. Its foundation in ensemble theory and statistical learning makes it ideal for tabular, structured data where neural nets often struggle.
  </p>

  <p><em>Explore the implementation via <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" target="_blank">scikit-learn's RandomForestClassifier</a>.</em></p>
</section>
