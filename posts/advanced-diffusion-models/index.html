<style>
  /* Put your styles here */
  /* ----------  TL;DR call-out ---------- */
/* ----------  TL;DR call-out (greyscale theme) ---------- */
.tldr {
  border: 1px solid #c0c0c0;      /* soft grey border */
  border-radius: 0.5rem;
  padding: 1rem 1.25rem;
  background: #f5f5f5;            /* very light grey background */
  color: #333;                    /* dark grey text for readability */
}

.tldr h2 {
  margin: 0 0 0.5rem 0;
  color: #111;                    /* nearly black for the heading */
}

.tldr ol {
  margin-left: 1.3rem;
}

.tldr li strong {
  color: #000;                    /* highlight keywords in pure black */
}

/* -  Ensure TL;DR inline math and text wrap on mobile  ---------- */
.tldr,
.tldr p,
.tldr ol,
.tldr li {
  /* allow words, symbols, and math to break anywhere if needed */
  word-wrap: break-word;
  overflow-wrap: break-word;
  word-break: break-word;
  hyphens: auto;
  white-space: normal;
}

/* target KaTeX/MathJax inline spans if you’re using them */
.tldr .katex,
.tldr .MathJax,
.tldr span {
  display: inline-block;
  max-width: 100%;
  white-space: normal;
}

/* shrink font slightly for extra safety on very small screens */
@media (max-width: 400px) {
  .tldr {
    font-size: 0.90rem;
  }
}


/* ----------  table styling ---------- */
table.tbl{
  width:100%;
  border-collapse:collapse;
  margin:2rem 0 2.5rem 0;
  font-size:.95rem;
}
table.tbl caption{
  caption-side:top;
  font-weight:600;
  margin-bottom:.4rem;
}
table.tbl th,
table.tbl td{
  border:1px solid #d0d0d0;
  padding:.45rem .65rem;
  text-align:left;
  vertical-align:top;
}
table.tbl thead{background:#f5f7ff;}
.tbl .shape{
  font-family:"Roboto Mono",ui-monospace,monospace;
  white-space:nowrap;
}
.fact-table th{
  background:#f5f5f5;
  width:160px;
}

/* ----------  lead paragraph ---------- */
.lead{
  font-size:1.05rem;
  line-height:1.6;
  margin:1.2rem 0 2rem 0;
  color:#333;
}
.lead ul{margin:.6rem 0 .6rem 1.4rem}

/* ----------  bridge note ---------- */
.bridge{
  font-size:.95rem;
  margin:.8rem 0 1.1rem 0;
  color:#444;
}          

/* ----------  note paragraph ---------- */
.note {
  border-left: 3px solid #888;     /* subtle grey accent */
  padding: 0.6rem 1rem;
  margin: 1rem 0;
  background: #f9f9f9;             /* very light grey background */
  color: #333;                     /* dark text for readability */
  font-size: 0.97rem;
  line-height: 1.5;
}
.note strong {
  color: #000;                     /* bold keywords in pure black */
}

/* ----------  display equations ---------- */
.eq-scroll{
  display:block;
  overflow-x:auto;
  white-space:nowrap;
  text-align:center;
  margin:1.2rem auto;
  font-size:1.02rem;
}
@media(max-width:600px){
  .eq-scroll{font-size:.9rem}
  table.tbl{font-size:.9rem}
}

/* ----------  inline & block code ---------- */
code,pre{
  font-family:"Fira Code","SFMono-Regular",ui-monospace,monospace;
  font-size:.92rem;
}
code{
  background:#f3f4f6;
  color:#1a1a1a;
  padding:0 .25em;
  border-radius:4px;
}
pre{
  background:#f8f9fb;
  border:1px solid #cfd2d7;
  border-radius:6px;
  padding:.9rem 1rem;
  line-height:1.45;
  overflow-x:auto;
  margin:1.6rem 0;
}
pre code{background:none;padding:0}
@media(max-width:600px){
  pre{font-size:.82rem}
  code{font-size:.86rem}
}

/* ----------  GitHub-style code card ---------- */
.code-card{
  background:#f6f8fa;
  border:1px solid #d0d7de;
  border-radius:6px;
  overflow:hidden;
  margin:1.6rem 0;
}
.code-card .code-header{
  background:#eaeef2;
  border-bottom:1px solid #d0d7de;
  font:.75rem/1 system-ui,sans-serif;
  color:#24292f;
  padding:.45rem .9rem;
  text-transform:lowercase;
}
.code-card pre{
  margin:0;
  padding:.8rem 1rem;
  background:inherit;
  font-size:.92rem;
  white-space:pre;
}
@media(max-width:600px){
  .code-card pre{font-size:.82rem}
}
  
/* === let wide tables side-scroll on narrow screens === */
@media (max-width: 600px){
  table.tbl{
    display:block;          /* makes it a scroll container   */
    overflow-x:auto;        /* side-scroll if too wide        */
    -webkit-overflow-scrolling: touch;
  }
  table.tbl thead,
  table.tbl tbody{
    display:table;          /* keeps header & body aligned    */
    width:100%;
  }
  table.tbl th,
  table.tbl td{
    white-space:nowrap;     /* prevent ugly line wraps        */
  }
}

/* ----------  Responsive heading wraps & scaling ---------- */

/* Allow long words in headings to break */
h1, h2, h3, h4, h5, h6 {
  overflow-wrap: break-word;
  word-wrap: break-word;
  hyphens: auto;
  white-space: normal;       /* override any no-wrap */
}

/* Shrink heading text on narrow viewports */
@media (max-width: 600px) {
  h1 { font-size: 1.5rem; }
  h2 { font-size: 1.3rem; }
  h3 { font-size: 1.15rem; }
  /* you can add h4, h5 as needed */
}

/* Optional: make the entire page text flow better on mobile */
body {
  word-wrap: break-word;
  overflow-wrap: break-word;
}

/* Override scroll-to-top button on mobile so it never hangs off-screen */
#scrollTopBtn {
  /* default for desktop */
  bottom: 2rem;
  right: 2rem;
}

/* on small viewports, reduce the offsets & size */
@media (max-width: 600px) {
  #scrollTopBtn {
    bottom: 1rem !important;
    right: 1rem !important;
    padding: 0.4rem 0.6rem !important;
    font-size: 1rem !important;
    max-width: 2.5rem;    /* ensure it stays compact */
    max-height: 2.5rem;
  }
}

</style>

<h1>Part 4: Advanced Diffusion Models – Guidance, Latent Spaces, Convergence & Control</h1>

<div class="tldr">
  <h2>TL;DR</h2>
  <ol>
    <li><strong>Guidance & Conditioning:</strong> Steering diffusion models using <em>classifier gradients</em>, <em>classifier-free interpolation (CFG)</em>, and <em>model guidance (MG)</em>. These methods modify the reverse SDE drift term to control generation towards desired conditions like text prompts or labels.</li>
    <li><strong>Latent Diffusion:</strong> Scaling diffusion to high resolutions by operating in VAE-compressed latent spaces. We explain the KL divergence factorization and multi-scale hierarchical diffusion in detail.</li>
    <li><strong>Convergence Theory:</strong> Deriving non-asymptotic convergence rates for score-based models, interpreting them as Wasserstein gradient flows, and detailing the Jordan-Kinderlehrer-Otto (JKO) scheme step-by-step.</li>
    <li><strong>Inverse Problems & Control:</strong> Reformulating reverse SDEs as stochastic control problems (Pontryagin’s Maximum Principle), and exploring Plug-and-Play priors and Regularized Score Distillation (RSD) as applications.</li>
    <li>This blog ties all theory back to practical diffusion models, creating a unified narrative of <strong>control, scalability, convergence, and applications.</strong></li>
  </ol>
</div>

<!-- ========== Introduction Section ========== -->
<h2 id="introduction">Introduction</h2>
<p class="lead">
  In the previous parts of this series, we built up a mathematical foundation for understanding diffusion models. <a href="https://backpropthoughts.netlify.app/post?postId=diffusion-maths" target="_blank">Part 1</a> introduced the forward and reverse stochastic differential equations (SDEs) and showed how score functions drive the generative process. <a href="https://backpropthoughts.netlify.app/post?postId=stochastic-to-diffusion" target="_blank">Part 2</a> extended this by analyzing variational objectives and Fisher information, revealing how ELBO minimization governs model training. Finally, <a href="https://backpropthoughts.netlify.app/post?postId=continious-time-diffuision-models" target="_blank">Part 3</a> bridged discrete-time DDPMs with their continuous-time counterparts, framing diffusion as a limit of variational bounds and connecting it to KL divergence flows.
</p>

<p>
  In this part, we turn to <strong>advanced techniques and theoretical perspectives</strong> that extend diffusion models beyond their vanilla formulations. We explore methods for <em>controlling the generative process</em> (via guidance and conditioning), scaling to <em>higher resolutions efficiently</em> (through latent diffusion), and understanding the <em>convergence behavior</em> of these models. We also examine connections between diffusion and stochastic control, illuminating how inverse problems can be solved using learned score functions.
</p>

<p>
  Throughout, we will focus on new derivations and insights, referring back to the earlier parts where foundational concepts are already established. This allows us to delve deeper into the mathematics of advanced diffusion techniques while keeping the narrative tightly connected to their practical implementations.
</p>

<!-- ========== Section 2: Guidance & Conditioning ========== -->
<h2 id="guidance-conditioning">2. Guidance &amp; Conditioning</h2>

<!-- ----- 2.1 Intuition ----- -->
<h3 id="guidance-intuition">2.1 Intuition: Steering Diffusion with Conditional Information</h3>
<p class="lead">
  Vanilla diffusion models learn to reverse a noising process, recovering data distributions \(p(x)\) without explicit conditioning. However, many tasks require controlling generation based on auxiliary inputs like class labels or text prompts. Guidance techniques achieve this by modifying the reverse SDE drift.
</p>

<p>
  Recall from <a href="https://backpropthoughts.netlify.app/post?postId=diffusion-maths" target="_blank">Part 1</a> that the reverse SDE is:
</p>

<div class="eq-scroll">
  \[
  dx = \big[f(x,t) - g(t)^2 \nabla_x \log p_t(x)\big]\, dt + g(t)\, d\bar{B}_t.
  \tag{1}
  \]
</div>

<p>
  Guidance augments the score \(\nabla_x \log p_t(x)\) with conditional gradients to bias trajectories towards \(p(x|y)\).
</p>

---

<!-- ----- 2.2 Classifier Guidance ----- -->
<h3 id="classifier-guidance">2.2 Classifier Guidance</h3>
<p>
  Classifier guidance (Dhariwal & Nichol, 2021) introduces an external classifier \(p_\phi(y|x)\) to steer samples. Using Bayes’ theorem:
</p>

<div class="eq-scroll">
  \[
  \log p(x|y) = \log p(y|x) + \log p(x) - \log p(y).
  \tag{2}
  \]
</div>

<p>
  Differentiating w.r.t. \(x\) gives:
</p>

<div class="eq-scroll">
  \[
  \nabla_x \log p(x|y) = \nabla_x \log p(y|x) + \nabla_x \log p(x).
  \tag{3}
  \]
</div>

<p>
  Substituting Eq. (3) into Eq. (1), the modified drift becomes:
</p>

<div class="eq-scroll">
  \[
  f(x,t) - g(t)^2\big[\nabla_x \log p(x) + \nabla_x \log p(y|x)\big].
  \tag{4}
  \]
</div>

<details>
  <summary><strong>Step-by-step derivation of Eq. (4)</strong></summary>
  <p>
    Start from the original reverse SDE:
  </p>
  <div class="eq-scroll">
    \[
    dx = [f(x,t) - g(t)^2 \nabla_x \log p(x)]\, dt + g(t)\, d\bar{B}_t.
    \]
  </div>
  <p>
    To bias toward \(p(x|y)\), replace \(\nabla_x \log p(x)\) with \(\nabla_x \log p(x|y)\):
  </p>
  <div class="eq-scroll">
    \[
    \nabla_x \log p(x|y) = \nabla_x \log p(x) + \nabla_x \log p(y|x).
    \]
  </div>
  <p>
    Substitution yields Eq. (4) directly.
  </p>
</details>

<p class="bridge">
  This additional term \(\nabla_x \log p(y|x)\) acts as a guiding force, pulling samples toward regions where the classifier assigns high probability to \(y\).
</p>

---

<!-- ----- 2.3 Classifier-Free Guidance (CFG) ----- -->
<h3 id="cfg">2.3 Classifier-Free Guidance (CFG)</h3>
<p>
  Classifier guidance requires training a separate classifier \(p_\phi(y|x)\) across all noise levels, which can be expensive. Classifier-Free Guidance (Ho & Salimans, 2021) bypasses this by jointly training a model to predict both conditional and unconditional scores.
</p>

<p>
  At sampling time, CFG interpolates:
</p>

<div class="eq-scroll">
  \[
  s_\theta^{CFG}(x, t, y) = s_\theta(x, t) + w\big[s_\theta(x, t, y) - s_\theta(x, t)\big],
  \tag{5}
  \]
</div>

<p>
  where \(w > 1\) amplifies conditional information, increasing fidelity at the cost of sample diversity.
</p>

<details>
  <summary><strong>Step-by-step derivation of Eq. (5)</strong></summary>
  <p>
    Approximate \(\log p(x|y)\) using a Taylor expansion:
  </p>
  <div class="eq-scroll">
    \[
    \log p(x|y) \approx \log p(x) + w[\log p(x|y) - \log p(x)].
    \tag{6}
    \]
  </div>

  <p>
    Differentiating:
  </p>
  <div class="eq-scroll">
    \[
    \nabla_x \log p(x|y) \approx \nabla_x \log p(x) + w[\nabla_x \log p(x|y) - \nabla_x \log p(x)].
    \tag{7}
    \]
  </div>

  <p>
    Rearranging leads to Eq. (5).
  </p>
</details>

<p class="note">
  <strong>Key Insight:</strong> CFG treats conditional guidance as a weighted correction to the unconditional score, requiring no external classifier.
</p>

---

<!-- ----- 2.4 Model Guidance (MG) ----- -->
<h3 id="model-guidance">2.4 Model Guidance (MG)</h3>
<p>
  Model Guidance integrates conditioning directly into training by minimizing:
</p>

<div class="eq-scroll">
  \[
  \mathcal{L}_{MG} = \mathbb{E}_{q(x, y)}\left[\|s_\theta(x, t, y) - \nabla_x \log p(x, y)\|^2\right].
  \tag{8}
  \]
</div>

<p>
  The reverse SDE drift then becomes:
</p>

<div class="eq-scroll">
  \[
  f(x,t) - g(t)^2 s_\theta(x,t,y).
  \tag{9}
  \]
</div>

<p>
  This approach embeds conditional structure into the model itself, avoiding runtime interpolation but increasing training complexity.
</p>

<p class="bridge">
  In pipelines like Stable Diffusion, CFG is preferred for its simplicity, but MG remains an attractive end-to-end alternative.
</p>

